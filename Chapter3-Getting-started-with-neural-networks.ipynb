{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<< [第二章：开始之前：神经网络的数学知识](Chapter2-Mathematical-blocks-of-neural-networks.ipynb) || [目录](index.md) || [第四章：机器学习基础](Chapter4-Fundamentals-of-machine-learning.ipynb) >>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三章：进入神经网络\n",
    "\n",
    "> This chapter is designed to get you started with using neural networks to solve real\n",
    "problems. You will consolidate the knowledge you gained from our very first practical\n",
    "example, and you apply what you have learned to three new problems covering the three\n",
    "most common use cases of neural networks: binary classification, multi-class\n",
    "classification, and scalar regression.\n",
    "\n",
    "本章的主要目标是帮助你开始使用神经网络解决实际问题。你可以从中巩固我们前面第一个实际例子中学习到的知识，并且将学习到的内容应用到三个新的问题当中，这些问题涵盖了三个最常用的神经网络的应用场景：二元分类，多类别分类和标量回归。\n",
    "\n",
    "> In this chapter, you will:\n",
    "\n",
    "> - Take a closer look at the core components of neural networks we introduced in our first\n",
    "example: layers, networks, objective functions and optimizers.\n",
    "- Get a quick introduction to Keras, the Python deep learning library which we will use\n",
    "throughout the book.\n",
    "- Set up a workstation for deep learning, with TensorFlow, Theano, Keras, and GPU\n",
    "support.\n",
    "- Dive into three introductory examples of how to use neural networks to solve real\n",
    "problems:\n",
    "    - classifying movie reviews into positive and negative ones (binary classification).\n",
    "    - classifying news wires by their topic (multi-class classification).\n",
    "    - estimating the price of a house given real estate data (regression).\n",
    "\n",
    "在本章中，你将会：\n",
    "\n",
    "- 近距离的观察我们在第一个例子中用到的神经网络的核心组件：层、网络、目标函数和优化器。\n",
    "- 获得Keras的快速入门介绍，它是贯穿本书我们用的的Python深度学习库。\n",
    "- 搭建一个深度学习工作站，使用TensorFlow、Theano、Keras和GPU支持。\n",
    "- 深入三个入门级的例子，介绍神经网络如何解决实际问题：\n",
    "    - 将影评分为正向和负向（二元分类）。\n",
    "    - 将新闻归类到它们的主题中（多类别分类）。\n",
    "    - 从给定的房价数据中预测房子的价格（回归）。\n",
    "\n",
    "> By the end of this chapter, you will already be able to use neural networks to solve\n",
    "simple machine problems such as classification or regression over vector data. You will\n",
    "then be ready to start building a more principled and theory-driven understanding of\n",
    "machine learning, in the next chapter.\n",
    "\n",
    "学习完本章内容后，你将会具备使用神经网络解决一些简单问题的能力，比方说在矢量数据上进行分类或回归。为下一章开始构建更主要的、更理论化的理解机器学习做好准备。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 神经网络剖析\n",
    "\n",
    "> As we saw in the previous chapters, training a neural network revolves around the\n",
    "following objects:\n",
    "\n",
    "> - Layers, which are combined into a network (or model ).\n",
    "- The input data and corresponding targets .\n",
    "- The loss function , which defines the feedback signal which is used for learning.\n",
    "- The optimizer , which determines how the learning proceeds.\n",
    "\n",
    "上一章我们看到，训练一个神经网络设计下面这些对象：\n",
    "\n",
    "- 层，多个层可以组成一个网络（或模型）。\n",
    "- 输入数据和对应的目标值。\n",
    "- 损失函数，它定义着用来进行学习的反馈信号。\n",
    "- 优化器，它指导着学习发展的进程。\n",
    "\n",
    "> You can visualize their interaction in the following way: the network , composed of layers chained together, maps the input data into predictions . The loss function then\n",
    "compares these predictions to the targets , producing a loss value, a measure how well the\n",
    "predictions of the network match what was expected. The optimizer uses this loss value\n",
    "to update the weights of the network.\n",
    "\n",
    "你可以将上面几个对象之间的相互作用可视化出来：网络，有多个层串联构成，将输入数据映射到预测值。然后损失函数比较这些预测值和目标值，产生一个损失值，这是一个衡量网络预测能力的指标。优化器使用这个损失值来更新网络的权重。\n",
    "\n",
    "![network layers loss func optimizer](imgs/f3.1.jpg)\n",
    "\n",
    "图3-1 网络、层、损失函数和优化器之间的联系\n",
    "\n",
    "> Let’s take a closer look at layers, networks, loss functions and optimizers.\n",
    "\n",
    "本节我们详细的研究层、网络、损失函数和优化器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 层：深度学习的乐高积木\n",
    "\n",
    "> The fundamental data structure in neural networks is the \"layer\", to which you have\n",
    "already been introduced in the previous chapter. A layer is a data-processing module that\n",
    "takes as input one or more tensors, and outputs one or more tensors. Some layers are\n",
    "stateless, but more frequently layers have a state: the layer’s \"weights\", one or several\n",
    "tensors learned with stochastic gradient descent, and which together contain the\n",
    "\"knowledge\" of the network.\n",
    "\n",
    "神经网络中最基本的数据结构就是“层”，在上一章中我们已经初步接触过它。层是一个数据处理模块，它接受输入一个或多个张量，然后输出一个或多个张量。有些层是无状态的，但更普遍的情况是层具有状态：层的“权重”，它们是一个或数个使用随机梯度下降学习到的张量，其中含有网络的“知识“。\n",
    "\n",
    "> Different layers are appropriate for different tensor formats and different types of data\n",
    "processing. For instance, simple vector data, stored in 2D tensors of shape (samples,\n",
    "features) , is often processed by \"fully-connected\" layers, also called\n",
    "\"densely-connected\" or \"dense\" layers (the Dense class in Keras). Sequence data, stored\n",
    "in 3D tensors of shape (samples, timesteps, features) , is typically processed by\n",
    "\"recurrent\" layers such as a LSTM layer. Image data, stored in 4D tensors, is usually\n",
    "processed by 2D convolution layers ( Conv2D ).\n",
    "\n",
    "不同的层适合不同的张量格式和不同类型的数据处理。例如简单的矢量数据，存储在形状为(样本, 特征)的2D张量当中，通常用“全连接”层进行处理，也被成为“紧密连接”或“紧密”层（在Keras中是Dense类）。序列数据，存储在形状为(样本, 序列号, 特征)的3D张量中，通常使用“循环”层如LSTM层进行处理。图像数据，存储在4D张量中，通常使用2D卷积层（Conv2D）进行处理。\n",
    "\n",
    "> You can think of layers as the Lego bricks of deep learning, a metaphor which is\n",
    "made explicit by frameworks like Keras. Building deep learning models in Keras is done\n",
    "by clipping together compatible layers to form useful data transformation pipelines. The\n",
    "notion of \"layer compatibility\" here refers specifically to the fact that every layer will\n",
    "only accept input tensors of a certain shape, and will return output tensors of a certain\n",
    "shape. Consider the following example:\n",
    "\n",
    "你可以将层想象成深度学习的乐高积木，这是一个比喻，但是它被类似Keras这样的框架具体化了。在Keras中构建深度学习模型就是通过将兼容的层次堆叠在一起形成有用的数据转换管道完成的。这里的“层兼容性”具体表示的就是每个层只能接受某种特定形状的输入张量，并且输出某种特定形状的张量。考虑下面的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# 一个带有32个输入单元的全连接层\n",
    "layer = layers.Dense(32, input_shape=(784,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We are creating a layer that will only accept as input 2D tensors where the first\n",
    "dimension is 784 (the zero-th dimension, the batch dimension, is unspecified and thus\n",
    "any value would be accepted). And this layer will return a tensor where the first\n",
    "dimension has been transformed to be 32 :\n",
    "\n",
    "我们上面创建了一个仅接受2D张量输入的层，输入的张量的第一个维度是784（第一个轴本来应该是批次轴，在这里没有设置，因此能够接受任何值）。这个层会输出一个张量，其第一个维度会被转换为32："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layer)\n",
    "layer.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Thus this layer can only be connected to an upstream that expects 32-dimensional\n",
    "vectors as its input. When using Keras you don’t have to worry about compatibility,\n",
    "because the layers that you add to your models are dynamically built to match the shape\n",
    "of the incoming layer. For instance, if you write the following:\n",
    "\n",
    "因此这个层只能够被连接到接受一个32维矢量作为输入的层的上游。当使用Keras时，你不需要担心兼容性问题，因为加入网络的层会自动匹配上游输出的形状然后进行动态构建，例如下面的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Dense(32, input_shape=(784,)))\n",
    "model.add(layers.Dense(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The second layer did not receive an input shape argument—instead it automatically\n",
    "inferred its input shape as being the output shape of the layer that came before.\n",
    "\n",
    "第二层没有指定输入张量形状参数 - 它依据上面层次的输出形状自动推断出了输入的形状。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 模型：层的网络结构\n",
    "\n",
    "> A deep learning model is simply a directed acyclic graph of layers. The most common\n",
    "instance would be a linear stack of layers, mapping a single input to a single output.\n",
    "\n",
    "一个深度学习模型简单来说就是一个有向无环的层网络。其中最常见的就是一个线性堆叠的层栈，将一个输入映射成一个输出。\n",
    "\n",
    "> However, as you move forward, you will be exposed to a much broader variety of\n",
    "network topologies. Some common ones include:\n",
    "\n",
    "> - Two-branch networks\n",
    "- Multi-head networks\n",
    "- Inception blocks\n",
    "\n",
    "然而随着你慢慢深入，你也会碰到一些更加复杂的网络拓扑。其中常见的包括：\n",
    "\n",
    "- 双分支网络\n",
    "- 多头网络\n",
    "- 认知结构块\n",
    "\n",
    "> The topology of a network defines an hypothesis space . You may remember that in\n",
    "chapter one, we defined machine learning as \"searching for useful representations of\n",
    "some input data, within a pre-defined space of possibilities, using guidance from some\n",
    "feedback signal\". By choosing a network topology, you have constrained your \"space of\n",
    "possibilities\" (hypothesis space) to a specific series of tensor operations, mapping input data to output data. What you will then be \"searching\" for, is a good set of values for the\n",
    "weight tensors involved in these tensor operations.\n",
    "\n",
    "网络的拓扑定义了一个假设空间。你应该还记得我们在第一章中把机器学习定义为“对一些预设概率空间的输入数据，通过反馈信号指引，寻找有用的表现形式”。在选择网络拓扑时，你就已经限制了你的“概率空间”（假设空间），使得它只能应用特定的一些张量操作，将输入数据映射到输出数据。然后你将“找到”的是涉及假设空间和这些张量操作的一组较理想的值作为权重张量。\n",
    "\n",
    "> Picking the right network architecture is more an art than a science, and while there\n",
    "are some best practices and principles you can rely on, only practice can really help you\n",
    "become a proper neural network architect. The next few chapters will both teach you\n",
    "explicit principles for building neural networks, and will help you develop intuition as to\n",
    "what works or doesn’t work for specific problems.\n",
    "\n",
    "选择正确的网络结构更像是一门艺术而不是科学，而且虽然有一些最佳实践和原则你可以依赖，但是只有时间才能真正帮助你成为一个合格的神经网络架构师。下面一些章节既会指导你关于构建神经网络的明确原则，也会帮助你建立对于特定问题选择较好方案的直觉。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 损失函数和优化器：控制学习过程的关键\n",
    "\n",
    "> Once the network architecture is defined, we still have to pick two more things:\n",
    "\n",
    "> - The loss function (or objective function), the quantity that will be minimized during\n",
    "training. It represents a measure of success on the task at hand.\n",
    "- The optimizer, which determines how the network will be updated based on the loss\n",
    "function. It implements a specific variant of stochastic gradient descent.\n",
    "\n",
    "当网络结构定义好了之后，我们还需要再选择两样东西：\n",
    "\n",
    "- 损失函数（或者叫目标函数），这是一个会在学习过程中试图最小化的数值。它代表着当前手头任务的成功度量。\n",
    "- 优化器，它决定网络会如何依据损失函数更新它的参数。一般采用随机梯度下降的一个特定变种实现。\n",
    "\n",
    "> A neural network that has multiple outputs may have multiple loss functions (one per\n",
    "output), however the gradient descent process must be based on a single scalar loss value,\n",
    "so what happens for multi-loss networks is that all losses are combined (via averaging)\n",
    "into a single scalar quantity.\n",
    "\n",
    "一个有着多个输出的神经网络也可以有着多个损失函数（每个输出对应一个），然而梯度下降过程必须基于一个单独的标量损失值，因此具有多个损失函数的网络会将全部的损失组合（通过平均）成一个单独的标量值。\n",
    "\n",
    "> Picking the right objective function for the right problem is extremely important: your\n",
    "network will take any shortcut it can to minimize it, so if the objective doesn’t fully\n",
    "correlate with actual success on the task at hand, your network will end up doing things\n",
    "you may not have wanted. Imagine a stupid omnipotent AI trained via stochastic gradient\n",
    "descent, with the poorly-chosen objective function of \"maximizing the average\n",
    "well-being of all humans alive\". To make its job easier, this AI might choose to kill all\n",
    "humans except a few, and focus on the well-being on the remaining ones—since average\n",
    "well-being is not affected by how many humans are left. That might not be what you\n",
    "intended! Just remember that all neural networks you build will be just as ruthless in\n",
    "lowering their loss function—so choose the objective wisely.\n",
    "\n",
    "选择正确的目标函数对于解决问题是至关重要的：你的网络会尝试任何途径去最小化它，因此如果目标不完全与手头任务的成功关联，网络可能会停止在你不希望的状态上。我们可以设想这样一个场景，有一个通过随机梯度下降生成的愚笨却无所不能的人工智能，给它设定的目标函数是“最大化所有人类的平均幸福水平”。要使得这个任务更简单，这个AI可能会选择杀死大部分人类，只留下哪些幸福水平高的小部分人 - 因为平均幸福水平不会因为剩下多少人收到影响。这基本肯定不是你期望的结果！牢记所有你构建的神经网络只会粗暴地降低它们的损失函数 - 所以请明智的选择目标。\n",
    "\n",
    "> Thankfully, when it comes to common problems such as classification, regression, or\n",
    "sequence predictions, there are simple guidelines that you can follow to choose the right\n",
    "loss: for instance, you will use binary crossentropy for a two-class classification problem,\n",
    "categorical crossentropy for a many-class classification problem, mean squared error for\n",
    "a regression problem, CTC for a sequence learning problem... only when you are\n",
    "working on truly new research problems will you have to develop your own objective\n",
    "functions.\n",
    "\n",
    "所幸的是，常见的问题诸如分类、回归或者序列预测都有着简单的指引，你只需要遵循它们来选择正确的损失：例如，在二元分类问题中使用二分交叉熵，在多类别分类问题中使用类别交叉熵，在序列学习问题中使用连接时间分类CTC...只有当你真正在研究新问题时才需要开发你自己的目标函数。\n",
    "\n",
    "> In the next few chapters, we will detail explicitly which loss functions to pick, for a\n",
    "wide range of common tasks.\n",
    "\n",
    "后续章节中我们会详细介绍对于许多常见任务的损失函数的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Keras介绍\n",
    "\n",
    "> Throughout this book, all of our code examples use Keras. Keras is a deep learning\n",
    "framework for Python which provides a convenient way to define and train almost any\n",
    "kind of deep learning model. Keras was initially developed for researchers, aiming at\n",
    "enabling fast experimentation.\n",
    "\n",
    "贯穿本书所有的代码例子都是使用Keras的。Keras是一个Python深度学习框架，它提供了一个简便的方式来定义和训练几乎任何类型的深度学习模型。Keras最初视为研究人员开发的，目标是让快速实验变为可能。\n",
    "\n",
    "> Keras has the following key features:\n",
    "\n",
    "> - It allows the same code to run on CPU or on GPU, seamlessly.\n",
    "- It has a user-friendly API which makes it easy to quickly prototype deep learning models.\n",
    "- It has build-in support for convolutional networks (for computer vision), recurrent\n",
    "networks (for sequence processing), and any combination of both.\n",
    "- It supports arbitrary network architectures: multi-input or multi-output models, layer\n",
    "sharing, model sharing, etc. This means that Keras is appropriate for building essentially\n",
    "any deep learning model, from a generative adversarial network to a neural Turing\n",
    "machine.\n",
    "\n",
    "Keras具有一下一些关键特性：\n",
    "\n",
    "- 它允许无缝的使用相同的代码在CPU和GPU之间切换运行。\n",
    "- 它有一套用户友好的API，能够简单快捷的建立深度学习模型原型。\n",
    "- 它有着內建的卷积网络（提供计算机视觉使用），循环网络（提供序列处理使用），以及两者的任意组合。\n",
    "- 它支持任意的网络结构：多输入或多输出模型，共享层，共享模型等。这意味着Keras适合用来构建几乎任何深度学习模型，从生成对抗网络到神经图灵机。\n",
    "\n",
    "> Keras is distributed under the permissive MIT license, which means that it can be\n",
    "freely used in commercial projects. It is compatible with any version of Python from 2.7\n",
    "to 3.6 (as of mid-2017). Its documentation is available at keras.io .\n",
    "\n",
    "Keras遵循宽松的MIT协议发布，意味着它能自由的在商业项目中使用。它也能兼容任何Python版本，从2.7到3.6(2017年中）。可以访问[keras官网](https://keras.io)获取它的文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Keras has well over a hundred of thousands of users, ranging from academic\n",
    "researchers and engineers at both startups and large companies, to graduate students and\n",
    "even hobbyists. Keras is used at Google, Netflix, Uber, CERN, Yelp, and at hundreds of\n",
    "startups working on a wide range of problems. Keras is also very a popular framework on\n",
    "Kaggle, the machine learning competition website, where almost every recent deep\n",
    "learning competition has been won using Keras models.\n",
    "\n",
    "Keras有远超上万的用户，范围涵盖从学术研究者和工程师，无论是初创企业还是大企业，到本科生和甚至技术爱好者。Keras在Google、Netflix、Uber、CERN、Yelp以及成百上千的初创企业中被使用，运用到非常广泛的问题上。Keras也是机器学习竞赛网站Kaggle上非常流形的框架，最近基本每一次的深度学习竞赛都有获奖者使用Keras模型。\n",
    "\n",
    "![DNN trend](imgs/f3.2.jpg)\n",
    "\n",
    "图3-2 Google搜索不同的深度学习框架趋势"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Keras，TensorFlow，Theano和CNTK\n",
    "\n",
    "> Keras is a model-level library, providing high-level building blocks for developing deep\n",
    "learning models. It does not handle itself low-level operations such as tensor\n",
    "manipulation and differentiation. Instead, it relies on a specialized, well-optimized tensor\n",
    "library to do so, serving as the \"backend engine\" of Keras. Rather than picking one single\n",
    "tensor library and making the implementation of Keras tied to that library, Keras handles\n",
    "the problem in a modular way, and several different backend engines can be plugged\n",
    "seamlessly into Keras. Currently, the three existing backend implementations are the\n",
    "TensorFlow backend, the Theano backend, and the CNTK backend. In the future, it is\n",
    "likely that Keras will be extended to work with even more deep learning execution\n",
    "engines.\n",
    "\n",
    "Keras是一个模型级的库，为开发深度学习模型提供高层的构建组件。它自身并不处理底层的运算，比如张量操作和微分。相反，它依赖一个特定的优化的张量运算库来完成这些工作，这个库被称为Keras的“后端引擎”。但是Keras并没有将自身绑定在某一个特定的后端引擎上来实现，它采用了一种模块化的方式来解决这个问题，因此数个不同的后端引擎能够无缝的对接到Keras上。目前，有三种后端引擎实现，分别是TensorFlow后端、Theano后端和CNTK后端。未来Keras还可能扩展到能支持更多的深度学习执行引擎上。\n",
    "\n",
    "> TensorFlow, CNTK, and Theano are some of the main platforms for deep learning\n",
    "today. Theano is developed by the MILA lab at Université de Montréal , while\n",
    "TensorFlow is developed by Google, and CNTK is developed by Microsoft. Any piece of\n",
    "code that you write with Keras can be run with any of these backends without having to\n",
    "change anything to the code: you can seamlessly switch between the two during\n",
    "development, which often proves useful, for instance if one of these backends proves to\n",
    "be faster for a specific task. By default, I would recommend using the TensorFlow\n",
    "backend for most of your deep learning needs.\n",
    "\n",
    "TensorFlow、CNTK和Theano是今天主要的深度学习平台。Theano是蒙特利尔大学MILA实验室开发的，TensorFlow是Google开发的，CNTK是微软开发的。你在Keras上写的任何一段代码都可以在上述三个后端中运行，而不需要修改任何代码：你可以在开发过程中无缝的切换后端，这种情况有时非常有用，例如在其中一个后端被证明在某个特性任务上比较快的情况下。默认情况下，作者建议在大多数深度学习任务中使用TensorFlow后端。\n",
    "\n",
    "> Via TensorFlow (or Theano, or CNTK), Keras is able to run on both CPU and GPU\n",
    "seamlessly. When running on CPU, TensorFlow is itself wrapping a low-level library for\n",
    "tensor operations called Eigen. On GPU, TensorFlow wraps a library of well-optimized\n",
    "deep learning operations called cuDNN, developed by NVIDIA.\n",
    "\n",
    "通过TensorFlow（或Theano或CNTK），Keras能够无缝的在CPU和GPU之间切换运行。当运行在CPU下时，TensorFlow自身封装了一个底层的张量运算库，成为Eigen。在GPU情况下，TensorFlow封装了一个非常优化的深度学习运算库，NVIDIA开发的cuDNN。\n",
    "\n",
    "![Keras tech stack](imgs/f3.3.jpg)\n",
    "\n",
    "图3-3 深度学习软件和硬件技术栈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 使用Keras开发：快速概述\n",
    "\n",
    "> You’ve already seen one example of a Keras model: our MNIST example. The typical\n",
    "Keras workflow looks just like our example:\n",
    "\n",
    "> - Define your training data: input tensors and target tensors.\n",
    "- Define a network of layers (or model ) that maps your inputs to your targets.\n",
    "- Configure the learning process by picking a loss function, an optimizer, and some metrics\n",
    "to monitor.\n",
    "- Iterate on your training data by calling the fit method of your model.\n",
    "\n",
    "前面你已经看到一个Keras模型的例子：MNIST。典型的Keras流程就像我们的例子那样：\n",
    "\n",
    "- 定义训练数据：输入张量和目标张量。\n",
    "- 定义层组成的网络（或模型），将输入映射成目标。\n",
    "- 通过选择损失函数、优化器和一些监控指标来配置学习过程。\n",
    "- 在你的模型上调用fit方法迭代训练你的数据。\n",
    "\n",
    "> There are two ways to define a model: using the Sequential class (only for linear\n",
    "stacks of layers, which is the most common network architecture by far), and the\n",
    "\"functional API\" (for directed acyclic graphs of layers, allowing to build completely\n",
    "arbitrary architectures).\n",
    "\n",
    "有两种方法定义一个模型：使用Sequential类（仅提供线性层堆叠，也是目前位置最常见的网络结构），和“函数式API”（提供有向无环图结构，允许构建完全任意的结构）。\n",
    "\n",
    "> As a refresher, here’s a two-layer model defined using the Sequential class (note\n",
    "that we are passing the expected shape of the input data to the first layer):\n",
    "\n",
    "作为复习，下面是我们之前见过的使用Sequential类构建的两层模型（注意我们在第一层中传递了期望的输入张量形状）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(784,)))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> And here’s the same model defined using the functional API. With this API, you are\n",
    "manipulating the data tensor that the model processes, and applying layers to this tensor\n",
    "as if they were functions. A detailed guide to what you can with the functional API can\n",
    "be found in Chapter 7. Until Chapter 6, we will only be using the Sequential class in\n",
    "our code examples.\n",
    "\n",
    "下面是使用函数式API定义相同模型的方法。使用这些API，你控制的是模型处理的数据张量，然后将层应用到这些张量上，就好像它们（层）是函数一样。在第七章中会有关于函数式API的详细指引。在那之前，我们都只会使用Sequential类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = layers.Input(shape=(784,))\n",
    "x = layers.Dense(32, activation='relu')(input_tensor)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = models.Model(inputs=input_tensor, outputs=output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Once your model architecture is defined, it doesn’t matter whether you used a\n",
    "Sequential model or the functional API: all following steps are the same.\n",
    "\n",
    "一旦你的模型结构定义好了，无论你使用的是Sequential模型还是函数式API：下面的步骤都是相同的。\n",
    "\n",
    "> The learning process is configured at the \"compilation\" step, where you specify the\n",
    "optimizer and loss function(s) that the model should use, as well as the metrics you want\n",
    "to monitor during training. Here’s an example with a single loss function, by far the most\n",
    "common case:\n",
    "\n",
    "学习过程是在模型的“编译”步骤配置的，你在编译时应该指定模型需要使用的优化器和损失函数，还有训练过程中你希望监视的指标。下面例子使用了单个损失函数，也是目前最常见的情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lastly, the learning process itself consists of passing Numpy arrays of input data (and\n",
    "the corresponding target data) to the model via the fit() method, similarly to what you\n",
    "would do in Scikit-Learn or several other machine learning libraries:\n",
    "\n",
    "最后，学习过程本身值包括传递输入数据的Numpy数组（和相应的目标数据）到模型，者通过fit方法来实现，类似与你在Scikit-Learn或其他一些机器学习库中的做法一样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本例中无训练数据集，因此下面的代码无法运行\n",
    "model.fit(input_tensor, target_tensor, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Over the next few chapters, you will build a solid intuition as to what type of network\n",
    "architectures work for different kinds of problems, how to pick the right learning\n",
    "configuration, and how to tweak a model until it gives you the results you want to see.\n",
    "We’ll start with three basic examples in the next three sections: a two-class classification\n",
    "example, a many-class classification example, and a regression example.\n",
    "\n",
    "后面小节中，你会建立出哪种类型网络结构适合哪种类型问题，如何选择正确的学习参数，以及如何对模型进行调优以达到你希望的结果的直觉。我们后续三个小节会分别讲述三个基本例子：一个二元分类例子，一个多类别分类例子和回归例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 搭建深度学习工作站\n",
    "\n",
    "### 3.3.1 初步考虑\n",
    "\n",
    "> Before you can get started developing deep learning applications, you need to set up your\n",
    "workstation. It is highly recommended, though not strictly necessary, to run deep\n",
    "learning code on a modern NVIDIA GPU. Some applications, in particular image\n",
    "processing with convolutional networks and sequence processing with recurrent neural\n",
    "networks, will be excruciatingly slow on CPU, even with a fast multi-core CPU. And\n",
    "even for applications that can realistically be run on CPU, you would generally observe a\n",
    "5x to 10x speedup by using a modern GPU. If you don’t want to install a GPU on your\n",
    "machine, you could alternatively consider running your experiments on a AWS EC2\n",
    "GPU instance, or on Google Cloud Platform. But note that that cloud GPU instances can\n",
    "get quite expensive over time.\n",
    "\n",
    "在你开始开发深度学习应用之前，你需要搭建你的工作站。虽然不是严格需要，但是依然是高度推荐能够使用现代NVIDIA GPU来运行深度学习代码。一些应用，特别是使用卷积网络进行图像处理和使用循环神经网络处理序列数据，在CPU上运行都会令人难以忍受的慢，即使是用一个高速多核的CPU也是这样。而且即使哪些能够在CPU上运行的应用，在使用GPU时也能获得5倍到10倍的速度提升。如果你不打算在你的计算机上安装GPU，你也可以考虑使用云服务运行你的实验，比方说AWS EC2 GPU或者GCP。但应该注意的是，云端的GPU服务实例在长时间运行时可能会很昂贵。\n",
    "\n",
    "> Also, whether running locally or in the cloud, it is better for you to be using a Unix\n",
    "workstation. While it is technically possible to use Keras on Windows (all three Keras\n",
    "backends support Windows), we don’t recommend it. In the installation instructions we\n",
    "provide as an appendix to this book, we will consider an Ubuntu machine. If you are a\n",
    "Windows user, the simplest solution to get everything running is to set up an Ubuntu dual\n",
    "boot on your machine. It may seem like a hassle, but using Ubuntu will save you a lot of\n",
    "time and a lot of trouble in the long run.\n",
    "\n",
    "还有无论是本地运行还是云端运行，你都应该使用Unix工作站。虽然在Windows上使用Keras在技术上是可行的（所有三种Keras的后端都支持Windows），但是我们不推荐使用。在本书附录中提供的安装指引中，我们推荐的是Ubuntu系统。如果你是一个Windows用户，最简单的解决方案是在你的机器上安装一个Ubuntu系统作为双启动。这样做看起来很麻烦，但是使用Ubuntu能在你长时间运行过程中节省很多的时间和减少很多的麻烦。\n",
    "\n",
    "> Note that in order to use Keras, you need to install Theano, or CTNK, or TensorFlow\n",
    "(or all of them, if you want to be able to switch back and forth between the three\n",
    "backends). In this book, we will focus on TensorFlow, with some light instructions\n",
    "relative to Theano. We will not cover CNTK.\n",
    "\n",
    "还需要提醒的是，要使用Keras，你需要安装Theano或CNTK或TensorFlow（或者全部都安装，如果你希望能够在三种后端之间进行切换的话）。在本书中，我们会聚焦在TensorFlow，其中会涉及Theano的一些指引，但不会设计CNTK。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Jupyter notebooks：运行深度学习实验的推荐工具\n",
    "\n",
    "> Jupyter notebooks are a great way to run deep learning experiments, in particular the\n",
    "many code examples contained in this book. They are widely used in the data science and\n",
    "machine learning community. A Jupyter Notebook is a file generated by the Jupyter app,\n",
    "which you can edit in your browser. It mixes the ability to execute Python code, together\n",
    "with rich text editing capabilities for annotating what you are doing. A Notebook also\n",
    "allows you to break up long experiments into smaller \"cells\" which can be executed\n",
    "independently, which makes development interactive, and means that you don’t have to\n",
    "re-run all of your previous code in case something goes wrong late in an experiment.\n",
    "\n",
    "Jupyter notebooks是非常棒的运行深度学习实验的方法，特别是对于本书里面的许多代码例子来说。它在数据科学和机器学习社区被广泛使用。一个Jupyter Notebook是由Jupyter应用程序产生的文件，它能在浏览器中进行编辑。它兼有在其中执行Python代码和富文本编辑的能力，方便在其中记录你的工作。一个Notebook还支持你将一个很长的实验拆散成为多个小的“单元”来独立执行，既能交互式的进行开发，也意味着当实验的最新步骤发生错误时，你不需要重新运行所有之前代码。\n",
    "\n",
    "> You can find more information about Jupyter at jupyter.org .\n",
    "\n",
    "> We recommend using Jupyter notebooks to get started with Keras, albeit that is not a\n",
    "requirement: you could also be running standalone Python scripts, or you could be\n",
    "running code from within an IDE such as PyCharm. We are making all code examples in\n",
    "this book available as open-source Notebooks, downloadable online.\n",
    "\n",
    "更多Jupyter的内容可以在[Jupyter官网](https://jupyter.org)得到。\n",
    "\n",
    "我们推荐使用Jupyter notebooks来学习Keras，尽管这不是必须的：你也可以在独立的Python脚本中执行代码，或者在诸如PyCharm之类的IDE中执行。我们会将本书所有的代码例子以开源Notebooks的形式放在网上提供下载。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 让Keras跑起来：两种选择\n",
    "\n",
    "> To get started in practice, we recommend one of the following two options:\n",
    "\n",
    "> - Use the official EC2 \"Deep Learning\" AMI and run Keras experiments as Jupyter\n",
    "notebooks on EC2.\n",
    "    - Do this preferably if you do not already have a GPU on your local machine.\n",
    "    - We provide a step by step guide in the appendix \"Running Jupyter notebooks on a EC2 GPU instance\".\n",
    "- Install everything from scratch on a local Unix workstation. You can then either run local Jupyter Notebooks or a regular Python codebase.\n",
    "    - Do this if you already have a high-end NVIDIA GPU.\n",
    "    - We provide a step by step guide in the appendix \"Installing Keras and its dependencies on Unix\".\n",
    "    \n",
    "要真正开始实践，我们推荐下面两种选择之一：\n",
    "\n",
    "- 使用官方的EC2 \"Deep Learning\" AMI然后在EC2上使用Jupyter notebooks运行Keras实验。\n",
    "    - 如果你在本地计算机上没有安装GPU，这是推荐的选择。\n",
    "    - 我们在附录“在EC2 GPU实例上运行Jupyter notebooks”中提供了详细的教程。\n",
    "- 在本地Unix工作站上从0搭建所有东西。你可以使用运行本地的Jupyter notebooks或者运行常规的Python代码脚本。\n",
    "    - 如果你有一个高端的NVIDIA GPU的话，采用这个选择。\n",
    "    - 我们在附录“在Unix上安装Keras和它的依赖”中提供了详细的教程。\n",
    "\n",
    "> Let’s take a closer look at some of the compromises involved in picking one option\n",
    "over the other.\n",
    "\n",
    "我们来看看这两种选择的对比。\n",
    "\n",
    "译者注：事实上，对于初学者来说，现在还有一个更好的选择，那就是[Google的Colab](https://colab.research.google.com/)，一个基于Jupyter notebooks的在线工作站，并且提供了免费的GPU可以使用，是并不宽裕的初学者的最佳选择。当然大陆读者因为不可描述的原因，需要一把优秀的梯子。本翻译的所有notebooks都提供了相应的Colab链接可供使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 云端运行深度学习任务的优缺点\n",
    "\n",
    "> If you don’t already have a GPU that you can use for deep learning (a recent, high-end\n",
    "NVIDIA GPU), then running deep learning experiments in the cloud is a simple,\n",
    "low-cost way for you to get started without having to buy any additional hardware. If you\n",
    "are using Jupyter notebooks, the experience of running in the cloud is no different from\n",
    "running locally. As of mid-2017, the cloud offering that makes it easiest to get started\n",
    "with deep learning is definitely AWS EC2. In appendix, we provide a step-by-step guide\n",
    "to start running Jupyter notebooks on a EC2 GPU instance.\n",
    "\n",
    "如果你还没有可以用来进行深度学习任务的GPU（目前来说就是高端的NVIDIA GPU），那么在云端运行深度学习实验是个比较简单的、省钱的方式来开始入门，因为你无需购买任何额外的硬件。如果你在使用Jupyter notebooks，在云端运行的体验和本地运行并没有区别。在2017年中时候，云端提供深度学习能力让初学者容易入门的服务无疑是AWS EC2。在附录中提供了在EC2 GPU实例上运行Jupyter notebooks的指南。\n",
    "\n",
    "> However, if you are a heavy user of deep learning, this setup is not sustainable in the\n",
    "long term—or even past a few weeks. EC2 instances are expensive: the instance type we\n",
    "recommend in our appendix, the p2.xlarge instance, which will not provide you with\n",
    "much power, alreadys costs $0.90 per hour (as of mid-2017). Meanwhile, a solid consumer-class GPU will cost you somewhere between $1,000 and $1,500—a price that\n",
    "has been fairly stable over time, even as the specs of these GPUs keep improving. If you\n",
    "are serious about deep learning, you should set up a local workstation with one or more\n",
    "GPUs.\n",
    "\n",
    "然而，如果你是深度学习的重度用户，这不是长期合适的选择 - 或者只是超过一个星期的时间就不合适了。EC2实例很贵：在附录中我们推荐的实例类型p2.xlarge实例，并不能为你提供很强大的算力，已经需要每小时0.9美元（在2017年中）。同时，一个实体消费级GPU需要花费大约1000-1500美元 - 即使新的GPU还在发展，这个价格已经基本稳定下来。如果你要投入深度学习，你应该搭建一个或多个GPU组成的工作站。\n",
    "\n",
    "> In short: EC2 is a great way to get started. You could follow the code examples in this\n",
    "book entirely on a EC2 GPU instance. But if you are going to be a power user of deep\n",
    "learning, then get your own GPUs.\n",
    "\n",
    "简单来说：EC2是一个入门的好地方。你可以在EC2 GPU实力上完成本书的所有代码例子。但是如果你要成为一个深度学习的超级用户，那么需要购买自己的GPU。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 哪种GPU最合适？\n",
    "\n",
    "> If you are going to buy a GPU, which one should you choose? The first thing to note it\n",
    "that it would have to be a NVIDIA GPU. NVIDIA is the only graphics computing\n",
    "company to have heavily invested into deep learning so far, and modern deep learning\n",
    "frameworks can only run on NVIDIA cards.\n",
    "\n",
    "如果你打算购买GPU，那么应该选择哪个呢？首先要确保的它必须是一个NVIDIA GPU。NVIDIA是目前唯一大量投入在深度学习领域的图形计算公司，而且现在的深度学习框架智能在NVIDIA显卡上运行。\n",
    "\n",
    "> As of mid-2017, I would recommend the NVIDIA Titan Xp as the best card on the\n",
    "market for deep learning. For lower budgets, you might want to consider the GTX 1060.\n",
    "If you are reading these pages in 2018 or later, do take the time to look online for fresher\n",
    "recommendations, as new models come out every year.\n",
    "\n",
    "在2017年中，作者会推荐NVIDIA Titan Xp作为市面上深度学习使用的最佳显卡。如果预算不够，你可以考虑GTX 1060。如果你是在2018年之后读到本书，请花些时间搜索网上的推荐，因为每年都会推出新的显卡。\n",
    "\n",
    "> From this section onwards, we will assume that you have access to a machine with\n",
    "Keras and its dependencies installed—preferably with GPU support. Make sure you get\n",
    "this step done before you go any further. Go through our step-by-step guides provided in\n",
    "appendix, and look online if you need further help. There is no shortage of tutorials on\n",
    "how to install Keras and common deep learning dependencies.\n",
    "\n",
    "看完前面章节后，我们假设你已经有了一台设备安装好了Keras和它的依赖了 - 最好有GPU支持的。请在往下进行之前确认这一步已经完成。你可以参考附录中的指南，在网上寻找更多帮助，有关安装Keras和通用深度学习依赖的教程在网上汗牛充栋。\n",
    "\n",
    "> We can now start diving into practical Keras examples.\n",
    "\n",
    "我们现在可以进入实际的Keras例子了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 二元分类例子：影评分类\n",
    "\n",
    "> Two-class classification, or binary classification, may be the most widely applied kind of\n",
    "machine learning problem. In this example, we will learn to classify movie reviews into\n",
    "\"positive\" reviews and \"negative\" reviews, just based on the text content of the reviews.\n",
    "\n",
    "二元分类，或叫二分分类，可能是最广泛应用的机器学习问题。在这个例子中，我们会学习基于影评的文本内容将影评数据分成“积极”和“消极”两类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 IMDB数据集\n",
    "\n",
    "> We’ll be working with \"IMDB dataset\", a set of 50,000 highly-polarized reviews from\n",
    "the Internet Movie Database. They are split into 25,000 reviews for training and 25,000\n",
    "reviews for testing, each set consisting in 50% negative and 50% positive reviews.\n",
    "\n",
    "我们会使用“IMDB数据集”，它包含着国际互联网电影数据库（IMDB）中50000个高度具有偏向性的评论。它们被分成25000个训练集和25000个数据集，每个数据集都有50%的消极评论和50%的积极评论组成。\n",
    "\n",
    "> Why do we have these two separate training and test sets? You should never test a\n",
    "machine learning model on the same data that you used to train it! Just because a model\n",
    "performs well on its training data doesn’t mean that it will perform well on data it has\n",
    "never seen, and what you actually care about is your model’s performance on new data\n",
    "(since you already know the labels of your training data—obviously you don not need\n",
    "your model to predict those). For instance, it is possible that your model could end up\n",
    "merely memorizing a mapping between your training samples and their targets—which\n",
    "would be completely useless for the task of predicting targets for data never seen before.\n",
    "We will go over this point in much more detail in the next chapter.\n",
    "\n",
    "为什么我们需要两个独立的训练和测试数据集呢？因为原则就是永远不要使用与训练相同的数据来对模型进行测试！仅仅因为模型在训练数据上表现优异不代表它能在没有见过的数据上也能很好的工作，而实际上你关心的是模型在新数据上的性能（因为你已经知道了训练数据上的标签 - 显然你不需要模型再去预测它们）。例如，你的模型可能仅仅是记住了训练样本和它们目标之间的映射关系 - 这对于预测未知数据的目标任务来说是完全没有作用的。我们会在下一章中更加深入的讨论这一点。\n",
    "\n",
    "> Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) have been turned into\n",
    "sequences of integers, where each integer stands for a specific word in a dictionary.\n",
    "\n",
    "就像MNIST数据集一样，IMDB数据集也內建在Keras中。它已经被预处理过：这些影评（单词的序列）已经被转换为整数的序列，其中的每个整数代表这字典中一个特定的单词。\n",
    "\n",
    "> The following code will load the dataset (when you run it for the first time, about\n",
    "80MB of data will be downloaded to your machine):\n",
    "\n",
    "下面的代码会装载数据集（当你第一次运行是，会下载大约80MB的数据）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "(train_data, train_label), (test_data, test_label) = datasets.imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The argument num_words=10000 means that we will only keep the top 10,000 most\n",
    "frequently occurring words in the training data. Rare words will be discarded. This\n",
    "allows us to work with vector data of manageable size.\n",
    "\n",
    "参数`num_words=10000`表示我们仅保留训练数据中出现频率最高的头10000个单词。生僻单词会被抛弃。这让我们可以使用一个理想大小的矢量来进行实验。\n",
    "\n",
    "> The variables train_data and test_data are lists of reviews, each review being a\n",
    "list of word indices (encoding a sequence of words). train_labels and test_labels\n",
    "are lists of 0s and 1s, where 0 stands for \"negative\" and 1 stands for \"positive\":\n",
    "\n",
    "变量`train_data`和`test_data`是影评的列表，每个评论是一个单词序号的列表（编码成单词的序列）。`train_label`和`test_label`是一个由0和1组成的列表，这里的0代表消极，1代表积极："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since we restricted ourselves to the top 10,000 most frequent words, no word index\n",
    "will exceed 10,000:\n",
    "\n",
    "因为我们将训练数据限制在频率最高的10000个单词上，因此没有单词的序号会超过10000："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For kicks, here’s how you can quickly decode one of these reviews back to English\n",
    "words:\n",
    "\n",
    "为了更加清晰，下面是我们将一条评论解码回到英文单词的方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_index是将单词映射成整数的字典\n",
    "word_index = datasets.imdb.get_word_index()\n",
    "# 将其中的KV反过来，形成一个将整数映射回单词的字典\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# 解码评论内容，注意序号从3开始，因为0，1，2是保留的，分别代表“对齐”，“序列开始”和“未知”\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
    "decoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 数据准备\n",
    "\n",
    "> We cannot feed lists of integers into a neural network. We have to turn our lists into\n",
    "tensors. There are two ways we could do that:\n",
    "\n",
    "> - We could pad our lists so that they all have the same length, and turn them into an integer\n",
    "tensor of shape (samples, word_indices) , then use as first layer in our network a\n",
    "layer capable of handling such integer tensors (the Embedding layer, which we will cover\n",
    "in detail later in the book).\n",
    "- We could one-hot-encode our lists to turn them into vectors of 0s and 1s. Concretely, this\n",
    "would mean for instance turning the sequence [3, 5] into a 10,000-dimensional vector\n",
    "that would be all-zeros except for indices 3 and 5, which would be ones. Then we could\n",
    "use as first layer in our network a Dense layer, capable of handling floating point vector\n",
    "data.\n",
    "\n",
    "我们不能直接将一个整数列表放入神经网络。我们将列表转换成张量。有两种方式来处理：\n",
    "\n",
    "- 我们对列表进行填充，让它们有相同的长度，然后将它转换成一个形状为(样本, 单词序号)的张量，然后在我们的网络第一层中应用一个能够处理这样的整数张量的层（被称为嵌入层，我们在本书后面会详细介绍）。\n",
    "- 我们也可以使用one-hot编码将列表编程一个仅含有0和1的矢量。具体来说，例如我们有一个序列`[3, 5]`，我们可以将它转换成一个10000维度的矢量，矢量上面除了序号3和5的位置为1外，其余都是0。这样我们就可以在网络中使用全连接层作为第一层，用来正确处理浮点矢量数据。\n",
    "\n",
    "> We will go with the latter solution. Let’s vectorize our data, which we will do\n",
    "manually for maximum clarity:\n",
    "\n",
    "下面我们采用第二种方法。将数据矢量化，这里为了清晰起见，我们采用手动实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # 创建一个全0矩阵，形状是((len(sequences), dimension))\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1. # 设置相应的位置为1\n",
    "    return results\n",
    "\n",
    "# 获得矢量化的训练数据\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# 获得矢量化的测试数据\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矢量化后训练数据看起来如下\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We should also vectorize our labels, which is straightforward:\n",
    "\n",
    "我们也需要矢量化标签，很直接的使用类型转换就行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_label).astype('float32')\n",
    "y_test = np.asarray(test_label).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now our data is ready to be fed into a neural network.\n",
    "\n",
    "现在数据已经准备好了，可以输入到神经网络中了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 构建我们的网络\n",
    "\n",
    "> Our input data is simply vectors, and our labels are scalars (1s and 0s): this is the easiest\n",
    "setup you will ever encounter. A type of network that performs well on such a problem\n",
    "would be a simple stack of fully-connected ( Dense ) layers with relu activations:\n",
    "Dense(16, activation='relu')\n",
    "\n",
    "输入数据是简单的矢量，标签是标量（1和0）：这是你将会遇到的最简单的情况。有一种网络能在这种情况下运行良好，那就是简单将全连接（Dense）层堆叠起来使用relu作为激活函数的网络：`Dense(16, activation='relu')`\n",
    "\n",
    "> The argument being passed to each Dense layer (16) is the number of \"hidden units\"\n",
    "of the layer. What’s a hidden unit? It’s a dimension in the representation space of the\n",
    "layer. You may remember from the previous chapter that each such Dense layer with a\n",
    "relu activation implements the following chain of tensor operations:\n",
    "\n",
    "传递给全连接层的参数（16）是该层当中“隐藏单元”的数量。什么是隐藏单元？它是该层表示空间的维度。你可能还记得前一章中我们介绍过这样使用relu激活函数的全连接层实现了下面的链式张量操作：\n",
    "$$output = relu(W \\cdot input + b)$$\n",
    "\n",
    "> Having 16 hidden units means that the weight matrix W will have shape\n",
    "(input_dimension, 16) , i.e. the dot product with W will project the input data onto a\n",
    "16-dimensional representation space (and then we would add the bias vector b and apply\n",
    "the relu operation). You can intuitively understand the dimensionality of your\n",
    "representation space as \"how much freedom you are allowing the network to have when\n",
    "learning internal representations\". Having more hidden units (a higher-dimensional\n",
    "representation space) allows your network to learn more complex representations, but it\n",
    "makes your network more computationally expensive and may lead to learning unwanted\n",
    "patterns (patterns that will improve performance on the training data but not on the test\n",
    "data).\n",
    "\n",
    "拥有16个隐藏单元表示这里的权重矩阵W的形状是(输入维度, 16)，也就是说，W和输入数据的点积其实就是W会将输入数据投射到一个16维的表示空间上（然后我们会加上偏差和应用relu操作）。你可以将表示空间的维度直观的理解为“当网络学习内部表示时有多少的自由度”。有更多隐藏单元（更高的表示空间维度）允许你的网络能够学习更加复杂的表示，但它会让网络更加消耗计算资源和可能导致学习到不需要的模式（这些模式会改善模型在训练数据上的性能但不会改善在测试数据上的性能）。\n",
    "\n",
    "> There are two key architecture decisions to be made about such stack of dense layers:\n",
    "\n",
    "> - How many layers to use.\n",
    "- How many \"hidden units\" to chose for each layer.\n",
    "\n",
    "对于全连接层的堆叠结构来说有两个关键的决定：\n",
    "\n",
    "- 要使用的层数。\n",
    "- 每一层要选择的“隐藏单元”。\n",
    "\n",
    "> In the next chapter, you will learn formal principles to guide you in making these\n",
    "choices. For the time being, you will have to trust us with the following architecture\n",
    "choice: two intermediate layers with 16 hidden units each, and a third layer which will\n",
    "output the scalar prediction regarding the sentiment of the current review. The\n",
    "intermediate layers will use relu as their \"activation function\", and the final layer will\n",
    "use a sigmoid activation so as to output a probability (a score between 0 and 1, indicating\n",
    "how likely the sample is to have the target \"1\", i.e. how likely the review is to be\n",
    "positive). A relu (rectified linear unit) is a function meant to zero-out negative values\n",
    "(see Figure 3.4) while a sigmoid \"squashes\" arbitrary values into the [0, 1] interval,\n",
    "(see Figure 3.5), thus outputting something that can be interpreted as a probability.\n",
    "\n",
    "在下一章中，你会学习到做上述选择的正式原则。现在，你只需要相信我们选择了一下的结构：两个中间层，每层有16个隐藏单元，第三层会根据当前评论的情绪输出标量预测结果。这两个中间层会使用relu作为它们的“激活函数”，最终层会使用sigmoid激活函数让它能够输出一个概率（0到1之间的分值，标志这个样本有多大概率为“1”，也就是这条评论有多大概率是积极的）。relu（整流线性单元）是一个函数会将所有负值变成0（参见图3-4），sigmoid将任意值压缩到`[0, 1]`区间（参见图3-5），因此可以输出概率。\n",
    "\n",
    "![relu](imgs/f3.4.jpg)\n",
    "\n",
    "图3-4 整流线性单元函数\n",
    "\n",
    "![sigmoid](imgs/f3.5.jpg)\n",
    "\n",
    "图3-5 sigmoid函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here’s what our network looks like:\n",
    "\n",
    "下图是我们网络的结构：\n",
    "\n",
    "![arch of network](imgs/f3.6.jpg)\n",
    "\n",
    "图3-6 我们的3层网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> And here’s the Keras implementation, very similar to the MNIST example you saw\n",
    "previously:\n",
    "\n",
    "下面是Keras的实现，这些代码与前面看到的MNIST例子非常相似："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注：什么是激活函数，为什么我们需要它？\n",
    "\n",
    "> Without an activation function like relu (also called a non-linearity ), our\n",
    "Dense layer would consist of two linear operations, a dot product and an\n",
    "addition.\n",
    "output = dot(W, input) + b\n",
    "\n",
    "如果没有诸如relu这样的激活函数（也被成为非线性），我们的全连接层仅包括两个线性操作，一个点积和一个加法。\n",
    "$$output = W \\cdot input + b$$\n",
    "\n",
    "> So the layer could only learn linear transformations (affine\n",
    "transformations) of the input data, i.e. the hypothesis space of the layer\n",
    "would be the set of all possible linear transformations of the input data\n",
    "into a 16-dimensional space. Such an hypothesis space is too restricted,\n",
    "and wouldn’t benefit from multiple layers of representations, because a\n",
    "deep stack of linear layers would still implement a linear operation: adding\n",
    "more layers wouldn’t extend the hypothesis space.\n",
    "\n",
    "因此这个层智能学习到输入数据的线性变换（也叫仿射变换），也就是说层的16维假设空间只是一组输入数据的所有可能的线性变换的组合。这样的假设空间的局限性非常大，并且也无法因为层次的增加而改善，因为一个很深的线性层堆叠依旧智能实现线性操作：增加更多的层不能扩展假设空间。\n",
    "\n",
    "> Lastly, we need to pick a loss function and an optimizer. Since we are facing a binary\n",
    "classification problem and the output of our network is a probability (we end our network\n",
    "with a single-unit layer with a sigmoid activation), is it best to use the\n",
    "binary_crossentropy loss. It isn’t the only viable choice: you could use, for instance,\n",
    "mean_squared_error . But crossentropy is usually the best choice when you are dealing\n",
    "with models that output probabilities. Crossentropy is a quantity from the field of\n",
    "Information Theory, that measures the \"distance\" between probability distributions, or in\n",
    "our case, between the ground-truth distribution and our predictions.\n",
    "\n",
    "最后，我们需要选取损失函数和优化器。因为我们面对的是二元分类问题，且网络的输出是一个概率（我们网络最终是只有一个单元和使用sigmoid激活函数的层），因此最好使用`binary_crossentropy`损失函数。但这不是唯一可行的选择：你可以使用例如`mean_sqaure_err`。但是交叉熵通常是当你处理模型的输出是概率的情况下的最佳选择。交叉熵是信息论领域的一个量，它量度概率分布之间的“距离”，或者在我们这个例子中，就是真实分布和我们预测之间的距离。\n",
    "\n",
    "> Here’s the step where we configure our model with the rmsprop optimizer and the\n",
    "binary_crossentropy loss function. Note that we will also monitor accuracy during\n",
    "training.\n",
    "\n",
    "下面的步骤用来配置我们网络使用`rmsprop`优化器和`binary_crossentropy`损失函数。注意我们还配置了在训练过程中监视准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We are passing our optimizer, loss function and metrics as strings, which is possible\n",
    "because rmsprop , binary_crossentropy and accuracy are packaged as part of Keras.\n",
    "Sometimes you may want to configure the parameters of your optimizer, or pass a\n",
    "custom loss function or metric function. This former can be done by passing an optimizer\n",
    "class instance as the optimizer argument:\n",
    "\n",
    "在这里我们使用字符串形式传递了优化器，损失函数和指标参数，这是因为`rmsprop`、`binary_crossentropy`和`accuracy`被打包成了Keras的一部分。有时你可能需要定义优化器的参数，或者传递一个自定义的损失函数或者指标函数。前者可以通过传递一个`optimizer`类的实例来作为优化器参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The latter can be done by passing function objects as the loss or metrics arguments:\n",
    "\n",
    "后者可以通过传递函数对象到loss或metrics参数上实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses, metrics\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss=losses.binary_crossentropy,\n",
    "              metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4 验证模型\n",
    "\n",
    "> In order to monitor during training the accuracy of the model on data that it has never\n",
    "seen before, we will create a \"validation set\" by setting apart 10,000 samples from the\n",
    "original training data:\n",
    "\n",
    "为了能够在训练过程中监视模型在它没见过的数据上的准确率，我们会创建一个“验证集”，通过从原始训练数据中抽取10000个样本来获得："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We will now train our model for 20 epochs (20 iterations over all samples in the\n",
    "x_train and y_train tensors), in mini-batches of 512 samples. At this same time we\n",
    "will monitor loss and accuracy on the 10,000 samples that we set apart. This is done by\n",
    "passing the validation data as the validation_data argument:\n",
    "\n",
    "我们现在开始对我们的模型进行20 epochs的训练（在训练集所有样本上进行20次的迭代），使用的批次大小是512个样本。与此同时我们会在刚才分出来的10000个样本的验证集上监视损失和准确率。这是通过将验证集传递到`validation_data`参数来实现的：\n",
    "\n",
    "译者注：因本人笔记本并不高端，因此内存不足以跑下面的训练过程，因此无法执行下面代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train, partial_y_train,\n",
    "                    epochs=20, batch_size=256,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On CPU, this will take less than two seconds per epoch—training is over in 20\n",
    "seconds. At the end of every epoch, there is a slight pause as the model computes its loss\n",
    "and accuracy on the 10,000 samples of the validation data.\n",
    "\n",
    "在CPU上这个训练每次epoch会花费少于两秒的时间 - 训练会在约20秒完成。在每个epoch的结束，会有一个轻微的停顿，因为模型需要在验证数据的10000个样本上计算它的损失和准确率。\n",
    "\n",
    "> Note that the call to model.fit() returns a History object. This object has a\n",
    "member history , which is a dictionary containing data about everything that happened\n",
    "during training. Let’s take a look at it:\n",
    "\n",
    "注意调用`model.fit()`会返回一个History对象。这个对象有一个成员history，是一个包含着几乎在训练过程中发生的所有事情的字典。让我们来看一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history.history.keys()\n",
    "\n",
    "# 会输出：dict_keys(['loss', 'binary_accuracy', 'val_loss', 'val_binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It contains 4 entries: one per metric that was being monitored, during training and\n",
    "during validation. Let’s use Matplotlib to plot the training and validation loss side by\n",
    "side, as well as the training and validation accuracy:\n",
    "\n",
    "它包含四个项目：每个对应着一个在训练或验证过程中被监视的指标。让我们使用Matplotlib来绘制训练和验证损失，以及训练和验证准确率：\n",
    "\n",
    "译者注：以下是在Colab执行的代码片段和结果\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['binary_accuracy']\n",
    "val_acc = history.history['val_binary_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "# \"bo\" 训练损失绘制成蓝色原点\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# \"b\" 验证损失绘制成蓝色线\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "```\n",
    "\n",
    "![training validation loss](imgs/f3.7.png)\n",
    "\n",
    "图3-7 训练和验证损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['binary_accuracy']\n",
    "val_acc = history.history['val_binary_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "# \"bo\" 训练损失绘制成蓝色原点\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# \"b\" 验证损失绘制成蓝色线\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "```\n",
    "\n",
    "![training validation accuracy](imgs/f3.8.png)\n",
    "\n",
    "图3-8 训练和验证准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The dots are the training loss and accuracy, while the solid lines are the validation\n",
    "loss and accuracy. Note that your own results may vary slightly due to a different random\n",
    "initialization of your network.\n",
    "\n",
    "蓝色的点是训练损失和准确率，而蓝色的实线是验证损失和准确率。注意你自己运行的结果可能会由于网络随机初始化而产生一定差别。\n",
    "\n",
    "> As you can see, the training loss decreases with every epoch and the training accuracy\n",
    "increases with every epoch. That’s what you would expect when running gradient\n",
    "descent optimization—the quantity you are trying to minimize should get lower with\n",
    "every iteration. But that isn’t the case for the validation loss and accuracy: they seem to\n",
    "peak at the fourth epoch. This is an example of what we were warning against earlier: a\n",
    "model that performs better on the training data isn’t necessarily a model that will do\n",
    "better on data it has never seen before. In precise terms, what you are seeing is\n",
    "\"overfitting\": after the second epoch, we are over-optimizing on the training data, and we\n",
    "ended up learning representations that are specific to the training data and do not\n",
    "generalize to data outside of the training set.\n",
    "\n",
    "正如你在上面的图形可以看到，训练损失在每个epoch过后都会下降且训练准确率在每个epoch过后都会上升。这是当你运行梯度下降优化时可以预期的 - 你试图最小化的值在每次迭代后都会更低。但是对于验证损失和准确率来说这就不一样了：它们看起来在第四次epoch之后达到了极值。这个例子印证了我们之前警告过的内容：一个在训练数据上运行良好的模型并不代表它能在未知数据上很好工作。用更精确的术语来描述的话，你这里看到的是“过拟合”：第二次epoch之后，我们过度优化了训练数据，我们最终学习到了训练数据上特定的表现形式，而且无法泛化到训练集之外的数据上。\n",
    "\n",
    "> In this case, to prevent overfitting, we could simply stop training after three epochs.\n",
    "In general, there is a range of techniques you can leverage to mitigate overfitting, which\n",
    "we will cover in the next chapter.\n",
    "\n",
    "在这个例子中，我们可以简单在在三次epochs之后停止训练来防止过拟合。更通常的情况下，我们有许多的技术来减少过拟合的发生，它们将在下一章中介绍。\n",
    "\n",
    "> Let’s train a new network from scratch for four epochs, then evaluate it on our test\n",
    "data:\n",
    "\n",
    "让我们重新训练这个模型，只进行4个epochs，然后在我们的测试集上去评估模型的性能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "loss='binary_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Our fairly naive approach achieves an accuracy of 88%. With state-of-the-art\n",
    "approaches, one should be able to get close to 95%.\n",
    "\n",
    "我们这个很原始的方案取得了88%的准确率。使用一系列的改进，这个准确率可以接近95%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.5 使用训练好的网络生成新数据的预测\n",
    "\n",
    "> After having trained a network, you will want to use it in a practical setting. You can\n",
    "generate the likelihood of reviews being positive by using the predict method:\n",
    "\n",
    "有了训练好的网络之后，你会想将它使用在实际环境中。你可以调用predict方法来生成评论是积极的似然值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As you can see, the network is very confident for some samples (0.99 or more, or\n",
    "0.01 or less) but less confident for others (0.6, 0.4).\n",
    "\n",
    "你可以看到，网络在某些样本上很确定（0.99或更高，或者0.01或更低），但是在其他一些样本上就不太确定（0.6，0.4）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.6 更多实验\n",
    "\n",
    "> - We were using 2 hidden layers. Try to use 1 or 3 hidden layers and see how it affects\n",
    "validation and test accuracy.\n",
    "- Try to use layers with more hidden units or less hidden units: 32 units, 64 units...\n",
    "- Try to use the mse loss function instead of binary_crossentropy .\n",
    "- Try to use the tanh activation (an activation that was popular in the early days of neural\n",
    "networks) instead of relu .\n",
    "\n",
    "- 我们使用了2个隐藏层。尝试使用1个或3个隐藏层，看看对验证和测试准确率有什么影响。\n",
    "- 尝试在层上使用更多的隐藏单元或更少的隐藏单元：32个，64个等等。\n",
    "- 尝试使用`mse`损失函数而不是`binary_crossentropy`。\n",
    "- 尝试使用`tanh`激活函数（这是一个神经网络早起很流行的激活函数）而不是`relu`。\n",
    "\n",
    "> These experiments will help convince you that the architecture choices we have made are\n",
    "all fairly reasonable, although they can still be improved!\n",
    "\n",
    "这些实验会令你确信我们前面结构的选择已经是很合理的，虽然它还可以有改进的地方！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.7 总结\n",
    "\n",
    "> Here’s what you should take away from this example:\n",
    "\n",
    "> - There’s usually quite a bit of preprocessing you need to do on your raw data in order to\n",
    "be able to feed it—as tensors—into a neural network. In the case of sequences of words,\n",
    "they can be encoded as binary vectors—but there are other encoding options too.\n",
    "- Stacks of Dense layers with relu activations can solve a wide range of problems\n",
    "(including sentiment classification), and you will likely use them frequently.\n",
    "- In a binary classification problem (two output classes), your network should end with a\n",
    "Dense layer with 1 unit and a sigmoid activation, i.e. the output of your network should\n",
    "be a scalar between 0 and 1, encoding a probability.\n",
    "- With such a scalar sigmoid output, on a binary classification problem, the loss function\n",
    "you should use is binary_crossentropy .\n",
    "- The rmsprop optimizer is generally a good enough choice of optimizer, whatever your\n",
    "problem. That’s one less thing for you to worry about.\n",
    "- As they get better on their training data, neural networks eventually start overfitting and\n",
    "end up obtaining increasingly worse results on data never-seen-before. Make sure to\n",
    "always monitor performance on data that is outside of the training set.\n",
    "\n",
    "这个例子中你应该学习到下面的这些点：\n",
    "\n",
    "- 你可能需要在原始数据上进行很多的预处理，才能使得这些数据能够以张量的形式输入到神经网络当中。在单词序列的场景中，它们被编码成二元矢量 - 不过也有其他的编码选择。\n",
    "- 使用relu激活的全连接层的堆叠可以解决很广泛的问题（包括情绪分类），你可能会经常使用它们。\n",
    "- 在一个二元分类问题中（两个输出类别），你的网络最后一层会是只有一个单元和sigmoid激活的全连接层，也就是网络的输出应该是一个0到1之间的标量，代表着概率。\n",
    "- 在一个二元分类问题中使用了这样的标量sigmoid输出，你应该使用`binary_crossentropy`作为损失函数。\n",
    "- `rmsprop`优化器通常来说都是一个好的选择，无论是怎样的问题。这是你需要关心最少的方面。\n",
    "- 当模型在训练数据上获得越来越好的结果时，神经网络最终会开始过拟合，并且在未知数据上会获得越来越差的结果。确保你一直监视着模型在训练集之外的数据上的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 多类别分类器例子：分类新闻专线\n",
    "\n",
    "> In the previous section we saw how to classify vector inputs into two mutually exclusive\n",
    "classes using a densely-connected neural network. But what happens when you have\n",
    "more than two classes?\n",
    "\n",
    "在上一节中我们看到如何使用全连接神经网络将矢量输入分到两个互斥的类别当中。但当有着多于两个类的时候会怎么样？\n",
    "\n",
    "> In this section, we will build a network to classify Reuters newswires into 46 different\n",
    "mutually-exclusive topics. Since we have many classes, this problem is an instance of\n",
    "\"multi-class classification\", and since each data point should be classified into only one\n",
    "category, the problem is more specifically an instance of \"single-label, multi-class\n",
    "classification\". If each data point could have belonged to multiple categories (in our case,\n",
    "topics) then we would be facing a \"multi-label, multi-class classification\" problem.\n",
    "\n",
    "在本节中，我们会构建一个网络来将路透社新闻专线分成46个不同的主题中。因为有多个类别，这个问题就是一个“多类别分类”的例子，而且因为每个数据点都应该被分到唯一的类别中，这个问题应该更明确的说是“单标签，多类别分类”。如果每个数据点能够从属于多个类别（在我们的例子中就是主题）那么我们面对的就是“多标签，多类别分类”问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 路透社数据集\n",
    "\n",
    "> We will be working with the Reuters dataset , a set of short newswires and their topics,\n",
    "published by Reuters in 1986. It’s a very simple, widely used toy dataset for text\n",
    "classification. There are 46 different topics; some topics are more represented than\n",
    "others, but each topic has at least 10 examples in the training set.\n",
    "\n",
    "我们将要使用路透社数据集来完成这个例子，这是一个由短通讯和它们的主题构成的数据集，由路透社在1986年发表。这是一个在文本分类当中广泛使用的简单数据集。它有46个不同主题；有些主题比其他的要丰富一些，但是训练集的每个主题中至少有10个样本。\n",
    "\n",
    "> Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let’s\n",
    "take a look right away:\n",
    "\n",
    "如同IMDB和MNIST，路透社数据集也被內建在Keras中。我们下面来看一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Like with the IMDB dataset, the argument num_words=10000 restricts the data to the\n",
    "10,000 most frequently occurring words found in the data.\n",
    "\n",
    "> We have 8,982 training examples and 2,246 test examples:\n",
    "\n",
    "就像IMDB数据集，参数`num_words=10000`将数据限制在数据中前10000个出现频率最高的单词内。\n",
    "\n",
    "我们有了8982个训练样本和2246个测试样本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2246,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As with the IMDB reviews, each example is a list of integers (word indices):\n",
    "\n",
    "每个样本是一个整数列表（单词序号列表），与IMDB评论一样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 245,\n",
       " 273,\n",
       " 207,\n",
       " 156,\n",
       " 53,\n",
       " 74,\n",
       " 160,\n",
       " 26,\n",
       " 14,\n",
       " 46,\n",
       " 296,\n",
       " 26,\n",
       " 39,\n",
       " 74,\n",
       " 2979,\n",
       " 3554,\n",
       " 14,\n",
       " 46,\n",
       " 4689,\n",
       " 4329,\n",
       " 86,\n",
       " 61,\n",
       " 3499,\n",
       " 4795,\n",
       " 14,\n",
       " 61,\n",
       " 451,\n",
       " 4329,\n",
       " 17,\n",
       " 12]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here’s how you can decode it back to words, in case you are curious:\n",
    "\n",
    "下面是将列表解码回单词的方法，如果你感兴趣的话："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The label associated with an example is an integer between 0 and 45: a topic index.\n",
    "\n",
    "一个样本对应的标签是一个介于0到45的整数：对应着主题的序号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 数据准备\n",
    "\n",
    "> We can vectorize the data with the exact same code as in our previous example:\n",
    "\n",
    "我们可以使用与上例中完全一样的代码对数据进行矢量化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To vectorize the labels, there are two possibilities: we could just cast the label list as\n",
    "an integer tensor, or we could use a \"one-hot\" encoding. One-hot encoding is a widely\n",
    "used format for categorical data, also called \"categorical encoding\". For a more detailed\n",
    "explanation of one-hot encoding, you can refer to Chapter 6, Section 1. In our case,\n",
    "one-hot encoding of our labels consists in embedding each label as an all-zero vector\n",
    "with a 1 in the place of the label index, e.g.:\n",
    "\n",
    "要将标签矢量化，有两种做法：我们可以简单的将变迁列表转换为整数张量，或者我们可以使用“one-hot”编码。One-hot编码在分类数据中是广泛使用的，因此也被称为“分类编码”。你可以参考第六章第1节的内容，学习有关one-hot编码更详细的说明。在我们的例子中，标签的one-hot编码是一个全零的矢量，除了标签所在位置为1之外，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimensions=46):\n",
    "    result = np.zeros((len(labels), dimensions))\n",
    "    for i, label in enumerate(labels):\n",
    "        result[i, label] = 1\n",
    "    return result\n",
    "\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that there is a built-in way to do this in Keras, which you have already seen in\n",
    "action in our MNIST example:\n",
    "\n",
    "注意在Keras中有內建的函数能够完成这项任务，你已经在MNIST例子中见过它了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils\n",
    "\n",
    "one_hot_train_labels = utils.to_categorical(train_labels)\n",
    "one_hot_test_labels = utils.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 构建网络\n",
    "\n",
    "> This topic classification problem looks very similar to our previous movie review\n",
    "classification problem: in both cases, we are trying to classify short snippets of text.\n",
    "There is however a new constraint here: the number of output classes has gone from 2 to\n",
    "46, i.e. the dimensionality of the output space is much larger.\n",
    "\n",
    "这个主题分类问题看起来非常像前面的那个影评分类问题：两种情况中，我们都是土对简短的文本片段进行分类。但是这里有一个新的限制：输出类别的数量从2增加到了46，也就是说这个问题的输出空间在维度上远超上一个例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<< [第二章：开始之前：神经网络的数学知识](Chapter2-Mathematical-blocks-of-neural-networks.ipynb) || [目录](index.md) || [第四章：机器学习基础](Chapter4-Fundamentals-of-machine-learning.ipynb) >>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
