{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<< [第三章：进入神经网络](Chapter3_Getting_started_with_neural_networks.ipynb) || [目录](index.md) || [第五章：计算机视觉中的深度学习](Chapter5_Deep_learning_for_computer_vision.ipynb) >>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四章：机器学习基础 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After three practical examples, you are starting to get familiar with how to approach\n",
    "classification and regression problems using neural networks, and you have witnessed the\n",
    "central problem of machine learning: overfitting. This chapter will formalize some of the\n",
    "intuition you are starting to form into a solid conceptual framework for attacking and\n",
    "solving deep learning problems.\n",
    "\n",
    "经过了三个实践性的例子之后，你已经开始熟悉使用神经网络方法来解决分类和回归问题了，并且还亲身感受了机器学习中的核心问题：过拟合。本章会将你前面获得的感性认知转化为脑中扎实的概念性框架，来挑战和解决深度学习问题。\n",
    "\n",
    "> In this chapter, you will:\n",
    "\n",
    "> - Learn about more forms of machine learning, beyond classification and regression.\n",
    "- Learn about formal evaluation procedures for machine learning models, a simple version\n",
    "of which you have already seen in action a few times.\n",
    "- Learn how to prepare data for deep learning, and what is \"feature engineering\".\n",
    "- Learn ways to tackle the central problem of machine learning: overfitting, which we\n",
    "faced in all of our three previous examples.\n",
    "\n",
    "本章中，我们将：\n",
    "\n",
    "- 了解机器学习的更多形式，除了分类和回归之外。\n",
    "- 学习机器学习模型的正规验证方法，前面我们已经多次实际遇到过这个概念。\n",
    "- 学习如何在深度学习中准备数据，以及什么叫做“特征工程”。\n",
    "- 学习解决机器学习核心问题的方法：过拟合，这个问题在前面三个例子中我们都已经接触过。\n",
    "\n",
    "> Finally, we will consolidate all these concepts—model evaluation, data preprocessing\n",
    "and feature engineering, tackling overfitting—into a detailed 7-step workflow for\n",
    "tackling any machine learning problem.\n",
    "\n",
    "最后，我们会将所有这些概念 - 模型验证、数据预处理和特征工程、解决过拟合 - 总结成一个详细的7步工作流，用来解决所有的机器学习问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 机器学习的四个类别\n",
    "\n",
    "> Throughout our previous examples, you’ve become familiar with three specific types of\n",
    "machine learning problems: binary classification, multi-class classification, and scalar\n",
    "regression. All three are instances of \"supervised learning\", where the goal is to learn the\n",
    "relationship between training inputs and training targets.\n",
    "\n",
    "在前面的例子中，你已经熟悉了三种特定的机器学习问题：二分分类、多类别分类和标量回归。所有这三种问题都是“有监督学习”的例子，其目标是让模型能够学习获得训练输入和训练目标之间的关联关系。\n",
    "\n",
    "> Supervised learning is just the tip of the iceberg. Machine learning is a vast field with\n",
    "a complex subfield taxonomy. Machine learning algorithms generally fall into four broad\n",
    "categories:\n",
    "\n",
    "有监督学习仅仅是冰山一角。机器学习实际上是一个包含了很多复杂子范围的广泛领域。机器学习算法通常可以分为下面四个类别："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 有监督学习\n",
    "\n",
    "> This is by far the most common case. It consists of learning to map input data to known\n",
    "targets (also called annotations), given a set of examples (often annotated by humans).\n",
    "All four examples you’ve encountered in this book so far were canonical examples of\n",
    "supervised learning. Generally, almost all applications of deep learning that are getting\n",
    "the spotlight these days belong in this category, such as optical character recognition,\n",
    "speech recognition, image classification or language translation.\n",
    "\n",
    "这是目前为止最常见的场景。它学习从数据数据到已知目标（也称为标记）之间的映射关系，通过在给定一组样本（通常由人来进行标记）的基础上进行。本书中前面介绍的所有四个例子都是有监督学习。更普遍来说，进来深度学习领域受到关注的应用基本上也都是这个类别，例如光学字母辨识、语音识别、图像分类和机器翻译。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 无监督学习\n",
    "\n",
    "> This one consists of finding interesting transformations of the input data without the help\n",
    "of any targets, for the purposes of data visualization, data compression, data denoising...\n",
    "or simply to better understand the correlations present in the data at hand. Unsupervised\n",
    "learning is the bread and butter of \"data analytics\", and is often a necessary step in better\n",
    "understanding a dataset before attempting to solve a supervised learning problem.\n",
    "\"Dimensionality reduction\" and \"clustering\" are well-known categories of unsupervised\n",
    "learning.\n",
    "\n",
    "这个类别包含着从输入数据中找到有趣的转换形式，其中不需要任何目标的帮助，用来进行数据可视化、数据压缩、数据去噪......或者仅仅只是为了更好的理解手头数据的内在关联。无监督学习是“数据分析”的黄油加面包，并且经常是在试图解决有监督学习问题之前，用来更好理解数据集的必须步骤。“降维”和“聚类”是无监督学习中广为人知的类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 自监督学习\n",
    "\n",
    "> This is actually a specific instance of supervised learning, but it different enough that it\n",
    "deserves its own category. Self-supervised learning is supervised learning without\n",
    "human-annotated labels. There are still labels involved (since the learning has to be\n",
    "supervised by something), but they are generated from the input data itself, typically\n",
    "using a heuristic algorithm. You can think of it as supervised learning without any\n",
    "humans in the loop. For instance, \"autoencoders\" are a well-known instance of\n",
    "self-supervised learning, where the generated targets are... the input themselves,\n",
    "unmodified. In the same way, trying to predict the next frame in a video given past\n",
    "frames, or the next word in a text given previous words, would be another instance of\n",
    "self-supervised learning (temporally supervised learning, in this case: supervision comes\n",
    "from future input data). Note that the distinction between supervised, self-supervised and\n",
    "unsupervised learning can be blurry sometimes—these categories are more of continuum\n",
    "without solid frontiers. Self-supervised learning can be reinterpreted as either supervised\n",
    "or unsupervised learning depending on whether you pay attention to the learning\n",
    "mechanism or to the context of its application.\n",
    "\n",
    "这实际上是有监督学习中的一个特定种类，但它又很特殊足以自立山头。自监督学习是不需要人工标记的有监督学习。虽然它还是需要标签（因为学习必须被某些指标监督），但是标签能够从输入数据中自动产生，通常使用的是启发性算法来生成。你可以将它想象成在整个循环中不需要人类参与的有监督学习。例如，“自动编码器”就是一种自监督学习，它能自动产生目标...与输入一模一样的目标。同样的，在视频中根据前面帧的内容预测下一帧，或者在文本中根据前面的单词预测下一个单词，也是自监督学习的例子（瞬时性有监督学习，这里的监督来自未来的输入数据）。注意有监督学习、自监督学习和无监督学习可能会是很模糊的 - 这些类别更像是连续的而非有着明确边界的。自监督学习可以被重新解读为有监督学习或者无监督学习，取决于你着重研究的是学习的机制还是应用的上下文方面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 强化学习\n",
    "\n",
    "> Long overlooked, this branch of machine learning has recently started getting a lot of\n",
    "attention, after Google DeepMind successfully applied it to learning to play Atari games\n",
    "(and later, to learning to play Go at the highest level). In reinforcement learning, an\n",
    "\"agent\" receives information about its environment and learns to pick actions that will\n",
    "maximize some reward. For instance, a neural network that \"looks\" at a video game\n",
    "screen and outputs game actions in order to maximize its score can be trained via\n",
    "reinforcement learning. Currently, reinforcement learning is mostly a research area and\n",
    "has not yet had significant practical successes beyond games. In time, however, I would\n",
    "expect to see reinforcement learning take over an increasingly large range of real-world\n",
    "applications—self-driving, robotics, resource management, education... It is an idea\n",
    "whose time has come, or will come soon.\n",
    "\n",
    "这种机器学习类别曾经长期被忽视，最近开始得到了相当多的关注，特别是自从Google DeepMind将其成功的应用到了计算机学习进行Atari游戏（还有就是后来学习成为了超人类的围棋算法）之后。在强化学习中，存在一个“agent”用来从环境中获取信息然后学习选择能够最大化奖赏的行为。例如，一个神经网络可以“看见”视频游戏画面然后输出相应的游戏动作，目标是为了最大化获得高分，这就可以使用强化学习。目前强化学习还大多只是研究领域而并没有在除了游戏之外的实践中获得重要的成功。然而待以时日，作者期待能看到强化学习会占领真实世界应用的广泛领域 - 自动驾驶、机器人、资源管理、教育......当时机成熟时，作者坚信时机肯定会成熟   。\n",
    "\n",
    "> In this book, we will focus specifically on supervised learning, since it is by far the\n",
    "dominant form of deep learning today, with a wide range of industry applications. We\n",
    "will also take a briefer look at self-supervised learning in later chapters.\n",
    "\n",
    "本书我们会集中在有监督学习上，因为它是目前机器学习应用中的统治者，在广泛范围的工业应用中得到了使用和验证。我们也会在后面的章节中对自监督学习作简要的介绍。\n",
    "\n",
    "> Although supervised learning mostly consists of classification and regression, there\n",
    "are more exotic variants as well:\n",
    "\n",
    "> - Sequence generation (e.g. given a picture, predict a caption describing it). Sequence\n",
    "generation can sometimes be reformulated as a series of classification problems (e.g.\n",
    "repeatedly predicting the word or token in a sequence).\n",
    "- Syntax tree prediction (e.g. given a sentence, predict its decomposition into a syntax\n",
    "tree).\n",
    "- Object detection: given a picture, draw a bounding box around certain objects inside the\n",
    "picture. This can also be expressed as a classification problem (given many candidate\n",
    "bounding boxes, classify the contents of each one) or as a joint classification and\n",
    "regression problem, where the bounding box coordinates are being predicted via vector\n",
    "regression.\n",
    "- Image segmentation: given a picture, draw a pixel-level mask on a specific object.\n",
    "- etc...\n",
    "\n",
    "虽然有监督学习通常就是分类和回归，但是它也有一些很有趣的变体：\n",
    "\n",
    "- 序列生成（例如给定照片，预测描述照片的说明）。序列生成有时能被重定义为一系列的分类问题（例如重复的在一个序列中预测单词或符号）。\n",
    "- 语法树预测（例如给定一个句子，预测它解构成语法树的形式）。\n",
    "- 目标检测：给定照片，在其中某个特定目标上绘制方框。这也可以被表达成为一个分类问题（给定很多个可选的方框，对每个其中的内容进行分类）或者一个分类和回归问题的联合体，其中方框所在的坐标点是通过矢量回归预测得到的。\n",
    "- 图像分割：给定照片，在特定目标之上绘制像素级的遮盖。\n",
    "- 等等......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5 分类和回归问题术语词汇表\n",
    "\n",
    "> Classification and regression involve many specialized terms. You have already come\n",
    "across some of them in our first examples, and you will see more of them come up in the\n",
    "following chapters. They have precise, machine-learning specific definitions, and you\n",
    "should be familiar with them.\n",
    "\n",
    "分类和回归包含着很多特定的术语。在我们前面那些例子中，你已经看到了其中的一些，后续章节内容中还会出现更多的相关术语。它们都有着机器学习特定的精确定义，你应该熟悉它们。\n",
    "\n",
    "> Sample, or input : one data points that goes into your model.\n",
    "\n",
    "样本，或者叫输入：代入到你模型中的一个数据点。\n",
    "\n",
    "> Prediction, or output : what goes out of your model.\n",
    "\n",
    "预测，或者叫输出：从你的模型得到的结果。\n",
    "\n",
    "> Target: the truth. What your model should ideally have predicted, according to an\n",
    "external source of data.\n",
    "\n",
    "目标：真实结果。你的模型理想情况下应该得到的预测，这些目标都是通过外部数据得到的。\n",
    "\n",
    "> Prediction error, or loss value : a measure of the distance between you model’s\n",
    "prediction and the target.\n",
    "\n",
    "预测误差，或者叫损失值：在你的模型预测和目标之间距离的度量。\n",
    "\n",
    "> Classes: set of possible labels to choose from in a classification problem, e.g. when classifying cat and dog pictures, \"dog\" and \"cat\" are the two classes.\n",
    "\n",
    "类别：在分类问题中为样本选择的一组可能的标签，例如当从猫和狗的照片中分类时，“猫”和“狗”就是两个类别。\n",
    "\n",
    "> Label: specific instance of a class annotation in a classification problem. For instance,\n",
    "if picture #1234 is annotated as containing the class \"dog\", then \"dog\" is a label of picture\n",
    "#1234.\n",
    "\n",
    "标签：在分类问题中对某个特定类别所作的标记。例如照片序号1234被标记为类别“狗”，那么“狗”就是1234号照片的标签。\n",
    "\n",
    "> Ground-truth, or annotations : all targets for a dataset, typically collected by humans.\n",
    "\n",
    "基本事实，或者叫标记：一个数据集的所有目标集合，通常是由人工收集整理的。\n",
    "\n",
    "> Binary classification: classification task where each input sample should be\n",
    "categorized into two exclusive categories.\n",
    "\n",
    "二分分类：指的是每个输入样本都应该被划分到两个互斥的类别之中的分类任务。\n",
    "\n",
    "> Multi-class classification: classification task where each input sample should be\n",
    "categorized into more than two categories: for instance, classifying handwritten digits is a\n",
    "multi-class classification task.\n",
    "\n",
    "多类别分类：指的是每个输入样本都应该被划分到多于两个的类别之中的分类任务，例如前面的手写数字识别就是一个多类别分类任务。\n",
    "\n",
    "> Multi-label classification: classification task where each input sample can be assigned\n",
    "multiple labels. For instance, a given image may contain both a cat and a dog, and should\n",
    "be annotated both with the \"cat\" label and the \"dog\" label. The number of labels per\n",
    "image is usually variable.\n",
    "\n",
    "多标签分类：指的是每个输入样本都能被指定为多个标签的分类任务。例如，一个图像可能既包括猫也包括狗，因此可以被标记为“猫”标签以及“狗”标签。每张图像上标签的数量通常是可变的。\n",
    "\n",
    "> Scalar regression: task where the target is a continuous scalar value. House price\n",
    "prediction is a good example: the different target prices form a continuous space.\n",
    "\n",
    "标量回归：指的是预测目标是一个连续标量的任务。前面的房价预测就是这样的例子，不同的目标价格构成了一个连续的空间。\n",
    "\n",
    "> Vector regression: task where the target is a set of continuous values, e.g. a\n",
    "continuous vector. If you are doing regression against multiple values (e.g. the\n",
    "coordinates of a bounding box in an image) then your are doing vector regression.\n",
    "\n",
    "矢量回归：指的是预测目标是一组连续值的任务，也就是一个连续的矢量。如果你针对多个值进行回归任务（例如图像上目标定位框的坐标点），那么你就应该使用矢量回归。\n",
    "\n",
    "> Mini-batch or batch : a small set of samples that are being processed at once by the\n",
    "model (typically between 8 and 128 samples). It is often a power of 2 in order to facilitate\n",
    "memory allocation on GPU. When training, a mini-batch is used to compute a single\n",
    "gradient descent update applied to the weights of the model.\n",
    "\n",
    "小批量或批量：每次让模型处理一小部分的样本集（通常介于8和128个样本之间）。通常使用2的幂值作为批量样本数来适应GPU上显存的分布。在训练过程中，每个小批量样本都被用来计算一次梯度下降优化，并更新模型上的权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 验证机器学习模型\n",
    "\n",
    "> In the three examples we covered in the previous chapters, we split our data into a\n",
    "training set, a validation set, and a test set. The reason why we did not evaluate our\n",
    "models on the same data as they were trained on quickly became evident: after just a few\n",
    "epochs, all three models started to overfit , which is to say that their performance on\n",
    "never-seen-before data started stalling (or even worsening) compared to their\n",
    "performance on the training data—which always go up as training progresses.\n",
    "\n",
    "上一章的三个例子中，我们将整个数据集划分成了一个训练集、一个验证集和一个测试集。我们不在同一个数据集上对模型进行验证的原因是很明显的：仅仅经过几次迭代之后，前面的三个模型就开始出现过拟合，也就是说它的性能在从未见过的数据上开始停滞（甚至变差）了，而它在训练数据上的性能却是一直增加的。\n",
    "\n",
    "> In machine learning, our goal is to achieve models that generalize , i.e. that perform\n",
    "well on never-seen-before data, and overfitting is the central obstacle. We can only\n",
    "control that which we can observe, so it is crucial to be able to reliably measure the\n",
    "generalization power of our model. In the next sections, we will take a look at strategies\n",
    "for mitigating overfitting and maximizing generalization. In the present section, we will\n",
    "focus on how we can measure generalization, i.e. how to evaluate machine learning\n",
    "models.\n",
    "\n",
    "在机器学习中，我们的目标是让模型实现泛化，也就是说能够在从未见过的数据上表现良好，因此过拟合是比较严重的障碍。而我们只能对可观测的实验结果来控制训练过程，因此能够可靠的测量模型的泛化能力是十分重要的。在下面章节中我们会学习到抑制过拟合以及最大化泛化能力的策略。本小节我们重点关注我们如何测量泛化能力，也就是如何验证机器学习模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 训练集、验证集和测试集\n",
    "\n",
    "> Evaluating a model always boils down to splitting your available data into three sets:\n",
    "training, validation, and test set. You train on the training data, and evaluate your model\n",
    "on the validation data. Once your model is ready for prime time, you test it one final time\n",
    "on the test data.\n",
    "\n",
    "验证模型归结起来总是需要将可用的数据分成三个部分：训练集、验证集和测试集。在训练集上训练模型，在验证集上验证模型。一旦模型经过上述步骤并优化封装后，就可以最终在测试集上来测试模型的性能了。\n",
    "\n",
    "> You may ask, why not simply have two sets, a training set and a test set? We would\n",
    "train on the training data, and evaluate on the test data. Much simpler!\n",
    "\n",
    "你可能会问，为什么不简单的分成两个集合，一个训练集和一个测试集？然后在训练集上训练在测试集上验证，多简单啊！\n",
    "\n",
    "> The reason is that developing a model always involves tuning its configuration, e.g.\n",
    "picking the number of layers or the size of the layers (what is called the\n",
    "\"hyperparameters\" of the model, to distinguish them from the \"parameters\", which are\n",
    "the network’s weights). You will do this tuning by using as feedback signal the\n",
    "performance of the model on the validation data, so in essence this tuning is a form of\n",
    "learning : a search for a good configuration in some parameter space. As a result, tuning\n",
    "the configuration of the model based on its performance on the validation set can quickly\n",
    "result in overfitting to the validation set , even though your model is never being directly\n",
    "trained on it.\n",
    "\n",
    "不这样做的原因是，当你开发一个模型时，你永远需要在过程中调整它的配置，例如模型的层数或者层的单元尺寸（这些被称为模型的“超参数”，用来与模型自身的权重“参数”作为区别）。你需要通过模型在验证数据上的性能表现来作为调整它们的依据，因此本质上这种调整也是一种形式的学习过程：在一些参数空间中搜索最佳的超参数配置。这个学习的结果就是，基于在验证集数据上的性能来调整模型的配置也会导致在验证集上的过拟合，即使在模型从未直接在它之上训练的情况下。\n",
    "\n",
    "> Central to this phenomenon is the notion of \"information leak\". Every time you are\n",
    "tuning a hyperparameter of your model based on the model’s performance on the\n",
    "validation set, some information about the validation data is leaking into your model. If\n",
    "you only do this once, for one parameter, then very few bits of information would be\n",
    "leaking and your validation set would remain a reliable way to evaluate your model. But\n",
    "if you repeat this many times, running one experiment, evaluating on the validation set,\n",
    "modifying your model as a result, then you are leaking an increasingly significant amount\n",
    "of information about the validation set into your model.\n",
    "\n",
    "这个现象的重点是一种被称为“信息泄露”的问题。每次当你使用模型在验证集上的性能来调整它的超参时，验证数据中的一些信息就被泄露进入到你的模型当中。如果你只在一个参数上进行了一次调整，那么仅有极少量的信息被泄露到模型之中。但是如果你在一个实验中重复这个过程多次，在验证集上验证模型并依此修改模型，那么就会泄露大量的信息进入到你的模型当中。\n",
    "\n",
    "> At the end of the day, you end up with a model that performs artificially well on the\n",
    "validation data, because it is what you optimized it for. Since what you care about is\n",
    "actually performance on completely new data, not the validation data, you need a\n",
    "completely different, never-seen-before dataset to evaluate your model: the test dataset.\n",
    "Your model shouldn’t have had access to any information about the test set, even\n",
    "completely indirectly. If anything about model has been tuned based on test-set\n",
    "performance, then your measure of generalization will be flawed.\n",
    "\n",
    "最终，模型在你的干预之下会在验证数据上表现出良好的性能，因为这就是你调整超参的目的。因为你实际上关注的是它在全新的数据上的性能表现，而不是验证集，你需要一个完全不同的模型从未见过的数据集来检验模型：测试数据集。你的模型不应该与测试集的信息有任何的接触，即使是间接的接触也不允许。如果模型依据测试集性能做了任何的调整，你就破坏了整个泛化性测量的基础。\n",
    "\n",
    "> Splitting your data into a training, validation, and test sets may seem straightforward,\n",
    "but there are a few advanced ways to do it which can come when very few data\n",
    "is available. Let’s review three classic evaluation recipes.\n",
    "\n",
    "将数据分为训练集、验证集和测试集看起来虽然直接，但当手头的数据非常少的时候也有着一些高级的方法。我们来查看一下三种经典的验证配方。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 简单的留出验证\n",
    "\n",
    "> Set apart some fraction of your data as your test set. Train on remaining data, evaluate on\n",
    "the test set. As you saw in the previous sections, in order to prevent information leaks,\n",
    "you should not tune your model based on the test set, and therefore you should also\n",
    "reserve a validation set.\n",
    "\n",
    "从数据中分出一部分作为你的测试集。在剩下的数据上进行训练，然后使用测试集进行验证。正如上一节你看到的，为了避免信息泄露，你不应该基于测试集来调整模型，因此你应该再留出部分的验证集。\n",
    "\n",
    "> Schematically, hold-out validation looks like this:\n",
    "\n",
    "留出验证在原理上如下图：\n",
    "\n",
    "![hold-out validation](imgs/f4.1.jpg)\n",
    "\n",
    "图4-1 简单留出验证数据分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here’s a simple implementation:\n",
    "\n",
    "下面是其简单的实现：\n",
    "\n",
    "译者注：下面是伪代码实现，无法运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples = 10000\n",
    "\n",
    "# 对数据进行重新洗牌通常都是有用的\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# 分出验证集\n",
    "validation_data = data[:num_validation_samples]\n",
    "data = [num_validation_samples:]\n",
    "\n",
    "# 分出训练集\n",
    "training_data = data[:]\n",
    "\n",
    "# 使用训练集训练模型，然后用验证集进行验证\n",
    "model = get_model()\n",
    "model.train(training_data)\n",
    "validation_score = model.evaluate(validation_data)\n",
    "\n",
    "# 到这里，你开始根据验证性能调整模型超参\n",
    "# 然后重新训练，再次调整.....直至满意\n",
    "\n",
    "# 然后你已经完成了超参数调整，可以将训练集和验证集合起来对模型作最后一次训练\n",
    "# 完成后在测试数据集上进行最后验证\n",
    "model = get_model()\n",
    "model.train(np.concatenate([training_data,\n",
    "validation_data]))\n",
    "test_score = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is the simplest evaluation protocol, and it suffers from one flaw: if little data is\n",
    "available, then your validation and test sets may contain too few samples to be\n",
    "statistically representative of the data at hand. This is easy to notice: if different random\n",
    "shuffling rounds of the data before splitting end up yielding very different model\n",
    "performance measures, then you are having this issue. K-fold validation and iterated\n",
    "K-fold validation are two ways to address this.\n",
    "\n",
    "这种方法是最简单的验证方案，它有一个缺点：如果可用的数据很少，那么你的验证集和测试集可能包括很少的样本，它们将无法正确的表示样本空间。这很容易能被注意到：如果对样本数据进行不同的随机洗牌过程，会导致模型性能结果差异很大，就可以确认这是产生的原因了。K-折验证和迭代K-折验证是解决这个问题的两种方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 K-折验证\n",
    "\n",
    "> Split your data into K partitions of equal size. For each partition i , train a model on the\n",
    "remaining N-1 partitions, and evaluate it on partition i . Your final score would then be\n",
    "the averages of the K scores obtained. This method is helpful when the performance of\n",
    "your model shows significant variance based on your train-test split. Like hold-out\n",
    "validation, this method doesn’t exempt you from using a distinct validation set for model\n",
    "calibration.\n",
    "\n",
    "将数据均分为K个不同部分。对于每个分区i，使用其他的K-1个分区进行训练，然后在第i个分区上进行验证。最终的验证分数是K个验证分数的均值。这个方法在模型使用训练-测试划分情况下显示出较高方差的时候非常有帮助。像留出验证一样，这个方法仍然依赖于留出一个独立的验证集进行模型调整。\n",
    "\n",
    "> Schematically, K-fold cross-validation looks like this:\n",
    "\n",
    "K-折交叉验证原理上如下图：\n",
    "\n",
    "![K-fold validation](imgs/f4.2.jpg)\n",
    "\n",
    "图5-2 K-折验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here’s a simple implementation:\n",
    "\n",
    "下面是简单的实现：\n",
    "\n",
    "译者注：伪代码，无法运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "num_validation_samples = len(data) // k\n",
    "np.random.shuffle(data)\n",
    "\n",
    "validation_scores = []\n",
    "for fold in range(k):\n",
    "    # 分出第i个验证集\n",
    "    validation_data = data[num_validation_samples * fold: num_validation_samples * (fold + 1)]\n",
    "    \n",
    "    # 剩下的数据部分全部用来作为训练集\n",
    "    training_data = data[:num_validation_samples * fold] + data[num_validation_samples * (fold + 1):]\n",
    "    \n",
    "    # 每个折都创建一个全新的网络，然后训练和验证模型，将验证分数加到分数列表中\n",
    "    model = get_model()\n",
    "    model.train(training_data)\n",
    "    validation_score = model.evaluate(validation_data)\n",
    "    validation_scores.append(validation_score)\n",
    "\n",
    "# 求出K折的平均验证分数\n",
    "validation_score = np.average(validation_scores)\n",
    "# 最后在除了测试集外的全部数据上重新训练一遍模型，最后得到测试分数\n",
    "model = get_model()\n",
    "model.train(data)\n",
    "test_score = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 带随机洗牌的迭代K-折验证\n",
    "\n",
    "> This one is for situations in which you have relatively little data available and you need\n",
    "to evaluate your model as precisely as possible. I have found it to be extremely helpful in\n",
    "Kaggle competitions. It consists of applying K-fold validation multiple times, shuffling\n",
    "the data every time before splitting it K-ways. Your final score would be the average of\n",
    "the scores obtained at each run of K-fold validation. Note that you end up training and\n",
    "evaluating P * K models (where P is the number of iterations you use), which can very\n",
    "expensive.\n",
    "\n",
    "本方法适用于当你有相对很少的数据，而却需要尽可能精确的验证你的模型。作者发现它在Kaggle竞赛中非常有用。它包括将K-折验证应用多次，每次在应用K-折方法之前都将数据进行随机洗牌。最终的验证分数将会是多次K-折验证中的平均分数。注意这个方法中数据会被训练和验证 $P \\times K$次（其中P是迭代的次数），这可能需要大量的计算资源。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 需要记住的内容\n",
    "\n",
    "> There are a few things to keep an eye out for when picking an evaluation protocol:\n",
    "\n",
    "> - Data representativeness. You want your training set and test set to be both representative\n",
    "of the data at hand; for instance if you are trying to classify images of digits, and you are\n",
    "starting from an array of samples where the samples are ordered by their class, taking the\n",
    "first 80% of the array as your training set and the remaining 20% as your test would\n",
    "result in your training set only having classes 0-7 while your test set would only have\n",
    "classes 8-9. This seems like a ridiculous mistake, but it’s surprisingly common. For this\n",
    "reason, you should most likely randomly shuffle your data before splitting it into a\n",
    "training and test set.\n",
    "- The arrow of time. If you are trying to predict the future given the past (e.g. the weather\n",
    "tomorrow, stock movements, and so on), you should not randomly shuffle your data\n",
    "before splitting it, because that would create a \"temporal leak\": you model would\n",
    "effectively be trained on data from the future. In such situations you should always make\n",
    "sure that all data in your test set is posterior to the data in the training set.\n",
    "- Redundancy in your data. If some data points in your data appear twice (fairly common\n",
    "with real-world data), then shuffling the data and splitting it into a training set and a test\n",
    "set will result in redundancy between the training and test set. In effect, you would be\n",
    "testing on part of your training data, which is the worst thing you could do! Make sure\n",
    "that your training sets and tests sets are disjoint.\n",
    "\n",
    "当选择验证方法时，有如下需要牢记的要点：\n",
    "\n",
    "- 数据代表性。你必须保证训练集和测试集都能够全面代表手头的样本空间；例如当你进行手写数字识别时，你获得的数据集的样本是按照它们的类别标签排序的，你将前80%的样本取出来作为训练集而剩下20%作为测试集，这会导致训练集中仅包含0-7的类别而测试集中仅包含8-9的类别。这看起来很滑稽，但其实这个错误经常犯。因此，在划分训练集和测试集之前你应该坚持对样本进行随机洗牌。\n",
    "- 时间箭头。如果你在进行未来预测的任务（如明天的天气，股市变化等），你不应该对数据进行随机洗牌，因为这会造成“时间泄露”：模型会有效地从未来的数据中得到训练。在这种情况下你应该保证测试机中的数据在时间上都处于训练集数据之后。\n",
    "- 数据冗余性。如果数据中的样本多次出现（真实世界中很常见），那么分训练集和测试集之前进行随机洗牌有可能会将冗余的样本分到训练集和测试集中。结果你会在部分的训练数据上对模型进行验证测试，这是你可能遇到的最糟糕的事情了！必须确保训练集和测试集必须是正交的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 数据预处理、特征工程和特征学习\n",
    "\n",
    "> Besides model evaluation, an important question we must tackle before we dive deeper\n",
    "into model development is the following: how to prepare the input data and targets before\n",
    "feeding them into a neural network? Many data preprocessing and feature engineering\n",
    "techniques are domain-specific (e.g. specific to text data or image data), and we will\n",
    "cover those in the next chapters as we encounter them in practical examples. For now, we\n",
    "will review the basics, common to all data domains.\n",
    "\n",
    "除了模型验证，在我们深入学习开发模型之前还有一个重要的问题：在输入数据和目标代入神经网络之前，我们应该如何准备它们？很多数据预处理和特征工程的技巧都是针对特定领域的（例如针对文本数据或者图像数据），后续章节中我们会使用实际的例子来介绍它们。本节中我们将进行概述，也就是全部数据领域中通用的部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 神经网络中的数据预处理\n",
    "\n",
    "#### 矢量化\n",
    "\n",
    "> All inputs and targets in a neural network must be tensors of floating point data (or in\n",
    "specific cases, tensors of integers). Whatever data you need to process—sound, images,\n",
    "text—you must first turn it into tensors, a step called \"data vectorization\". For instance, in\n",
    "our two previous text classification examples, we started from text represented as lists of\n",
    "integers (standing for sequences of words), and we used \"one-hot encoding\" to turn them\n",
    "into a tensor of float32 data. In the digits classification example and house price\n",
    "prediction example, the data already came in vectorized form, so we could skip this step.\n",
    "\n",
    "在神经网络中所有的输入和目标都必须是浮点数的张量（或者在特定情况下，整数张量）。无论你是在处理声音、图像还是文本，你都需要将它转换成张量，被称为“数据矢量化”。例如，在前面两个文本分类的例子当中，我们拿到的都是整数列表的输入数据（代表着单词的序列），然后使用“one-hot”编码将它们转换成float32数据的张量。在手写数字分类和房价预测例子中，拿到的数据已经是矢量化的形式了，因此可以跳过这个步骤。\n",
    "\n",
    "#### 值标准化\n",
    "\n",
    "> In our digits classification example, we started from image data encoded as integers in\n",
    "the 0-255 range, encoding grayscale values. Before we fed this data into our network, we\n",
    "had to cast it to float32 and divide by 255, so we would end up with floating point\n",
    "values in the 0-1 range. Similarly, in our house price prediction example, we started from\n",
    "features that took a variety of ranges—some features had small floating point values,\n",
    "others had fairly large integer values. Before we fed this data into our network, we had to\n",
    "normalize each feature independently so that each feature would have a standard\n",
    "deviation of 1 and a mean of 0.\n",
    "\n",
    "在手写数据分类例子中，我们拿到的数据被编码成0-255区间的整数，代表灰度值。在我们将数据代入网络之前，需要通过将它除以255转换成一个float32类型，得到的是一个0-1区间的浮点数值。类似的，在房价预测例子中，数据中的特征具有不同的区间 - 有些特征是小浮点数值，其他一些却是较大的整数值。在代入网络之前，我们需要对每个特征进行标准化，得到的每个特征都具有0均值和1的标准差。\n",
    "\n",
    "> In general, it isn’t safe to feed into a neural network data that takes relatively \"large\"\n",
    "values (e.g. multi-digit integers, which is much larger than the initial values taken by the\n",
    "weights of a network), or data that is \"heterogeneous\", e.g. data where one feature would\n",
    "be in the 0-1 range and another in the 100-200 range. It can trigger large gradient\n",
    "updates which will prevent your network from converging. To make learning easier for\n",
    "your network, your data should:\n",
    "\n",
    "> - Take \"small\" values: typically most values should be in the 0-1 range.\n",
    "- Be homogenous, i.e. all features should take values roughly in the same range.\n",
    "\n",
    "通常来说，将相对较大的值代入神经网络中是不安全的（例如两位数的值，它们比起网络权重的初始值来说大很多），同样“异质的”的数据也是一样，例如样本中一个特征值处于0-1区间而另一个处于100-200区间。它们会触发巨大的梯度更新，这会影响网络收敛过程。要使得你的网络学习过程更加容易，你的数据应该具备：\n",
    "\n",
    "- 使用“较小”的数值：通常大部分数据值应该处于0-1区间。\n",
    "- 同质数据，也就是所有特征应该大致处于相同的取值区间。\n",
    "\n",
    "> Additionally, the following stricter normalization practice is common and can\n",
    "definitely help, although it isn’t always necessary (e.g. we did not do this in our digits\n",
    "classification example) :\n",
    "\n",
    "> - Normalizing each feature independently to have a mean of 0.\n",
    "- Normalizing each feature independently to have a standard deviation of 1.\n",
    "\n",
    "更具体来说，下面这些严格的标准化实践是通用和有帮助的，虽然并不是一定必须的（例如我们在手写数字分类例子中就没有使用）：\n",
    "\n",
    "- 对每个特征进行独立的标准化，使得均值为0。\n",
    "- 对每个特征进行独立的标准化，使得标准偏差为1。\n",
    "\n",
    "> This is easy to do with Numpy arrays:\n",
    "\n",
    "使用Numpy数组这很容易实现：\n",
    "\n",
    "译者注：伪代码，不能运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设x是一个二维张量，形状为(samples, features)\n",
    "x -= x.mean(axis=0)\n",
    "x /= x.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理缺失值\n",
    "\n",
    "> You may sometimes have missing values in your data. For instance, in our house price\n",
    "prediction example, the first feature (the column of index 0 in the data) was \"per capita\n",
    "crime rate\". What if this feature was not available for all samples? We would then have\n",
    "missing values in our training or test data.\n",
    "\n",
    "有的时候你的数据中会有一些缺失值。例如在房价预测例子中，第一个特征（也就是数据的第0列）的“人口犯罪率”。如果这个特征并不是所有样本都存在的话会怎样？这时就会出现训练数据或测试数据中的缺失值。\n",
    "\n",
    "> In general, with neural networks, it is safe to input missing values as 0 , under the\n",
    "condition that 0 is not already a meaningful value. The network will learn from exposure\n",
    "to the data that the value 0 simply means \"missing data\" and will start ignoring the value.\n",
    "However, note that if you are expecting missing values in the test data but the network\n",
    "was trained on data without any missing values, then the network will not have learned to\n",
    "ignore missing values! In this situation, then you should artificially generate training\n",
    "samples with missing entries: simply copy some training samples several times and drop\n",
    "some of the features that you expect are susceptible to go missing in the test data.\n",
    "\n",
    "通常来说在神经网络中，在数值0没有被用来表示有意义的值的情况下，将缺失值当成0是安全的。网络会从暴露给它的数据中自动学习到0就代表着“缺失值”然后开始忽略它。然而请注意，如果遇到测试集中有缺失值而训练集中没有缺失值的情况，网络将无法从数据中学习到忽略缺失值！在这样的情况下，你应该人工在训练集中生成具有缺失值的样本：只需要将一些训练样本复制并且丢弃其中一些特征值放入训练集中，这样就能令网络能够感知到缺失值的存在。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 特征工程\n",
    "\n",
    "> Feature engineering is the process of using your own knowledge about the data and about\n",
    "the machine learning algorithm at hand (in our case a neural network) to make the\n",
    "algorithm work better by applying hard-coded (non-learned) transformations to the data\n",
    "before it goes into the model. In many cases, it isn’t reasonable to expect a machine\n",
    "learning model to be able to learn from completely arbitrary data. The data needs to be\n",
    "presented to the model in a way that will make the job of the model easier. One intuitive\n",
    "example of this is the following: suppose that we are trying to develop a model that can\n",
    "take as input an image of a clock, and can output the time of the day.\n",
    "\n",
    "特征工程就是运用你手头对数据和机器学习算法（在本书中就是神经网络）的认识，通过硬编码的方式（而非学习）将数据在代入模型之前进行转换，达到使算法更好的工作的目的。在很多情况下，我们都有理由相信机器学习模型能够从完全任意的数据中进行学习。对数据进行相应的处理知识为了使得模型完成任务的工作更加容易。下面用一个直观的例子来进行说明：假如说我们希望开发一个模型能够接受一个钟表的图像作为输出，然后输出它指示的时间。\n",
    "\n",
    "![feature engineering](imgs/f4.3.jpg)\n",
    "\n",
    "图4-3 阅读时钟图像任务中的特征工程\n",
    "\n",
    "译者注：上图中第一行是原始数据，图像的像素点数据；第二行是处理后的数据，时钟指针的坐标值；第三行是更好的方式处理后的数据，时钟指针的角度值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you choose to use the raw pixels of the image as input data, then you have on your\n",
    "hands a difficult machine learning problem. You will need a convolutional neural\n",
    "network to solve it, and you will have to expend quite a bit of computational resources to\n",
    "train it.\n",
    "\n",
    "如果选择使用输入图像中的原始像素点数据，那么你就会面对一个更加困难的机器学习问题。你会需要用到卷积神经网络才能解决它，并且你不得不花费更多的计算资源来进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> However, if you already understand the problem at a high-level (you understand how\n",
    "humans read time on a clock face), then you can come up with much better input features\n",
    "for a ML algorithm: for instance, it is easy to write a 5-line Python script to follow the\n",
    "black pixels of the clock hands and output the coordinates of the tip of each (x, y)\n",
    "hand. Then a very simple ML algorithm can learn to associate these coordinates with the\n",
    "appropriate time of the day.\n",
    "\n",
    "但是如果你已经在高层次理解了问题（你理解人类是如何阅读时钟指示的时间的），那么你就可以将输入数据处理成更适合机器学习算法的特征值形式：例如很容易写出仅仅5行的Python代码沿着时钟指针黑色的像素并输出指针针尖位置的$(x, y)$坐标。然后使用一个很简单的机器学习算法就能够从这些坐标的组合中学习到时间的表示。\n",
    "\n",
    "> You can go even further: you can do a coordinate change, and express the (x, y)\n",
    "coordinates as polar coordinates with regard to the center of the image. Your input would\n",
    "simply become… the angle of each clock hand. At this point your features are theta\n",
    "making the problem so easy that no machine learning is required anymore; a simple\n",
    "rounding operation and dictionary lookup are enough to recover the approximate time of\n",
    "day.\n",
    "\n",
    "还能更进一步：你可以进行坐标转换，将$(x, y)$的坐标表示转换成极坐标方式。这时你的输入就会简化成每根时针的角度。在这种情况下特征值就只剩下$\\theta$角度，使得这个问题已经简化到根本不需要机器学习了；一个简单的四舍五入计算和查表就能还原时间表示。\n",
    "\n",
    "> That’s the essence of feature engineering: making a problem easier by expressing it in\n",
    "a simpler way. It usually requires understanding the problem in-depth.\n",
    "\n",
    "上面说的就是特征工程的实质：使用一种简单方式表示数据令我们面对的问题变得简单。它通常需要深度理解问题本身。\n",
    "\n",
    "> Before deep learning, feature engineering used to be critical, because classical\n",
    "\"shallow\" algorithms did not have hypothesis spaces rich enough to learn useful features\n",
    "by themselves. The way you would present the data to the algorithm would be essential\n",
    "to its success. For instance, before convolutional neural networks started becoming\n",
    "successful on the MNIST digits classification problem, solutions were typically based on\n",
    "hard-coded features such as the number of loops in a digit image, the height of each digit\n",
    "in an image, an histogram of pixel values, and so on.\n",
    "\n",
    "在深度学习之前，特征工程曾经非常重要，因为经典的浅学习算法都不具备足够大的假设空间来自动学习到有用的特征。所以输入到算法的数据形式对于模型的成功非常重要。例如，使用卷积神经网络在MNIST手写数字分类问题取得成功之前，应用的机器学习算法通常都基于硬编码的特征提取，诸如数字图像中圈的个数，一张图像中每个数字的高度，像素值的直方图等等。\n",
    "\n",
    "> Thankfully, modern deep learning removes the need for most feature engineering,\n",
    "since neural networks are capable of automatically extracting useful features from raw\n",
    "data. Does this mean you don’t have to care about feature engineering at all as long as\n",
    "you are using deep neural networks? No, for two reasons:\n",
    "\n",
    "> - Good features can still allow you to solve problems more elegantly while using less\n",
    "resources. For instance, it would be ridiculous to solve our clock face reading problem\n",
    "using a convolutional neural network.\n",
    "- Good features can allow you to solve a problem with much less data. The ability of deep\n",
    "learning models to learn features on their own relies on having lots of training data\n",
    "available; if only few samples are available, then the informativeness of their features\n",
    "becomes critical.\n",
    "\n",
    "幸运的是，现代深度学习已经不再需要大部分的特征工程，因为神经网络能够自动从原始数据中提取出有用的特征。这是否意味着当你使用深度神经网络时完全不再需要关心特征工程了呢？答案是否定的，两个原因：\n",
    "\n",
    "- 优质的特征仍然有助于你在使用更少资源的情况下更加优雅的解决问题。例如前面的时钟问题，如果我们选择使用卷积神经网络的话就会显得十分荒唐。\n",
    "- 优质的特征允许你使用更少的数据来解决问题。深度学习模型能够自动学习到有用特征的能力取决于它有很多的训练数据；如果手头只有少量的样本，那么它们特征的表示能力就变得特别重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 过拟合和欠拟合\n",
    "\n",
    "> In all the examples we saw in the previous chapter—movie review sentiment prediction,\n",
    "topic classification, and house price regression—we could notice that the performance of\n",
    "our model on the held-out validation data would always peak after a few epochs and\n",
    "would then start degrading, i.e. our model would quickly start to overfit to the training\n",
    "data. Overfitting happens in every single machine learning problem. Learning how to\n",
    "deal with overfitting is essential to mastering machine learning.\n",
    "\n",
    "在前面几章我们看到的例子中：影评观点预测，新闻主题分类和房价预测回归。我们观察到模型在验证集上的性能在数个迭代之后就会达到顶峰，然后开始下降，也就是说我们的模型很快开始对训练数据产生了过拟合。过拟合问题存在于每个机器学习问题中。学习如何应对过拟合在机器学习中非常关键。\n",
    "\n",
    "> The fundamental issue in machine learning is the tension between optimization and\n",
    "generalization. \"Optimization\" refers to the process of adjusting a model to get the best\n",
    "performance possible on the training data (the \"learning\" in \"machine learning\"), while\n",
    "\"generalization\" refers to how well the trained model would perform on data it has never\n",
    "seen before. The goal of the game is to get good generalization, of course, but you do not\n",
    "control generalization; you can only adjust the model based on its training data.\n",
    "\n",
    "机器学习中的基本问题是优化和泛化之间的博弈。“优化”指的是在训练过程中尽可能提高模型在训练数据上的性能的过程（也就是“机器学习”中的“学习”），而“泛化”指的是已经训练好的模型在它从未见过的数据上的性能表现。这个过程的目标当然是获得足够好的泛化能力，但是你无法直接控制泛化；你只能依据训练数据调整模型。\n",
    "\n",
    "> At the beginning of training, optimization and generalization are correlated: the lower\n",
    "your loss on training data, the lower your loss on test data. While this is happening, your\n",
    "model is said to be : there is still progress to be made; the network hasn’t yet under-fit\n",
    "modeled all relevant patterns in the training data. But after a certain number of iterations\n",
    "on the training data, generalization stops improving, validation metrics stall then start\n",
    "degrading: the model is then starting to over-fit, i.e. is it starting to learn patterns that are\n",
    "specific to the training data but that are misleading or irrelevant when it comes to new\n",
    "data.\n",
    "\n",
    "在训练的开始阶段，优化和泛化是相关的：在训练集上更低的损失也代表着在测试集上更低的损失。出现这种情况时，你的模型可以被认为：还有很多学习过程需要进行；网络还未学习到所有训练数据上的有关模式，这也叫作欠拟合。但是再经过在训练集上一定数量的迭代学习之后，泛化能力停止改善，验证指标停止上升并开始下降：模型开始过拟合，也就是它开始学习那些训练数据上特定的模式，但这些模式在处理新数据时会产生误导或是无关的。\n",
    "\n",
    "> To prevent a model from learning misleading or irrelevant patterns found in the\n",
    "training data, the best solution is of course to get more training data . A model trained on\n",
    "more data will naturally generalize better. When that is no longer possible, the next best\n",
    "solution is to modulate the quantity of information that your model is allowed to store, or\n",
    "to add constraints on what information it is allowed to store. If a network can only afford\n",
    "to memorize a small number of patterns, the optimization process will force it to focus on\n",
    "the most prominent patterns, which have a better chance of generalizing well.\n",
    "\n",
    "要阻止模型学习训练数据中那些误导性或无关的模式，最好的解决方法当然是获得更多的训练数据。一个在更多数据上训练得到模型自然会泛化的更好。但是在这个办法不可能的情况下，另一个最好的办法是限制模型能够存储信息的数量，或者为它存储的信息做出一定约束。如果一个网络只能记住少量的模式，那么优化过程就会强制它更加关注那些更加重要的模式，从而达到更好泛化的目标。\n",
    "\n",
    "> The processing of fighting overfitting in this way is called regularization . Let’s\n",
    "review some of the most common regularization techniques, and let’s apply them in\n",
    "practice to improve our movie classification model from the previous chapter.\n",
    "\n",
    "这种对抗过拟合的过程被称为正则化。下面我们来介绍一些最通用的正则化技巧，然后将它们应用到上一章的影评分类例子中来改善模型的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 对抗过拟合\n",
    "\n",
    "#### 缩减模型尺寸\n",
    "\n",
    "> The simplest way to prevent overfitting is to reduce the size of the model, i.e. the number\n",
    "of learnable parameters in the model (which is determined by the number of layers and\n",
    "the number of units per layer). In deep learning, the number of learnable parameters in a\n",
    "model is often referred to as the model’s \"capacity\". Intuitively, a model with more\n",
    "parameters will have more \"memorization capacity\" and therefore will be able to easily\n",
    "learn a perfect dictionary-like mapping between training samples and their targets, a\n",
    "mapping without any generalization power. For instance, a model with 500,000 binary\n",
    "parameters could easily be made to learn the class of every digits in the MNIST training\n",
    "set: we would only need 10 binary parameters for each of the 50,000 digits. Such a model\n",
    "would be useless for classifying new digit samples. Always keep this in mind: deep\n",
    "learning models tend to be good at fitting to the training data, but the real challenge is\n",
    "generalization, not fitting.\n",
    "\n",
    "防止过拟合最简单的方法就是缩减模型的尺寸，也就是减少模型中参数的数量（该数量是由模型层数和每层隐藏单元数决定的）。在深度学习中，一个模型中可训练的参数数量也通常被称作模型的“容量”。直观上来看，一个有着更多参数的模型也就有着更多“可记忆容量”，因此能够更加容易学习到训练样本和它们目标之间完美的映射关系，这种映射不包含任何泛化能力。例如，一个有着50万个二进制参数的模型能够很容易的学习到MNIST训练集中学习到分类目标：我们只需要为所有5万个数字样本每个准备10个二进制参数就足够了。这样一个模型在分类新的数字样本时却是没有帮助的。永远应该记住：深度学习模型倾向于更好的拟合训练数据，但是真正的挑战是泛化，而非拟合。\n",
    "\n",
    "> On the other hand, if the network has limited memorization resources, it will not be\n",
    "able to learn this mapping as easily, and thus, in order to minimize its loss, it will have to\n",
    "resort to learning compressed representations that have predictive power regarding the\n",
    "targets—precisely the type of representations that we are interested in. At the same time,\n",
    "keep in mind that you should be using models that have enough parameters that they\n",
    "won’t be underfitting: your model shouldn’t be starved for memorization resources.\n",
    "There is a compromise to be found between \"too much capacity\" and \"not enough\n",
    "capacity\".\n",
    "\n",
    "另一方面，如果网络只有受限制的记忆资源，它就无法容易的学习到训练样本到目标的映射，因此为了最小化损失，它不得不寻求针对目标获得具有预测能力的数据压缩表示形式，恰好就是我们感兴趣的那种表示形式。但是同时要注意的是你应该使用有足够数量参数的模型，避免它欠拟合：你的模型不应该匮乏记忆资源。在“过大容量”和“容量不足”之间存在着一个折中。\n",
    "\n",
    "> Unfortunately, there is no magical formula to determine what the right number of\n",
    "layers is, or what the right size for each layer is. You will have to evaluate an array of\n",
    "different architectures (on your validation set, not on your test set, of course) in order to\n",
    "find the right model size for your data. The general workflow to find an appropriate\n",
    "model size is to start with relatively few layers and parameters, and start increasing the\n",
    "size of the layers or adding new layers until you see diminishing returns with regard to\n",
    "the validation loss.\n",
    "\n",
    "不幸的是，并不存在着一个魔术公式来决定模型的层数，或者每层的合适尺寸。你需要在不同的结构上对模型进行验证（当然是在验证集上，而不是在测试集上），来找到合适的模型容量。通常我们的做法是从一个相对较少的层数和参数开始，然后逐渐增加层的单元容量或者增加层次直到你观察到验证损失值重新开始增加为止。\n",
    "\n",
    "> Let’s try this on our movie review classification network. Our original network was\n",
    "as such:\n",
    "\n",
    "让我们在影评分类网络中试验一下。之前我们的网络是这样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 译者注，为了重复影评分类试验，我们需要数据集以及预处理过程\n",
    "# 下面重新载入数据集和进行相应处理\n",
    "\n",
    "from tensorflow.keras import datasets\n",
    "(train_data, train_labels), (test_data, test_labels) = datasets.imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from Chapter3_Getting_started_with_neural_networks.ipynb\n"
     ]
    }
   ],
   "source": [
    "import nbimporter\n",
    "\n",
    "from Chapter3_Getting_started_with_neural_networks import vectorize_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 3s 210us/sample - loss: 0.4415 - accuracy: 0.8249 - val_loss: 0.3350 - val_accuracy: 0.8628\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.2380 - accuracy: 0.9175 - val_loss: 0.2762 - val_accuracy: 0.8908\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 69us/sample - loss: 0.1695 - accuracy: 0.9426 - val_loss: 0.2804 - val_accuracy: 0.8871\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 45us/sample - loss: 0.1312 - accuracy: 0.9541 - val_loss: 0.3023 - val_accuracy: 0.8819\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 46us/sample - loss: 0.1002 - accuracy: 0.9669 - val_loss: 0.3264 - val_accuracy: 0.8797\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 46us/sample - loss: 0.0760 - accuracy: 0.9766 - val_loss: 0.3966 - val_accuracy: 0.8743\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 58us/sample - loss: 0.0570 - accuracy: 0.9841 - val_loss: 0.4075 - val_accuracy: 0.8709\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 69us/sample - loss: 0.0394 - accuracy: 0.9904 - val_loss: 0.4486 - val_accuracy: 0.8743\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.0307 - accuracy: 0.9911 - val_loss: 0.4987 - val_accuracy: 0.8696\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 2s 108us/sample - loss: 0.0171 - accuracy: 0.9970 - val_loss: 0.5601 - val_accuracy: 0.8670\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 44us/sample - loss: 0.0140 - accuracy: 0.9970 - val_loss: 0.6098 - val_accuracy: 0.8662\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 84us/sample - loss: 0.0075 - accuracy: 0.9987 - val_loss: 0.6738 - val_accuracy: 0.8677\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 58us/sample - loss: 0.0045 - accuracy: 0.9995 - val_loss: 0.7441 - val_accuracy: 0.8661\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 55us/sample - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.8088 - val_accuracy: 0.8645\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 48us/sample - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.9241 - val_accuracy: 0.8551\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 42us/sample - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.9749 - val_accuracy: 0.8613\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 7.3862e-04 - accuracy: 0.9999 - val_loss: 1.0273 - val_accuracy: 0.8616\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 9.8828e-04 - accuracy: 0.9997 - val_loss: 1.0842 - val_accuracy: 0.8641\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 55us/sample - loss: 1.3482e-04 - accuracy: 1.0000 - val_loss: 1.1414 - val_accuracy: 0.8616\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.0018 - accuracy: 0.9994 - val_loss: 1.2491 - val_accuracy: 0.8580\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_old = model.fit(x_train, y_train, epochs=20, batch_size=256, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now let’s try to replace it with this smaller network:\n",
    "\n",
    "下面我们将它换成一个小一点的网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 1s 93us/sample - loss: 0.5897 - accuracy: 0.7193 - val_loss: 0.5067 - val_accuracy: 0.8497\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 48us/sample - loss: 0.4268 - accuracy: 0.8821 - val_loss: 0.4017 - val_accuracy: 0.8702\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 53us/sample - loss: 0.3137 - accuracy: 0.9217 - val_loss: 0.3288 - val_accuracy: 0.8853\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 50us/sample - loss: 0.2399 - accuracy: 0.9344 - val_loss: 0.2931 - val_accuracy: 0.8878\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 47us/sample - loss: 0.1933 - accuracy: 0.9448 - val_loss: 0.2858 - val_accuracy: 0.8870\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 47us/sample - loss: 0.1618 - accuracy: 0.9545 - val_loss: 0.2776 - val_accuracy: 0.8888\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 44us/sample - loss: 0.1388 - accuracy: 0.9609 - val_loss: 0.2815 - val_accuracy: 0.8874\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 44us/sample - loss: 0.1207 - accuracy: 0.9667 - val_loss: 0.2918 - val_accuracy: 0.8858\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 44us/sample - loss: 0.1061 - accuracy: 0.9719 - val_loss: 0.3013 - val_accuracy: 0.8837\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 44us/sample - loss: 0.0934 - accuracy: 0.9762 - val_loss: 0.3155 - val_accuracy: 0.8832\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 47us/sample - loss: 0.0830 - accuracy: 0.9793 - val_loss: 0.3319 - val_accuracy: 0.8817\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 45us/sample - loss: 0.0728 - accuracy: 0.9823 - val_loss: 0.3498 - val_accuracy: 0.8799\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 45us/sample - loss: 0.0647 - accuracy: 0.9849 - val_loss: 0.3686 - val_accuracy: 0.8763\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 78us/sample - loss: 0.0568 - accuracy: 0.9887 - val_loss: 0.3882 - val_accuracy: 0.8775\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 45us/sample - loss: 0.0502 - accuracy: 0.9893 - val_loss: 0.4119 - val_accuracy: 0.8745\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 44us/sample - loss: 0.0446 - accuracy: 0.9911 - val_loss: 0.4439 - val_accuracy: 0.8722\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 45us/sample - loss: 0.0392 - accuracy: 0.9930 - val_loss: 0.4688 - val_accuracy: 0.8700\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 44us/sample - loss: 0.0348 - accuracy: 0.9935 - val_loss: 0.4815 - val_accuracy: 0.8707\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 46us/sample - loss: 0.0308 - accuracy: 0.9943 - val_loss: 0.5110 - val_accuracy: 0.8692\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 45us/sample - loss: 0.0269 - accuracy: 0.9955 - val_loss: 0.5372 - val_accuracy: 0.8672\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_new = model.fit(x_train, y_train, epochs=20, batch_size=256, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here’s a comparison of the validation losses of the original network and the smaller\n",
    "network. The dots are the validation loss values of the smaller network, and the crosses\n",
    "are the initial network (remember: a lower validation loss signals a better model).\n",
    "\n",
    "下面是原来的网络和现在这个更小的网络验证损失的对比。原点表示更小网络的验证损失，×号表示原来网络的验证损失（注意：更低的验证损失标志着更好的模型性能）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fae90df5850>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de3xU9Z3/8dcHCLBcvBItyiWoULkEFALIsiqp19oVvLRefrRbsZZHY7XW9edtMWih9mdZ2/5Wqyi2Fq3UrpdV2a5trSVAtWIDLKLgCkiRBq1EKmBEJeBn/zgnYQiZySQzZ67v5+Mxj5k5c+bMJ4fhfOZ8L59j7o6IiBSvTtkOQEREskuJQESkyCkRiIgUOSUCEZEip0QgIlLkumQ7gPbq06ePl5WVZTsMEZG8smLFivfcvbS11/IuEZSVlbF8+fJshyEiklfM7K14r6lpSESkyCkRiIgUOSUCEZEil3d9BK1pbGykrq6Ojz/+ONuhSJK6d+9Ov379KCkpyXYoIkWvIBJBXV0dvXv3pqysDDPLdjjSBndn27Zt1NXVMWjQoGyHI1L0CqJp6OOPP+bwww9XEsgTZsbhhx+uMziRJMyZAzU1+y+rqQmWp0tBJAJASSDP6N9LJDljx8JFF+1LBjU1wfOxY9P3GQXRNCQiUqgqK+Gxx4KDf1UVzJ0bPK+sTN9nFMwZQbbV1dUxZcoUBg8ezLHHHss111zD7t27W1337bff5otf/GKb2zznnHPYvn17h+K57bbbuPPOOzv03mTNnz+fq666KuV1RCSxysogCcyeHdynMwlAESaCKNrb3J0LLriA8847j/Xr17Nu3ToaGhqYMWPGAevu2bOHo446iieeeKLN7T777LMccsghHQ9MRApCTU1wJlBdHdy3PIalqugSQRTtbYsWLaJ79+5MmzYNgM6dO/OjH/2IBx98kF27djF//nwmT57M5z73OU477TQ2bdrEiBEjANi1axcXXXQRw4YN4/zzz2f8+PHNJTTKysp477332LRpE0OHDuXrX/86w4cP58wzz+Sjjz4C4IEHHmDs2LGMGjWKCy+8kF27diWM9bLLLqOqqoqTTjqJY445hsWLF3P55ZczdOhQLrvssub1Hn30UcrLyxkxYgQ33nhj8/Kf/exnDBkyhHHjxvHiiy82L6+vr+fCCy9k7NixjB07dr/XRKTjmo5Rjz0Gs2btayZKZzIoukQQ2942c+a+HZzKqdaaNWsYM2bMfssOOuggBgwYwIYNGwBYuXIlTzzxBEuWLNlvvXvvvZdDDz2UtWvXMnv2bFasWNHqZ6xfv55vfvObrFmzhkMOOYQnn3wSgAsuuIDa2lpeeeUVhg4dyk9/+tM2433//fd56aWX+NGPfsTkyZO59tprWbNmDa+++iqrVq3i7bff5sYbb2TRokWsWrWK2tpann76ad555x1uvfVWXnzxRV544QXWrl3bvM1rrrmGa6+9ltraWp588kmuuOKKdu1DEWldbe3+x6imY1htbfo+oyg7i2Pb26qr09/e1pozzjiDww477IDlL7zwAtdccw0AI0aMYOTIka2+f9CgQZxwwgkAjBkzhk2bNgHw2muvccstt7B9+3YaGho466yz2ozl3HPPxcwoLy/nyCOPpLy8HIDhw4ezadMm3nrrLSZNmkRpaVCocOrUqSxduhRgv+UXX3wx69atA+D555/fLzHs3LmThoaGNmMRkcRuuOHAZZWV6ixOWbrb24YNG3bAL/mdO3eyefNmjjvuOAB69uyZ0md069at+XHnzp3Zs2cPEDT1/PjHP+bVV1/l1ltvTWpsftO2OnXqtN92O3Xq1Lzd9vr0009ZtmwZq1atYtWqVWzZsoVevXp1aFsikllFlwiiaG877bTT2LVrFw8//DAAe/fu5brrruOyyy6jR48eCd87ceJEHnvsMQDWrl3Lq6++2q7P/uCDD+jbty+NjY0sWLCgY39AC+PGjWPJkiW899577N27l0cffZRTTz2V8ePHs2TJErZt20ZjYyOPP/5483vOPPNM7r777ubnq1atSkssIhK9yBKBmT1oZlvN7LU4r081s9Vm9qqZ/dHMRkUVS6wo2tvMjKeeeorHH3+cwYMHM2TIELp37873vve9Nt975ZVXUl9fz7Bhw7jlllsYPnw4Bx98cNKfPXv2bMaPH8/EiRM5/vjjO/5HxOjbty933HEHlZWVjBo1ijFjxjBlyhT69u3LbbfdxoQJE5g4cSJDhw5tfs9dd93F8uXLGTlyJMOGDeO+++5LSywiEj1z92g2bHYK0AA87O4jWnn974HX3f19M/s8cJu7j29ruxUVFd7ywjSvv/76fgelfLJ3714aGxvp3r07b775JqeffjpvvPEGXbt2zXZokcvnfzeRfGNmK9y9orXXIussdvelZlaW4PU/xjxdBvSLKpZctmvXLiorK2lsbMTduffee4siCYhI7siVUUNfA34d70Uzmw5MBxgwYECmYsqI3r1769KbIpJVWe8sNrNKgkRwY7x13H2eu1e4e0XT0EUREUmPrJ4RmNlI4CfA5919WzZjEREpVlk7IzCzAcB/AF9x93XZikNEpNhFdkZgZo8Ck4A+ZlYH3AqUALj7fcBM4HDg3rA2/Z54PdoiIhKdyM4I3P1Sd+/r7iXu3s/df+ru94VJAHe/wt0PdfcTwlteJ4Hbb7+d4cOHM3LkSE444QRefvnltGy3aXZubKG6XDBp0qQ2O7mTWUdEsi/rncXZsGABlJVBp07BfaoTcl966SV+9atfsXLlSlavXs3zzz9P//790xFqh3W0VISIFJ+iSwQLFsD06fDWW+Ae3E+fnloyeOedd+jTp09z3Z4+ffpw1FFHAUEp6ZtvvpkTTjiBiooKVq5cyVlnncWxxx7bPPu2oaGB0047jdGjR1NeXs4zzzyT8PP27t3L9ddfz9ixYxk5ciT3338/AIsXL+bkk09m8uTJDBs27ID39erVi+uvv57hw4dz+umn86c//YlJkyZxzDHHsHDhQiC4/vO0adMoLy/nxBNPpCasvfHRRx9xySWXMHToUM4///zmMtgAzz33HBMmTGD06NF86UtfUrE5kXzj7nl1GzNmjLe0du3aA5bFM3Cge5AC9r8NHJj0Jg7wwQcf+KhRo3zw4MFeVVXlixcvjvm8gX7vvfe6u/u3v/1tLy8v9507d/rWrVv9iCOOcHf3xsZG37Fjh7u719fX+7HHHuuffvqpu7v37NnT3d3//Oc/+/Dhw93d/f777/fZs2e7u/vHH3/sY8aM8Y0bN3pNTY336NHDN27c2GqcgD/77LPu7n7eeef5GWec4bt37/ZVq1b5qFGj3N39zjvv9GnTprm7++uvv+79+/f3jz76yH/wgx80L3/llVe8c+fOXltb6/X19X7yySd7Q0ODu7vfcccd/p3vfMfd3U899VSvra2Nu9/a8+8mIqkBlnuc42quTCjLmM2b27c8Gb169WLFihX84Q9/oKamhosvvpg77rij+UIvkydPBqC8vJyGhgZ69+5N79696datG9u3b6dnz578y7/8C0uXLqVTp05s2bKFd999l8985jOtft5zzz3H6tWrm69ytmPHDtavX0/Xrl0ZN24cgwYNavV9Xbt25eyzz26OpVu3bpSUlFBeXt5c1vqFF17g6quvBuD4449n4MCBrFu3jqVLl/Ktb30LgJEjRzaXy162bBlr165l4sSJAOzevZsJEyZ0fGeKFJg5c4ILX8WWja6pCeqbtVZiOhuKLhEMGBA0B7W2PBWdO3dm0qRJTJo0ifLych566KHmRNBW2ecFCxZQX1/PihUrKCkpoaysLGE5aXfn7rvvPuDaA4sXL05Y7rqkpIRwhNZ+saRSftrdOeOMM3j00Uc79H6RQtd0VcSmYpexFZBzRdH1Edx+O7SsDN2jR7C8o9544w3Wr1/f/HzVqlUMHDgw6ffv2LGDI444gpKSEmpqanirtUwV46yzzmLu3Lk0NjYCsG7dOj788MOOBd/CySef3FzOet26dWzevJnPfvaznHLKKfziF78AgovhrF69GoCTTjqJF198sflKbB9++GHzxWpEJJqrIqZb0Z0RTJ0a3M+YETQHDRgQJIGm5R3R0NDA1Vdfzfbt2+nSpQvHHXcc8+bNa0dMUzn33HMpLy+noqKizXLSV1xxBZs2bWL06NG4O6WlpTz99NMd/wNiXHnllVRVVVFeXk6XLl2YP38+3bp1o6qqimnTpjF06FCGDh3afGnO0tJS5s+fz6WXXsonn3wCwHe/+12GDBmSlnhECkE2rorYHpGVoY5KoZWhLmb6d5Ni0dQcVFUVXBUxG2cEicpQF13TkIhIJkVxVcR0UyIQEYlQFFdFTLeC6SNw9+YRMZL78q1JUqSjWhsiWlmZW/0EBXFG0L17d7Zt26aDS55wd7Zt20b37t2zHYqIUCBnBP369aOuro76+vpshyJJ6t69O/36FeXVSUVyTkEkgpKSkrizaUVEJLGCaBoSEZGOUyIQEUlgzpwDh3rW1ATLC4USgYhIAk21gpqSQdO8gLFjsxtXOhVEH4GISFRiawVlc2ZwlHRGICLShthaQVVVhZUEQIlARKRNNTXBmUB1dXCfS+Uh0kGJQEQkgXyoFZQqJQIRkQTyoVZQqgqiDLWIiCSmMtQiIhKXEoGISJFTIhARKXJKBCJS0IqhRESqlAhEpKAVQ4mIVKnEhIgUtGIoEZEqnRGISMEr9BIRqVIiEJGCV+glIlKlRCAiBa0YSkSkKrJEYGYPmtlWM3stzutmZneZ2QYzW21mo6OKRUSKVzGUiEhVZCUmzOwUoAF42N1HtPL6OcDVwDnAeODf3H18W9tViQkRkfbLSokJd18K/C3BKlMIkoS7+zLgEDPrG1U8IiLSumz2ERwN/CXmeV247ABmNt3MlpvZ8vr6+owEJyJSLPKis9jd57l7hbtXlJaWZjscEZGCks1EsAXoH/O8X7hMRKSZSkREL5uJYCHwT+HooZOAHe7+ThbjEZEcpBIR0YusxISZPQpMAvqYWR1wK1AC4O73Ac8SjBjaAOwCpkUVi4jkL5WIiF5kicDdL23jdQe+GdXni0jhiC0RUV2tJJBuedFZLCLFTSUioqVEICI5TSUioqdEICI5TSUiohdZiYmoqMSEiEj7ZaXEhIiI5AclAhGRIqdEICKR0szg3KdEICKR0szg3NdmIjCziWbWM3z8ZTP7oZkNjD40ESkEsTODZ87cNxRUk8JyRzJnBHOBXWY2CrgOeBN4ONKoRKSg6OLxuS2ZRLAnLAcxBfixu98D9I42LBEpJJoZnNuSqTX0gZndDHwZOMXMOhEWjxMRaUvszODKyuCm5qHckswZwcXAJ8DX3P2vBNcN+NdIoxKRgqGZwbmvzZnFYUfxx+6+18yGAMcDv3b3xkwE2JJmFouItF+qM4uXAt3M7GjgOeArwPz0hSciItmUTCIwd98FXADc6+5fAkZEG5aIiGRKUonAzCYAU4H/asf7REQkDyRzQP82cDPwlLuvMbNjAA3+EhEpEG0OH3X3JcASM+tlZr3cfSPwrehDExGRTEimxES5mf03sAZYa2YrzGx49KGJiEgmJNM0dD/wz+4+0N0HEJSZeCDasEREJFOSSQQ93b25T8DdFwM9I4tIREQyKpkSExvNrBr4efj8y8DG6EISEZFMSuaM4HKgFPiP8FYaLhMRkQKQzKih99EoIRGRghU3EZjZfwJxCxG5++RIIhIRkYxKdEZwZ8aiEBGRrImbCMKJZCIiUuBUM0hEpMgpEYiIFDklAhFJaM6cA68xXFMTLJfCkEytoSFm9oCZPWdmi5puyWzczM42szfMbIOZ3dTK6wPMrMbM/tvMVpvZOR35I0QkOmPHBtcYbkoGTdcgHjs2u3FJ+iQzs/hx4D6C+kJ7k92wmXUG7gHOAOqAWjNb6O5rY1a7BXjM3eea2TDgWaAs2c8Qkeg1XWP4oougqgrmztWF5wtNMolgj7vP7cC2xwEbwrLVmNkvgSlAbCJw4KDw8cHA2x34HBGJWGVlkARmz4bqaiWBQpNMH8F/mtmVZtbXzA5ruiXxvqOBv8Q8rwuXxboN+LKZ1RGcDVzd2obMbLqZLTez5fX19Ul8tIikU01NcCZQXR3ct+wzkPyWTCL4KnA98EdgRXhbnqbPvxSY7+79gHOAn5vZATG5+zx3r3D3itLS0jR9tIgko6lP4LHHYNasfc1ESgaFI5laQ4M6uO0tQP+Y5/3CZbG+Bpwdfs5LZtYd6ANs7eBnikia1dbu3yfQ1GdQW6smokLRZiIwsxKgCjglXLQYuN/dG9t4ay0w2MwGESSAS4D/02KdzcBpwHwzGwp0B9T2I5JDbrjhwGWVlUoChSSZzuK5QAlwb/j8K+GyKxK9yd33mNlVwG+BzsCD7r7GzGYBy919IeHVzszsWoKO48vcPW6hOxERSb9kEsFYdx8V83yRmb2SzMbd/VmCTuDYZTNjHq8FJiazLRERiUYyncV7zezYpidmdgztmE8gIiK5LZkzguuBGjPbCBgwEJgWaVQiIpIxyYwa+r2ZDQY+Gy56w90/iTYsERHJlLhNQ2b2ufD+AuALwHHh7QvhMhHJAyoaJ21J1Edwanh/biu3f4w4LhFJExWNk7YkukLZreHDWe7+59jXwrkBIpIHVDRO2pLMqKEnW1n2RLoDEZHoxBaNq6pSEpD9xT0jMLPjgeHAwS36BA4imAEsInmiZdE4zQyWWIlGDX2WoC/gEIJ+gSYfAF+PMigRSZ/YonFNCSD2uUiiPoJngGfMbIK7v5TBmEQkjVQ0TtpibZX2CSuCfo2gmai5ScjdL482tNZVVFT48uXpqoItIlIczGyFu1e09loyncU/Bz4DnAUsISgn/UH6whMRkWxKJhEc5+7VwIfu/hDB5LLx0YYlIiKZkkwiaLruwHYzG0FwbeEjogtJREQyKZmic/PM7FCgGlgI9AJmJn6LiIjki2SKzv0kfLgEOCbacEREJNMSTSj750RvdPcfpj8cERHJtER9BL3DWwXBNYuPDm/fAEZHH5qIgKqHSvTiJgJ3/467f4dguOhod7/O3a8DxgADMhWgSLFT9VCJWjKdxUcCu2Oe7w6XiUgGqHqoRC2ZRPAw8Cczeyp8fh4wP7KIROQAsdVDq6uVBCS92pxH4O63E1yj+P3wNs3d/1/UgYnIPi2rh7bsMxBJRaJRQwe5+04zOwzYFN6aXjvM3f8WfXgiouqhErVETUO/IChDvQKIrUxn4XPNKRDJAFUPlai1WX0016j6qIhI+yWqPpqoaSjhXAF3X5lqYCIikn2JmoZ+kOA1Bz6X5lhERCQLEl2hTK2PIiJFIJl5BITlp4ex/xXKHo4qKBERyZw2E4GZ3QpMIkgEzwKfB14gmGgmIiJ5LpkL03wROA34q7tPA0YRXJxGRJKgonGS65JJBB+5+6fAHjM7CNgK9I82LJHCoaJxkuuSSQTLzewQ4AGCyWUrgZeS2biZnW1mb5jZBjO7Kc46F5nZWjNbY2a/SDpykTwRWzRu5kzNCpbck2gewT3AL9z9ynDRfWb2G+Agd1/d1obNrDNwD3AGUAfUmtlCd18bs85g4GZgoru/b2a6FrIUJBWNk1yW6IxgHXCnmW0yszlmdqK7b0omCYTGARvcfaO77wZ+CUxpsc7XgXvc/X0Ad9/a3j9AJB+oaJzkskQXpvk3d58AnApsAx40s/8xs1vNbEgS2z4a+EvM87pwWawhwBAze9HMlpnZ2a1tyMymm9lyM1teX1+fxEeL5I7YonGzZu1rJlIykFyRTBnqt9z9++5+InApwfUIXk/T53cBBhMMT70UeCDsj2gZwzx3r3D3itLS0jR9tEhmJCoaJ5ILkplH0IVg7sAlBMNIFwO3JbHtLew/uqhfuCxWHfCyuzcCfzazdQSJQf9FpGDccMOBy5rKSYvkgrhnBGZ2hpk9SHCw/jrwX8Cx7n6Juz+TxLZrgcFmNsjMuhIkkoUt1nma4GwAM+tD0FS0sd1/hYiIdFiiM4KbCa5JcF1TZ257uPseM7sK+C3QGXjQ3deY2SxgubsvDF8708zWAnuB6919W7v/ChER6TBdj0CkDXPmBJO/YptyamqCNv7Wmn1EclGi6xEkM6FMpKhpZrAUuqSqj4oUs9iZwVVVwTwAzQyWQqIzApEkxM4MrqpSEpDCokQgkgTNDJZCpkQg0gbNDJZsW7AAysqgU6fgfsGC9G5fiUCkDZoZLNm0YAFMnw5vvQXuwf306elNBho+KiKSw8rKgoN/SwMHwqZNyW9Hw0dFRPLU5s3tW94RSgQiIjlswID2Le8IJQIRkYil0tl7++3Qo8f+y3r0CJanS1Ekgqh73EVE4km1s3fqVJg3L+gTMAvu580LlqdLwXcWN/0j7Nq1b1mPHunfkSIirUlXZ2+qirqzeMaM/ZMABM9nzMhOPJJ5c+YcOOa/piZYLhK1THT2pqrgE0E+/CNItFQ0TrIpE529qSr4RJAP/wgSrdiicTNn7pslrHpBkgmZ6OxNVcEngnz4R5DoqWicpCKVASeZ6OxNmbvn1W3MmDHeXo884j5woLtZcP/II+3ehOS5RYvc+/Rxr64O7hctynZEki8eecS9Rw/3YMxPcOvRI/+OIwRXhmz1uFrwo4ZEYovGVVYe+FwkkVwZ9ZOqoh41JKKicZJK004xDDjRGYGIFLRU5xLpjEBEJM+lOpeoGAacKBGISEFLtWknL0b9pEiJQHKeZgZLKtIxl2jq1KAZ6NNPg/tCSgKgRCB5QDODJderd+Y7JQLJeZoZXNzyoXpnvtOoIckbM2cGM4Orq4OLyEtxKJRRO9mmUUOS92pqYO7cIAnMnXtgn4EUrmIYx59tSgSS82JnAs+ata+ZSMmgOKhwZPSUCCTnaWZwcVNnb/SUCCTn3XDDgR3DlZXBcskPBV+9M88pEUjkNA+guKU66gcKfxx/tikRJCGVXzOieQDFTpeLzX2RJgIzO9vM3jCzDWZ2U4L1LjQzN7NWhzZlUzp+zeS7VH/Rax5AcdOon9wXWSIws87APcDngWHApWY2rJX1egPXAC9HFUsq9GsmPb/odYWw4qVRP7kvyjOCccAGd9/o7ruBXwJTWllvNvB94OMIY+kw/ZpJzy96zQPIbyrxUODiXbos1RvwReAnMc+/Avy4xTqjgSfDx4uBijjbmg4sB5YPGDAggou4xTdw4P6XqGu6DRyY0TByQnV18LdXV7fvfU2XiWy6PGTL55Lb0nGpRl0uNvtIcKnKrHUWm1kn4IfAdW2t6+7z3L3C3StKS0ujDy6Gfs0EUvlFr3kA+S0dzaMa9ZPbukS47S1A/5jn/cJlTXoDI4DFZgbwGWChmU1295wpJtT0hZ0xI2gOGjAgSALF9EVueY3fysr2NQ+1Nt6/aTuS+9Q8WviiPCOoBQab2SAz6wpcAixsetHdd7h7H3cvc/cyYBmQU0mgSbH/mtEv+vyXShu/OnsLX2RnBO6+x8yuAn4LdAYedPc1ZjaLoK1qYeItSK7QL/r81vKavU1DoCG5HzW33976NX+LrXm0kKkMtUiBS0cZ5wULirt5tBAkKkOtRCBS4Dp1Csb6tGQWNHdKcdD1CETynNr4JUpKBCI5LtUyJxoCLW1RIhDJcamO41cZZ2lLwScClUCWfJeOcfzFPgRaEiv4RJALJZBVxlrUxi85LV7tiVy9jRkzpt01Nppq21RXZ77GTTrqtKTi+98/8O9dtChYLpmR6ncg298hKQwkqDWU9QN7e28dSQTuHS+YlqpsF61TwbfsS8d3QEXbJFVFnwiyeUZg1vpBwCxzMWTz7y8UqRyIc+E7IJIoERR8H0FswbRZs/bV1c9UPfxcaN/VRWFSk+rwzVz4DogkUvCJINsF03JhDLcuCpOaVIdv5sJ3QCSheKcKuXrraB9BNqXSrJBqZ6/6CALZbtpRG79kGwmahlRrKMfV1MDkycEvyPp6KC0Nfo0uXJhcE8+cOcFQ2dh1a2qCM6LWqooWopbVNyHYn8lOqkpH0TaRbFOtoTz29tvQ2Ahbtwa/Q7duDZ6//XZy77/hhgMTRmVl/iWBVMbhq2lHJDElghw3YwZ88sn+yz75pH2XCcz3CW2pdtamOjNXJRqk0CkRRCzVEhepHsRSPYimSzZ/0adj1I5KNEghUyJoQ6oH8lRLXJSWtm95S+m48DikdiDP9i96Ne2ItCFeL3Ku3jI9aigdo25SmdB18cXu3brtP1qlW7dgeTLSNeIllRIHqc6s1cxckdRR7DOLU5WOmbmplLhI5SCWjoNoqttINRmp1o5I6pQI0iCVA3m+F71L9UCuX/Qi2adEkKJUDuS5MKEr1YNoqgdy/aIXyT4lghSkeiAvhDLQ6TiQ6xe9SHYlSgSaWdwGzcwNLFgQjDTavDkYdnn77RpCKZJPEs0sViIQESkCKjEhIiJxKRGIiBQ5JQIRkSKnRCAiUuSUCEREilzejRoys3qglcuE5IQ+wHvZDiKBXI8Pcj9GxZcaxZeaVOIb6O6tlqvMu0SQy8xsebzhWbkg1+OD3I9R8aVG8aUmqvjUNCQiUuSUCEREipwSQXrNy3YAbcj1+CD3Y1R8qVF8qYkkPvURiIgUOZ0RiIgUOSUCEZEip0TQTmbW38xqzGytma0xs2taWWeSme0ws1XhbWaGY9xkZq+Gn31AqVYL3GVmG8xstZmNzmBsn43ZL6vMbKeZfbvFOhnff2b2oJltNbPXYpYdZma/M7P14f2hcd771XCd9Wb21QzG969m9j/hv+FTZnZInPcm/D5EGN9tZrYl5t/xnDjvPdvM3gi/jzdlML5/j4ltk5mtivPeSPdfvGNKRr9/8S5UoFucK/lAX2B0+Lg3sA4Y1mKdScCvshjjJqBPgtfPAX4NGHAS8HKW4uwM/JVgoktW9x9wCjAaeC1m2RzgpvDxTcD3W3nfYcDG8P7Q8PGhGYrvTKBL+Pj7rcWXzPchwvhuA/5vEt+BN4FjgK7AKy3/P0UVX4vXfwDMzMb+i3dMyeT3T2cE7eTu77j7yvDxB8DrwNHZjardpgAPe2AZcIiZ9c1CHKcBb7p71meKu/tS4G8tFk8BHgofPwSc18pbzxMLv+YAAASxSURBVAJ+5+5/c/f3gd8BZ2ciPnd/zt33hE+XAf3S/bnJirP/kjEO2ODuG919N/BLgv2eVoniMzMDLgIeTffnJiPBMSVj3z8lghSYWRlwIvByKy9PMLNXzOzXZjY8o4GBA8+Z2Qozm97K60cDf4l5Xkd2ktklxP/Pl8391+RId38nfPxX4MhW1smVfXk5wVlea9r6PkTpqrDp6sE4TRu5sP9OBt519/VxXs/Y/mtxTMnY90+JoIPMrBfwJPBtd9/Z4uWVBM0do4C7gaczHN4/uPto4PPAN83slAx/fpvMrCswGXi8lZezvf8O4MF5eE6OtTazGcAeYEGcVbL1fZgLHAucALxD0PySiy4l8dlARvZfomNK1N8/JYIOMLMSgn+wBe7+Hy1fd/ed7t4QPn4WKDGzPpmKz923hPdbgacITr9jbQH6xzzvFy7LpM8DK9393ZYvZHv/xXi3qcksvN/ayjpZ3Zdmdhnwj8DU8GBxgCS+D5Fw93fdfa+7fwo8EOdzs73/ugAXAP8eb51M7L84x5SMff+UCNopbE/8KfC6u/8wzjqfCdfDzMYR7OdtGYqvp5n1bnpM0KH4WovVFgL/FI4eOgnYEXMKmilxf4Vlc/+1sBBoGoXxVeCZVtb5LXCmmR0aNn2cGS6LnJmdDdwATHb3XXHWSeb7EFV8sf1O58f53FpgsJkNCs8SLyHY75lyOvA/7l7X2ouZ2H8JjimZ+/5F1RNeqDfgHwhO0VYDq8LbOcA3gG+E61wFrCEYAbEM+PsMxndM+LmvhDHMCJfHxmfAPQSjNV4FKjK8D3sSHNgPjlmW1f1HkJTeARoJ2lm/BhwO/B5YDzwPHBauWwH8JOa9lwMbwtu0DMa3gaB9uOl7eF+47lHAs4m+DxmK7+fh92s1wUGtb8v4wufnEIyUeTOT8YXL5zd972LWzej+S3BMydj3TyUmRESKnJqGRESKnBKBiEiRUyIQESlySgQiIkVOiUBEpMgpEYiEzGyv7V8ZNW2VMM2sLLbypUgu6ZLtAERyyEfufkK2gxDJNJ0RiLQhrEc/J6xJ/yczOy5cXmZmi8Kiar83swHh8iMtuD7AK+Ht78NNdTazB8Ka88+Z2d+F638rrEW/2sx+maU/U4qYEoHIPn/Xomno4pjXdrh7OfBj4P+Hy+4GHnL3kQQF3+4Kl98FLPGgaN5oghmpAIOBe9x9OLAduDBcfhNwYridb0T1x4nEo5nFIiEza3D3Xq0s3wR8zt03hsXB/uruh5vZewRlExrD5e+4ex8zqwf6ufsnMdsoI6gbPzh8fiNQ4u7fNbPfAA0EVVaf9rDgnkim6IxAJDke53F7fBLzeC/7+ui+QFD7aTRQG1bEFMkYJQKR5Fwcc/9S+PiPBNUyAaYCfwgf/x6oAjCzzmZ2cLyNmlknoL+71wA3AgcDB5yViERJvzxE9vk72/8C5r9x96YhpIea2WqCX/WXhsuuBn5mZtcD9cC0cPk1wDwz+xrBL/8qgsqXrekMPBImCwPucvftafuLRJKgPgKRNoR9BBXu/l62YxGJgpqGRESKnM4IRESKnM4IRESKnBKBiEiRUyIQESlySgQiIkVOiUBEpMj9L2FbLPvrPXfOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = range(1, 21)\n",
    "plt.plot(x, history_old.history['val_loss'], 'bx', label='Original model')\n",
    "plt.plot(x, history_new.history['val_loss'], 'bo', label='Smaller model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As you can see, the smaller network starts overfitting later than the reference one\n",
    "(after 6 epochs rather than 4) and its performance degrades much more slowly once it\n",
    "starts overfitting.\n",
    "\n",
    "从上图你可以看到，较小的网络会比原始有着较大容量的网络更晚开始出现过拟合（从第六次迭代开始而不是从第四次迭代开始），并且在开始过拟合后它的性能下降比较大网络要慢许多。\n",
    "\n",
    "> Now, for kicks, let’s add to this benchmark a network that has much more capacity,\n",
    "far more than the problem would warrant:\n",
    "\n",
    "下面，为了更进一步的对比，让我们试验一个更大容量的网络，远远超过本问题所需要的容量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 4s 237us/sample - loss: 0.4470 - accuracy: 0.8063 - val_loss: 0.3081 - val_accuracy: 0.8705\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 3s 179us/sample - loss: 0.1950 - accuracy: 0.9242 - val_loss: 0.3349 - val_accuracy: 0.8699\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 3s 181us/sample - loss: 0.0815 - accuracy: 0.9727 - val_loss: 0.3797 - val_accuracy: 0.8883\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 3s 195us/sample - loss: 0.0398 - accuracy: 0.9897 - val_loss: 0.5692 - val_accuracy: 0.8848\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 3s 179us/sample - loss: 0.0822 - accuracy: 0.9889 - val_loss: 0.5303 - val_accuracy: 0.8869\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 3s 178us/sample - loss: 0.0435 - accuracy: 0.9919 - val_loss: 0.6263 - val_accuracy: 0.8753\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 3s 181us/sample - loss: 6.2223e-04 - accuracy: 1.0000 - val_loss: 0.7686 - val_accuracy: 0.8860\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 3s 176us/sample - loss: 5.6923e-05 - accuracy: 1.0000 - val_loss: 1.1574 - val_accuracy: 0.8760\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 3s 185us/sample - loss: 0.0850 - accuracy: 0.9917 - val_loss: 0.8476 - val_accuracy: 0.8837\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 3s 177us/sample - loss: 1.9872e-05 - accuracy: 1.0000 - val_loss: 0.9842 - val_accuracy: 0.8840\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 3s 182us/sample - loss: 3.1329e-06 - accuracy: 1.0000 - val_loss: 1.1557 - val_accuracy: 0.8845\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 3s 180us/sample - loss: 0.0738 - accuracy: 0.9934 - val_loss: 0.9503 - val_accuracy: 0.8801\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 3s 180us/sample - loss: 9.0578e-06 - accuracy: 1.0000 - val_loss: 0.9956 - val_accuracy: 0.8794\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 3s 213us/sample - loss: 1.5204e-06 - accuracy: 1.0000 - val_loss: 1.0617 - val_accuracy: 0.8815\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 3s 196us/sample - loss: 4.1353e-07 - accuracy: 1.0000 - val_loss: 1.2041 - val_accuracy: 0.8827\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 3s 205us/sample - loss: 1.0206e-07 - accuracy: 1.0000 - val_loss: 1.2970 - val_accuracy: 0.8834\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 3s 189us/sample - loss: 2.4361e-08 - accuracy: 1.0000 - val_loss: 1.4011 - val_accuracy: 0.8838\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 3s 185us/sample - loss: 1.1252e-08 - accuracy: 1.0000 - val_loss: 1.4424 - val_accuracy: 0.8834\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 3s 180us/sample - loss: 7.6279e-09 - accuracy: 1.0000 - val_loss: 1.4662 - val_accuracy: 0.8839\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 3s 185us/sample - loss: 5.8335e-09 - accuracy: 1.0000 - val_loss: 1.4837 - val_accuracy: 0.8833\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_big = model.fit(x_train, y_train, epochs=20, batch_size=256, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fae90442950>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, history_old.history['val_loss'], 'bx', label='Original model')\n",
    "plt.plot(x, history_big.history['val_loss'], 'bo', label='Bigger model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The bigger network starts overfitting almost right away, after just one epoch, and\n",
    "overfits much more severely. Its validation loss is also more noisy.\n",
    "\n",
    "更大的网络几乎一开始就过拟合了，仅仅过了一次迭代而已，并且过拟合的程度更加严重。它的验证损失也比较杂乱无章。\n",
    "\n",
    "> Meanwhile, here are the training losses for our two networks:\n",
    "\n",
    "与此同时，我们来看一下两个网络的训练损失情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fae905b1990>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, history_old.history['loss'], 'b+', label='Original model')\n",
    "plt.plot(x, history_big.history['loss'], 'bo', label='Bigger model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Train loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As you can see, the bigger network gets its training loss near zero very quickly. The\n",
    "more capacity the network has, the quicker it will be able to model the training data\n",
    "(resulting in a low training loss), but the more susceptible it is to overfitting (resulting in\n",
    "a large difference between the training and validation loss).\n",
    "\n",
    "你可以看到，更大的网络很快能使得训练损失接近零值。网络的容量越大，它就能越快的拟合训练数据，但是更容易产生过拟合（导致训练损失和验证损失之间的巨大误差）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 增加权重正则化\n",
    "\n",
    "> You may be familiar with Occam’s Razor principle: given two explanations for\n",
    "something, the explanation most likely to be correct is the \"simplest\" one, the one that\n",
    "makes the least amount of assumptions. This also applies to the models learned by neural\n",
    "networks: given some training data and a network architecture, there are multiple sets of\n",
    "weights values (multiple models ) that could explain the data, and simpler models are less\n",
    "likely to overfit than complex ones.\n",
    "\n",
    "你可能熟悉奥卡姆剃刀原则：对于某个事物有两种解释的话，最简单的那种解释最有可能是正确的，因为它是使用了最少假设的那个解释。这个原则也可以应用到神经网络的模型训练过程上：给定训练数据和一个网络结构，可以有多种可能的权重数量方案（不同的模型）能够解释数据，更简单的模型更加不容易导致过拟合。\n",
    "\n",
    "> A \"simple model\" in this context is a model where the where the distribution of\n",
    "parameter values has less entropy (or a model with fewer parameters altogether, as we\n",
    "saw in the section above). Thus a common way to mitigate overfitting is to put\n",
    "constraints on the complexity of a network by forcing its weights to only take small\n",
    "values, which makes the distribution of weight values more \"regular\". This is called\n",
    "\"weight regularization\", and it is done by adding to the loss function of the network a\n",
    "cost associated with having large weights. This cost comes in two flavors:\n",
    "\n",
    "> - L1 regularization, where the cost added is proportional to the absolute value of the\n",
    "weights coefficients (i.e. to what is called the \"L1 norm\" of the weights).\n",
    "- L2 regularization, where the cost added is proportional to the square of the value of the\n",
    "weights coefficients (i.e. to what is called the \"L2 norm\" of the weights). L2\n",
    "regularization is also called weight decay in the context of neural networks. Don’t let the\n",
    "different name confuse you: weight decay is mathematically the exact same as L2\n",
    "regularization.\n",
    "\n",
    "这里一个“更简单的模型”指的是一个参数的分布具有更少的熵的模型（或者说一个模型有着更少的参数，正如前面小节介绍的那样）。因此有一个通用的方法来抑制过拟合，通过限制模型的参数只能使用较小的数值来减低模型的复杂度，这会使得模型权重值的分布更加的“正规”。我们将这种方法称为“权重正则化”，正则化方法通过在模型的损失函数中增加一个与其大权重参数相关的惩罚来实现。这个惩罚可以有两种：\n",
    "\n",
    "- L1正则化，增加的惩罚与权重系数的绝对值成比例（也叫做权重的“L1范数”）。\n",
    "- L2正则化，增加的惩罚与权重系统的平方成比例（也叫作权重的“L2范数”）。L2正则化在神经网络的上下文中也经常被叫做权重衰减。不要让这个名称迷惑了：权重衰减在数学上完全等同于L2正则化。\n",
    "\n",
    "> In Keras, weight regularization is added by passing weight regularizer instances to\n",
    "layers as keyword arguments. Let’s add L2 weight regularization to our movie review\n",
    "classification network:\n",
    "\n",
    "在Keras当中，权重正则化是通过在层当中增加一个关键字参数的权重正则实例实现的。让我们在我们影评分类网络中增加权重正则化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.5122 - accuracy: 0.8135 - val_loss: 0.3826 - val_accuracy: 0.8758\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 69us/sample - loss: 0.3145 - accuracy: 0.9057 - val_loss: 0.3490 - val_accuracy: 0.8739\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 46us/sample - loss: 0.2520 - accuracy: 0.9275 - val_loss: 0.3165 - val_accuracy: 0.8880\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 2s 127us/sample - loss: 0.2228 - accuracy: 0.9401 - val_loss: 0.3353 - val_accuracy: 0.8815\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 82us/sample - loss: 0.2043 - accuracy: 0.9457 - val_loss: 0.3480 - val_accuracy: 0.8777\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 46us/sample - loss: 0.1901 - accuracy: 0.9516 - val_loss: 0.3709 - val_accuracy: 0.8771\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 2s 100us/sample - loss: 0.1814 - accuracy: 0.9548 - val_loss: 0.3585 - val_accuracy: 0.8818\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 47us/sample - loss: 0.1758 - accuracy: 0.9586 - val_loss: 0.3695 - val_accuracy: 0.8797\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 81us/sample - loss: 0.1661 - accuracy: 0.9623 - val_loss: 0.3952 - val_accuracy: 0.8700\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 2s 117us/sample - loss: 0.1598 - accuracy: 0.9631 - val_loss: 0.3932 - val_accuracy: 0.8751\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 48us/sample - loss: 0.1539 - accuracy: 0.9665 - val_loss: 0.4210 - val_accuracy: 0.8689\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 80us/sample - loss: 0.1486 - accuracy: 0.9682 - val_loss: 0.4275 - val_accuracy: 0.8734\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.1462 - accuracy: 0.9689 - val_loss: 0.4304 - val_accuracy: 0.8721\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 2s 102us/sample - loss: 0.1399 - accuracy: 0.9716 - val_loss: 0.4383 - val_accuracy: 0.8716\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 79us/sample - loss: 0.1377 - accuracy: 0.9722 - val_loss: 0.4461 - val_accuracy: 0.8697\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 93us/sample - loss: 0.1318 - accuracy: 0.9740 - val_loss: 0.4642 - val_accuracy: 0.8649\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.1332 - accuracy: 0.9733 - val_loss: 0.4892 - val_accuracy: 0.8607\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.1266 - accuracy: 0.9764 - val_loss: 0.4804 - val_accuracy: 0.8697\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 68us/sample - loss: 0.1267 - accuracy: 0.9754 - val_loss: 0.4825 - val_accuracy: 0.8698\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 67us/sample - loss: 0.1251 - accuracy: 0.9771 - val_loss: 0.4911 - val_accuracy: 0.8655\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,),\n",
    "                       kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_l2_reg = model.fit(x_train, y_train, epochs=20, batch_size=256, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> l2(0.001) means that every coefficient in the weight matrix of the layer will add\n",
    "0.001 * weight_coefficient_value to the total loss of the network. Note that\n",
    "because this penalty is only added at training time , the loss for this network will be much\n",
    "higher at training than at test time.\n",
    "\n",
    "`l2(0.001)`表示权重矩阵中的每个系数都会将网络的总损失增加`0.001 * 权重系数值`的惩罚。注意这个惩罚只会在训练过程中增加，因此这个模型的损失在训练中会大大高于在测试情况中。\n",
    "\n",
    "> Here’s the impact of our L2 regularization penalty:\n",
    "\n",
    "下面展示了L2正则化惩罚的影响："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fae900e5f10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deZxU9Znv8c8DYjooCApxiAiNCYx0s9MshmhUlKAmuG9hjK1RooYb4hiXvHCkMSHLDEMSHY0XR4JGxAU37lUnDhrggprQjQiCGyiYVqKIEuS2C8szf5zTTdF0VVd31alT1fV9v1716qqz1VNFcZ5zfr/ze465OyIiUrzaxR2AiIjES4lARKTIKRGIiBQ5JQIRkSKnRCAiUuQOiDuAlurWrZuXlpbGHYaISEGpqan5wN27NzWv4BJBaWkp1dXVcYchIlJQzGxTsnlqGhIRKXJKBCIiRU6JQESkyBVcH0FTdu7cSW1tLZ9++mncoUiBKCkpoWfPnnTo0CHuUERi1yYSQW1tLZ06daK0tBQzizscyXPuztatW6mtraVPnz5xhyMSuzbRNPTpp59y2GGHKQlIWsyMww47TGeQUnCqqqLZbptIBICSgLSIfi9SiKZPj2a7bSYRiIhI6ygRZEltbS2nn346ffv25Stf+QpTpkzh888/b3LZd999l3POOafZbZ566qls27atVfFUVVUxc+bMVq2brrlz5zJ58uSMlxGR5KqqwCx4wN7n2WwmKupEkK0v0t0566yzOOOMM3jjjTd4/fXX2bFjB1OnTt1v2V27dvHlL3+ZBQsWNLvdJ598ki5dumQnSBEpSFVV4B48YO9zJYIsyVZ727PPPktJSQmXXHIJAO3bt+fXv/41c+bMoa6ujrlz5zJhwgROPPFExo4dy8aNGxkwYAAAdXV1nHfeeZSVlXHmmWcyatSohhIapaWlfPDBB2zcuJH+/ftz+eWXU15ezrhx4/jkk08AuPPOOxkxYgSDBw/m7LPPpq6uLmWslZWVXHnllYwePZqjjjqKxYsXc+mll9K/f38qKysblps/fz4DBw5kwIABXH/99Q3Tf//739OvXz9GjhzJ8uXLG6Zv2bKFs88+mxEjRjBixIh95olIfivqRJAta9euZfjw4ftM69y5M7169WL9+vUArFy5kgULFrBkyZJ9lrv99tvp2rUr69at46c//Sk1NTVNvscbb7zBD37wA9auXUuXLl14+OGHATjrrLNYsWIFL730Ev379+euu+5qNt6PPvqI559/nl//+tdMmDCBq6++mrVr17JmzRpWrVrFu+++y/XXX8+zzz7LqlWrWLFiBY899hibN29m2rRpLF++nGXLlrFu3bqGbU6ZMoWrr76aFStW8PDDD3PZZZe16DsUkeZNmxbNdtvEOIKWqKra90ygvt1t2rToLs0COPnkkzn00EP3m75s2TKmTJkCwIABAxg0aFCT6/fp04chQ4YAMHz4cDZu3AjAyy+/zI033si2bdvYsWMH3/zmN5uN5dvf/jZmxsCBAzn88MMZOHAgAOXl5WzcuJFNmzZx/PHH0717UKhw4sSJLF26FGCf6eeffz6vv/46AIsWLdonMWzfvp0dO3Y0G4uIpC+qfVRRJoL6L9Nsb7tbJsrKyvZr89++fTtvv/02X/3qV1m5ciUHHXRQRu/xhS98oeF5+/btG5qGKisreeyxxxg8eDBz585l8eLFaW+rXbt2+2y3Xbt27Nq1q1Wjbffs2cMLL7xASUlJi9cVkXipaSgLxo4dS11dHffccw8Au3fv5pprrqGyspKOHTumXHfMmDE8+OCDAKxbt441a9a06L0//vhjevTowc6dO5k3b17rPkAjI0eOZMmSJXzwwQfs3r2b+fPn841vfINRo0axZMkStm7dys6dO3nooYca1hk3bhy33nprw+tVq1ZlJRYRiV5kicDM5pjZ+2b2cpL5E81stZmtMbPnzGxwVLEkk632NjPj0Ucf5aGHHqJv377069ePkpISfv7znze77lVXXcWWLVsoKyvjxhtvpLy8nEMOOSTt9/7pT3/KqFGjGDNmDEcffXQmH6NBjx49+OUvf8kJJ5zA4MGDGT58OKeffjo9evSgqqqKY445hjFjxtC/f/+GdW655Raqq6sZNGgQZWVl3HHHHVmJRUSiZ56NtpGmNmx2HLADuMfdBzQx/2vAK+7+kZmdAlS5+6jmtltRUeGNb0zzyiuv7LNTKiS7d+9m586dlJSUsGHDBk466SRee+01DjzwwLhDa/MK+Xcj0lJmVuPuFU3Ni6yPwN2XmllpivnPJbx8AegZVSz5rK6ujhNOOIGdO3fi7tx+++1KAiKSU/nSWfw94KlkM81sEjAJoFevXrmKKSc6deqkW2+KSKxi7yw2sxMIEsH1yZZx99nuXuHuFfWXLoqISHbEekZgZoOA/wROcfetccYiIlKsYjsjMLNewCPARe7+elxxiIgUu8jOCMxsPnA80M3MaoFpQAcAd78DuAk4DLg9rA2/K1mPtoiIRCeyMwJ3v9Dde7h7B3fv6e53ufsdYRLA3S9z967uPiR8FHQSOPjgg/ebNmvWLMrKyhg0aBBjx45l06ZNOY/r+OOPb3Fn9E033cSiRYsyfu+mvpNsqy/Ml+kyIsUs9s7iOMybB6Wl0K5d8DdLA3L3M3ToUKqrq1m9ejXnnHMO1113XbPr7Nq1K5pg0rR7925uvvlmTjrppFjjEJHcKbpEMG8eTJoEmzYFdYY2bQpeR5EMTjjhhIYSE6NHj6a2trbJ5SorK7niiisYNWoU1113HRs2bGD8+PEMHz6cY489lldffRWADRs2MHr0aAYOHMiNN97YcMS9ePFivvWtbzVsb/LkycydO3e/97nyyiupqKigvLycaQnDqktLS7n++usZNmwYDz30EJWVlSxYsIDq6mqGDBnCkCFDGDhwYMPtHZPF99Zbb3HMMcc0xNeUjRs3cvTRR1NZWUm/fv2YOHEiixYtYsyYMfTt25e//OUvAHz44YecccYZDBo0iNGjR7N69WoAtm7dyrhx4ygvL+eyyy4jcUDkvffey8iRIxkyZAjf//732b17d/P/SCJSfIlg6lRoXLK/ri6YHqW77rqLU045Jen82tpannvuOWbNmsWkSZO49dZbqampYebMmVx11VVAUOp5ypQprFmzhp49Wz7+bsaMGQ1nKEuWLGnYuQIcdthhrFy5kgsuuKBhWkVFBatWrWLVqlWMHz+eH//4xwAp47vyyitZs2YNPXr0SBrH+vXrueaaa3j11Vd59dVXue+++1i2bBkzZ85sKMsxbdo0hg4dyurVq/n5z3/Od7/7XQCmT5/O17/+ddauXcuZZ57J22+/DQSjhB944AGWL1/OqlWraN++fdZqL4m0dfkyoCxnwv1G2tOz4d5776W6unq/exEkOvfcc2nfvj07duzgueee49xzz22Y99lnnwHw/PPP89hjjwHwne98p2HHnK4HH3yQ2bNns2vXLjZv3sy6desayl6ff/75Sdd74IEHWLlyJU8//XTK+JYvX95wn4SLLrponxvaJOrTp88+pa/Hjh3bUBa7vrz2smXLGrZ14oknsnXrVrZv387SpUt55JFHADjttNPo2rUrAM888ww1NTWMGDECgE8++YQvfelLLfp+RKKWWP04nxRdIujVK2gOamp6FBYtWsSMGTNYsmRJQ8nnqVOn8sQTTwB7q3TWl6nes2cPXbp0aVH1zgMOOIA9e/Y0vP7000/3W+att95i5syZrFixgq5du1JZWbnPcsnKZL/88stUVVWxdOlS2rdv32x89c1HqTQufZ1YFru1fSTuzsUXX8wvfvGLVq0vkgvTp+dnIii6pqEZM6BxZeiOHYPp2fbiiy/y/e9/n4ULF+5zdDpjxoyGJpfGOnfuTJ8+fRpKPLs7L730EhD0M9QfJd9///0N6/Tu3Zt169bx2WefsW3bNp555pn9trt9+3YOOuggDjnkEN577z2eeippRY8G27Zt48ILL+See+5puBlNqvjGjBnTEFemzTLHHntswzYWL15Mt27d6Ny5M8cddxz33XcfAE899RQfffQREJQCX7BgAe+//z4Q9DHEcZWWSCEqukQwcSLMng29ewc3pundO3g9cWJm262rq6Nnz54Nj1mzZnHttdeyY8cOzj33XIYMGcKECRPS2ta8efO46667GDx4MOXl5Tz++OMA/OY3v2HWrFkMGjSI9evXN5SrPvLIIznvvPMYMGAA5513HkOHDt1vm4MHD2bo0KEcffTRfOc732HMmDHNxvH444+zadMmLr/88oZO41Tx/fa3v+W2225j4MCBvPPOO2l91mSqqqqoqalh0KBB3HDDDdx9991A0HewdOlSysvLeeSRRxpqT5WVlfGzn/2McePGMWjQIE4++WQ2b96cUQwi2VBVFexr6k+W65/n05lBZGWoo9LWylC3RF1dHV/84hcxM+6//37mz5/fsBOWliuW343kj2zdFbF17x1DGWrJvpqaGiZPnoy706VLF+bMmRN3SCLSBigRFJBjjz22oT1eRApPtu6KmG1tpo+g0Jq4JF76vUgc8qlfIFGbSAQlJSVs3bpV/7klLe7O1q1bKSkpiTsUkbzQJpqGevbsSW1tLVu2bIk7FCkQJSUlrRqdLdIWtYlE0KFDB/r06RN3GCIiBalNNA2JiEjrKRGIiKQpXzt7M6VEICKSpunT444gGkoEIiJFTolARCSFQqgVlKk2UWtIRCQX4qwVlKlUtYZ0RiAiUuSUCERE0pSvtYIypUQgIpKmttQvkEiJQESkyCkRiIgUOSUCEZEip0QgIkWjrbbxZ0qJQESKRlstEZEpJQIRkSKnRCAibVoxlIjIlEpMiEjRKOQSEZlSiQkREUkqskRgZnPM7H0zeznJfDOzW8xsvZmtNrNhUcUiIgJtt0REpqI8I5gLjE8x/xSgb/iYBPwuwlhERNQvkERkicDdlwIfpljkdOAeD7wAdDGzHlHFIyIiTYuzj+AI4K8Jr2vDafsxs0lmVm1m1Vu2bMlJcCIixaIgOovdfba7V7h7Rffu3eMOR0SkTYkzEbwDHJnwumc4TUSkSWrjj0aciWAh8N3w6qHRwN/dfXOM8YhInlOJiGgcENWGzWw+cDzQzcxqgWlABwB3vwN4EjgVWA/UAZdEFYuIiCQXWSJw9wubme/AD6J6fxFpG6qq9j0TqC8VMW2amoqyRSUmRKRgFHOJiEypxISIiCSlRCAiBUMlIqKhRCAiBUN9AtFQIhARKXJKBCIiRU6JQERyRk07+UmJQERyRiOD81OzicDMxpjZQeHzfzKzWWbWO/rQREQkF9I5I/gdUGdmg4FrgA3APZFGJSJthm4en//SSQS7wnIQpwP/4e63AZ2iDUtE2oqqqmA0cP2I4PrnSgT5I51aQx+b2U+AfwKOM7N2hMXjRESk8KVzRnA+8BnwPXf/G8F9A/4t0qhEpE3SyOD8lNYZAfBbd99tZv2Ao4H50YYlIm2RmoPyUzpnBEuBL5jZEcDTwEXA3CiDEhGR3EknEZi71wFnAbe7+7nAgGjDEhGRXEkrEZjZMcBE4IkWrCciIgUgnR36j4CfAI+6+1ozOwr4U7RhiYhIrjTbWezuS4AlZnawmR3s7m8CP4w+NBERyYV0SkwMNLMXgbXAOjOrMbPy6EMTEZFcSKdp6H8D/+zuvd29F0GZiTujDUtERHIlnURwkLs39Am4+2LgoMgiEhGRnEpnQNmbZvYvwB/C1/8EvBldSCIikkvpnBFcCnQHHgkf3cNpIiLSBqRz1dBH6CohEZE2K2kiMLP/A3iy+e4+IZKIREQkp1KdEczMWRQiIhKbpIkgHEgmIiJtnGoGiYgUOSUCEZEip0QgImnTjWXapnRqDfUzszvN7Gkze7b+kc7GzWy8mb1mZuvN7IYm5vcysz+Z2YtmttrMTm3NhxCR3Jg+Pe4IJArpjCx+CLiDoL7Q7nQ3bGbtgduAk4FaYIWZLXT3dQmL3Qg86O6/M7My4EmgNN33EBGRzKXTNLTL3X/n7n9x95r6RxrrjQTWu/ub7v45cD9weqNlHOgcPj8EeDftyEUkJ6qqwCx4wN7naiZqO8w96ZixYAGzKuB94FHgs/rp7v5hM+udA4x398vC1xcBo9x9csIyPQjug9yVoJDdSU0lGTObBEwC6NWr1/BNmzal89lEJMvMoJldhuQpM6tx94qm5qVzRnAxcC3wHFATPqqzFNuFwFx37wmcCvzBzPaLyd1nu3uFu1d07949S28tIiKQXq2hPq3c9jvAkQmve4bTEn0PGB++z/NmVgJ0IzgDEZE8M21a3BFIFNK5aqiDmf3QzBaEj8lm1iGNba8A+ppZHzM7ELgAWNhombeBseH79AdKgC0t+wgikivqF2ib0rlq6HdAB+D28PVF4bTLUq3k7rvMbDLwR6A9MMfd15rZzUC1uy8kvNuZmV1N0HFc6c11WoiISFalkwhGuPvghNfPmtlL6Wzc3Z8kuCQ0cdpNCc/XAWPS2ZaIiEQjnc7i3Wb2lfoXZnYULRhPICIi+S2dM4JrgT+Z2ZuAAb2BSyKNSkREciadq4aeMbO+wD+Gk15z989SrSMiIoUjadOQmZ0Y/j0LOA34avg4LZwmIgVGV/1IU1L1EXwj/PvtJh7fijguEYmAisZJU1Ldoax+6MjN7v5W4jwza+0gMxERyTPpXDX0cBPTFmQ7EBGJhorGSXOSnhGY2dFAOXBIoz6BzgQjgEWkAFRV7d3pq2icNCXVVUP/SNAX0IWgX6Dex8DlUQYlIiK5k6qP4HHgcTM7xt2fz2FMIhIRFY2TpqQzoOxFM/sBQTNRQ5OQu18aWVQiEgn1C0hT0uks/gPwD8A3gSUE5aQ/jjIoERHJnXQSwVfd/V+A/+/udxMMLhsVbVgiIpIr6SSCneHfbWY2gODewl+KLiQREcmldPoIZptZV+BfCG4sczBwU+pVRESkUKRTdO4/w6dLgKOiDUdERHIt1YCyf061orvPyn44IiKSa6n6CDqFjwrgSuCI8HEFMCz60ESkMV3+KVGw5m4RbGZLgdPc/ePwdSfgCXc/Lgfx7aeiosKrq6vjeGuR2KlEhLSWmdW4e0VT89K5auhw4POE15+H00REpA1IJxHcA/zFzKrMrAr4MzA3yqBEZC9VD5WoNds0BGBmw4Bjw5dL3f3FSKNKQU1DUszUNCStlappKNVVQ53dfbuZHQpsDB/18w519w+zHaiIiOReqnEE9xGUoa4BEo9BLHytMQUiOabqoRKFVGWovxX+1W0pRfKE+gUkCqmahlKOFXD3ldkPR0REci1V09C/p5jnwIlZjkVERGKQqmnohFwGIiIi8Uin+ihh+eky9r1D2T1RBSUiIrnTbCIws2nA8QSJ4EngFGAZwUAzEREpcOmMLD4HGAv8zd0vAQYT3JxGRFpIV/1IPkonEXzi7nuAXWbWGXgfODLasETapunT445AZH/pJIJqM+sC3EkwuGwl8Hw6Gzez8Wb2mpmtN7MbkixznpmtM7O1ZnZf2pGLiEhWJE0EZnabmY1x96vcfZu73wGcDFwcNhGlZGbtgdsI+hTKgAvNrKzRMn2BnwBj3L0c+FEGn0UkL6lonOS7VJ3FrwMzzawH8CAwv4XF5kYC6939TQAzux84HViXsMzlwG3u/hGAu7/fkuBFCkFV1d6dvorGST5Kekbg7r9192OAbwBbgTlm9qqZTTOzfmls+wjgrwmva8NpifoB/cxsuZm9YGbjm9qQmU0ys2ozq96yZUsaby0iIulqto/A3Te5+6/cfShwIXAG8EqW3v8AoC/B5akXAneG/RGNY5jt7hXuXtG9e/csvbVI7qlonOSjZhOBmR1gZt82s3nAU8BrwFlpbPsd9r26qGc4LVEtsNDdd7r7WwTNUX3TilykAKlfQPJRqs7ik81sDsHO+nLgCeAr7n6Buz+exrZXAH3NrI+ZHQhcACxstMxjBGcDmFk3gqaiN1v8KUREpNVSdRb/hOCeBNfUd+a2hLvvMrPJwB+B9sAcd19rZjcD1e6+MJw3zszWAbuBa919a4s/hYiItFpat6rMJ7pVpcQp8QogkUKS6laV6QwoE5GQRgZLW6REICJS5JQIRJqhkcHS1qmPQKQFNDJYCpX6CEREJCklApEW0MhgaYuUCERaQP0C0hYpEYiIFDklAhGRIqdEICJS5JQIRESKnBKBiEiRUyKQoqKrfkT2p0QgRUVF46QQzZsHpaXQrl3wd9687G5fiUBE2ryod6RRmjcPJk2CTZuC8iabNgWvs/kZlAikzVPRuOKWjR1ppokkk/WnToW6un2n1dUF07NFReekqKhoXPEpLQ12/o317g0bNza/fn0iSdwZd+wIs2fDxInRr9+uXdO/WTPYs6f59fcun7zonBKBFBUlguKT6Y4000QS9/r1VH1UJKSiccWnV6+WTW/s7bdbNj3b68+YEZxBJOrYMZieLUoEUlTUL1CYMmljz3RHmmkiyXT9iRODZqTevYOzmN69029WSpcSgYjktUw7ezPdkWaaSLJxRD9xYtAMtGdP8DebSQDURyAieS5bbeSZmDcvuErn7beDI/kZM1q2M850/WxQH4G0GWraKUyZNO1k2saeDZkekUd9RJ8pJQIpKBoZXHgybdrJtI1dmqdEICLNinNAVC6umil2SgSS9zQyOF6ZHtFn2rSTi6tmip0SgeS9qqpgB1R/XUP982JKBIVc4iAbTTv53sZe6JQIRPJcpkfkcR/Rq2kn/ykRSEEp1JHBcR6Rx31Er6ad/KdxBCIRi7voWKbrZxq/5AeNI5C8UUzt+vXiPiLXEb00R4lAcqoYxwHE3cZeCCUOJF6RJgIzG29mr5nZejO7IcVyZ5uZm1mTpy2SP4rxiD5TcR+R64hemhNZIjCz9sBtwClAGXChmZU1sVwnYArw56hikexpzRF9PowDiPPyy3w4ItcRvaTk7pE8gGOAPya8/gnwkyaW+w1wGrAYqGhuu8OHD3eJD8S7fmvce697x471ow+CR8eOwfRcrF+/jd693c2Cvy1ZVyQbgGpPsl+NsmnoCOCvCa9rw2kNzGwYcKS7P5FqQ2Y2ycyqzax6y5YtLQ6kkG9cnQ/y4Yg+E3Fffgk6Ipf8FltnsZm1A2YB1zS3rLvPdvcKd6/o3r17i94nGzeuLnbZHNkbxziAuO8wJZLvokwE7wBHJrzuGU6r1wkYACw2s43AaGBhtjuMs3E0J9kTx1lE3JdfiuS7KBPBCqCvmfUxswOBC4CF9TPd/e/u3s3dS929FHgBmODuWR0tpqO57KhvXoN4mtfi7KxViQRp85J1HmTjAZwKvA5sAKaG024m2OE3XnYxEXQW9+69bydf/aN37xZtpqhlo7M07vfPtLNWnb1S6EjRWdzmS0xoeHzmsnGrwExu1ZcPtyoUKXRFXWJCg2kyl2nzWtzVL0UktTafCCDzS/cK/fLTTOPPtLM07lo7IpJaUSSCTBT65afZiD/TztK4a+2ISGpKBM0o9MtPszUYKpPmtbhr7YhIakWVCFpzDXs+tE9n0rSTrfgzaV7Lh1o7IpJcUSWC1hRMi7t9OtOmnbjjBx3Ri+S7okoErRF3+3SmTTtxx19PR/Qi+avNJ4JMC6bFfTSbadNOYvygo3ER2V+bH1CWyKzpe7dGLV8GU8X1+UUkfkU9oCxumbbxZ9q0U+glpEUkekV1RlBVlfsdYNzlGRLpjECkeKU6IyiqRBCHdu2a3vmaBR2nLZFpIlMiECleahqKUTYv32zN5a+J4rgpjIjkPyWCiOXL5ZugfgERaZoSQcQyvfxUnb0iEjX1EeSQ2vhFJC7qI8iSTI/CM23jFxGJghJBC8S9I1dnr4hEQYkgYtls41e/gIhEQYmgGZnuyKuq9t5yHfY+105dRPKFOotbINPOWnX2ikhc1FmcJ9TGLyL5SImgBTLdkas5SETykRJBC2hHLiJtkRKBiEiRUyIQESlySgQiIkVOiUBEpMgpEYiIFLmCG1BmZluAJm7+mBe6AR/EHUQK+R4f5H+Mii8zii8zmcTX2927NzWj4BJBPjOz6mQj9/JBvscH+R+j4suM4stMVPGpaUhEpMgpEYiIFDklguyaHXcAzcj3+CD/Y1R8mVF8mYkkPvURiIgUOZ0RiIgUOSUCEZEip0TQQmZ2pJn9yczWmdlaM5vSxDLHm9nfzWxV+LgpxzFuNLM14XvvdxcfC9xiZuvNbLWZDcthbP+Y8L2sMrPtZvajRsvk/Pszszlm9r6ZvZww7VAz+28zeyP82zXJuheHy7xhZhfnML5/M7NXw3/DR82sS5J1U/4eIoyvyszeSfh3PDXJuuPN7LXw93hDDuN7ICG2jWa2Ksm6kX5/yfYpOf39ubseLXgAPYBh4fNOwOtAWaNljgf+b4wxbgS6pZh/KvAUYMBo4M8xxdke+BvBQJdYvz/gOGAY8HLCtH8Fbgif3wD8qon1DgXeDP92DZ93zVF844ADwue/aiq+dH4PEcZXBfw4jd/ABuAo4EDgpcb/n6KKr9H8fwduiuP7S7ZPyeXvT2cELeTum919Zfj8Y+AV4Ih4o2qx04F7PPAC0MXMesQQx1hgg7vHPlLc3ZcCHzaafDpwd/j8buCMJlb9JvDf7v6hu38E/DcwPhfxufvT7r4rfPkC0DPb75uuJN9fOkYC6939TXf/HLif4HvPqlTxmZkB5wHzs/2+6UixT8nZ70+JIANmVgoMBf7cxOxjzOwlM3vKzMpzGhg48LSZ1ZjZpCbmHwH8NeF1LfEkswtI/p8vzu+v3uHuvjl8/jfg8CaWyZfv8lKCs7ymNPd7iNLksOlqTpKmjXz4/o4F3nP3N5LMz9n312ifkrPfnxJBK5nZwcDDwI/cfXuj2SsJmjsGA7cCj+U4vK+7+zDgFOAHZnZcjt+/WWZ2IDABeKiJ2XF/f/vx4Dw8L6+1NrOpwC5gXpJF4vo9/A74CjAE2EzQ/JKPLiT12UBOvr9U+5Sof39KBK1gZh0I/sHmufsjjee7+3Z33xE+fxLoYGbdchWfu78T/n0feJTg9DvRO8CRCa97htNy6RRgpbu/13hG3N9fgvfqm8zCv+83sUys32RkNygAAANSSURBVKWZVQLfAiaGO4v9pPF7iIS7v+fuu919D3BnkveN+/s7ADgLeCDZMrn4/pLsU3L2+1MiaKGwPfEu4BV3n5VkmX8Il8PMRhJ8z1tzFN9BZtap/jlBh+LLjRZbCHw3vHpoNPD3hFPQXEl6FBbn99fIQqD+KoyLgcebWOaPwDgz6xo2fYwLp0XOzMYD1wET3L0uyTLp/B6iii+x3+nMJO+7AuhrZn3Cs8QLCL73XDkJeNXda5uamYvvL8U+JXe/v6h6wtvqA/g6wSnaamBV+DgVuAK4IlxmMrCW4AqIF4Cv5TC+o8L3fSmMYWo4PTE+A24juFpjDVCR4+/wIIId+yEJ02L9/giS0mZgJ0E76/eAw4BngDeARcCh4bIVwH8mrHspsD58XJLD+NYTtA/X/w7vCJf9MvBkqt9DjuL7Q/j7Wk2wU+vROL7w9akEV8psyGV84fS59b+7hGVz+v2l2Kfk7PenEhMiIkVOTUMiIkVOiUBEpMgpEYiIFDklAhGRIqdEICJS5JQIREJmttv2rYyatUqYZlaaWPlSJJ8cEHcAInnkE3cfEncQIrmmMwKRZoT16P81rEn/FzP7aji91MyeDYuqPWNmvcLph1twf4CXwsfXwk21N7M7w5rzT5vZF8PlfxjWol9tZvfH9DGliCkRiOz1xUZNQ+cnzPu7uw8E/gP4TTjtVuBudx9EUPDtlnD6LcASD4rmDSMYkQrQF7jN3cuBbcDZ4fQbgKHhdq6I6sOJJKORxSIhM9vh7gc3MX0jcKK7vxkWB/ubux9mZh8QlE3YGU7f7O7dzGwL0NPdP0vYRilB3fi+4evrgQ7u/jMz+y9gB0GV1cc8LLgnkis6IxBJjyd53hKfJTzfzd4+utMIaj8NA1aEFTFFckaJQCQ95yf8fT58/hxBtUyAicD/C58/A1wJYGbtzeyQZBs1s3bAke7+J+B64BBgv7MSkSjpyENkry/avjcw/y93r7+EtKuZrSY4qr8wnPa/gN+b2bXAFuCScPoUYLaZfY/gyP9KgsqXTWkP3BsmCwNucfdtWftEImlQH4FIM8I+ggp3/yDuWESioKYhEZEipzMCEZEipzMCEZEip0QgIlLklAhERIqcEoGISJFTIhARKXL/A6gu4nR1iCDqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, history_old.history['val_loss'], 'b+', label='Original model')\n",
    "plt.plot(x, history_l2_reg.history['val_loss'], 'bo', label='L2-regularized model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As you can see, the model with L2 regularization (dots) has become much more\n",
    "resistant to overfitting than the reference model (crosses), even though both models have\n",
    "the same number of parameters.\n",
    "\n",
    "如上图所示，使用了L2正则化（圆点）的模型比参考的原始模型（×号）对于过拟合有着更强的抑制作用，尽管两个模型有着相同数量的参数。\n",
    "\n",
    "> As alternatives to L2 regularization, you could use one of the following Keras weight\n",
    "regularizers:\n",
    "\n",
    "除了L2正则化外，Keras还提供了下面的权重正则化方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.regularizers.L1L2 at 0x7fae900b84d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L1正则化\n",
    "regularizers.l1(0.001)\n",
    "\n",
    "# 同时进行L1和L2正则化\n",
    "regularizers.l1_l2(l1=0.001, l2=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 增加Dropout\n",
    "\n",
    "> Dropout is one of the most effective and most commonly used regularization techniques\n",
    "for neural networks, developed by Hinton and his students at the University of Toronto.\n",
    "Dropout, applied to a layer, consists of randomly \"dropping out\" (i.e. setting to zero) a\n",
    "number of output features of the layer during training. Let’s say a given layer would\n",
    "normally have returned a vector [0.2, 0.5, 1.3, 0.8, 1.1] for a given input sample\n",
    "during training; after applying dropout, this vector will have a few zero entries distributed\n",
    "at random, e.g. [0, 0.5, 1.3, 0, 1.1] . The \"dropout rate\" is the fraction of the\n",
    "features that are being zeroed-out; it is usually set between 0.2 and 0.5. At test time, no\n",
    "units are dropped out, and instead the layer’s output values are scaled down by a factor\n",
    "equal to the dropout rate, so as to balance for the fact that more units are active than at\n",
    "training time.\n",
    "\n",
    "Dropout是神经网络中最有效和最常用的正则化技巧，它是Hinton和他在多伦多大学的学生共同发明的。Dropout被应用到层上，包含着在训练中随机的“丢弃”（也就是设置为0）的层的输出特征值。比方说一个层通常在一个输入训练样本的情况下会返回一个向量$[0.2, 0.5, 1.3, 0.8, 1.1]$；使用了dropout之后，这个向量中就会存在着随机选择的零值，例如$[0, 0.5, 1.3, 0, 1.1]$。“丢弃比例”决定着多少占比的特征会被填充为零值；通常被设置为0.2到0.5之间。在测试阶段，没有单元会被丢弃，而是将该层的输出值缩小，缩小的比例等于丢弃的比例，使用这种方式来平衡测试时更多的隐藏单元参与了运算。\n",
    "\n",
    "> Consider a Numpy matrix containing the output of a layer, layer_output , of shape\n",
    "(batch_size, features) . At training time, we would be zero-ing out at random a\n",
    "fraction of the values in the matrix:\n",
    "\n",
    "考虑一下一个Numpy矩阵`layer_output`包含着层的输出，它有着形状(batch_size, features)。在训练阶段，我们会按照比例随机将该矩阵的值设置为零："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在训练时，会将50%的输出值设置为0，伪代码，无法运行\n",
    "import numpy as np\n",
    "layer_output *= np.random.randint(0, 2, layer_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> At test time, we would be scaling the output down by the dropout rate. Here we scale\n",
    "by 0.5 (because we were previous dropping half the units):\n",
    "\n",
    "在测试阶段，我们会将输出值减少相应的丢弃比率。这里是0.5（因为我们之前丢弃了一半的隐藏单元）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在测试时，会将输出值全部缩小50%，伪代码，无法运行\n",
    "layer_output *= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that this process can be implemented by doing both operations at training time\n",
    "and leaving the output unchanged at test time, which is often the way it is implemented in\n",
    "practice:\n",
    "\n",
    "当然我们也可以在训练阶段同时实现上面的两个操作，然后在测试阶段保持输出不变即可，这也是实践中通常使用的方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在训练时同时完成两个操作，则模型在测试的时候就可以保持不动。伪代码，无法运行\n",
    "layer_output *= np.random.randint(0, 2, layer_output.shape)\n",
    "layer_output /= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dropout](imgs/f4.8.jpg)\n",
    "\n",
    "图4-8 Dropout在训练阶段随机丢弃了部分单元值，并按比例扩大了参数值，在测试阶段保持参数值不变\n",
    "\n",
    "> This technique may seem strange and arbitrary. Why would this help reduce\n",
    "overfitting? Geoff Hinton has said that he was inspired, among other things, by a fraud\n",
    "prevention mechanism used by banks—in his own words: \"I went to my bank. The tellers\n",
    "kept changing and I asked one of them why. He said he didn’t know but they got moved\n",
    "around a lot. I figured it must be because it would require cooperation between\n",
    "employees to successfully defraud the bank. This made me realize that randomly\n",
    "removing a different subset of neurons on each example would prevent conspiracies and\n",
    "thus reduce overfitting\" .\n",
    "\n",
    "这个技巧看起来很奇怪和随意。为什么它能帮助减少过拟合？Geoff Hinton表示他是从银行防止舞弊诈骗的做法中获得的启示 - 用他的原话来说：“我去银行。发现出纳员不停的更替，于是我问了其中一个出纳员，他表示自己也不清楚原因，但是它们经常会进行更替。我发现这应该是与如果需要欺诈银行必须通过银行员工合作才能成功这个事实有关。这使得我想到如果随机的移除神经元的一小部分子集，能够预防神经元之间的合谋，从而减少过拟合”。\n",
    "\n",
    "> The core idea is that introducing noise in the output values of a layer can break up\n",
    "happenstance patterns that are not significant (what Hinton refers to as \"conspiracies\"),\n",
    "which the network would start memorizing if no noise was present.\n",
    "\n",
    "这里的核心思想是在层的输出值中引入噪音能够打破训练过程中偶然学习到的非重要的模式（也就是Hinton指的“合谋”），如果没有这些噪音，网络将会开始记忆这些模式。\n",
    "\n",
    "> In Keras you can introduce dropout in a network via the Dropout layer, which gets\n",
    "applied to the output of layer right before it, e.g.:\n",
    "\n",
    "在Keras中你可以通过Dropout层来将dropout技巧带进网络，它将应用到前面一层的输出值之上，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 伪代码，不能运行\n",
    "model.add(layers.Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let’s add two Dropout layers in our IMDB network to see how well they do at\n",
    "reducing overfitting:\n",
    "\n",
    "让我们在IMDB网络中加入两个dropout层来观察它们如何减少过拟合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 19s 1ms/sample - loss: 0.6302 - accuracy: 0.6693 - val_loss: 0.5478 - val_accuracy: 0.8044\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 59us/sample - loss: 0.5212 - accuracy: 0.8285 - val_loss: 0.4773 - val_accuracy: 0.8592\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 47us/sample - loss: 0.4567 - accuracy: 0.8793 - val_loss: 0.4448 - val_accuracy: 0.8672\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 79us/sample - loss: 0.4111 - accuracy: 0.8984 - val_loss: 0.4359 - val_accuracy: 0.8631\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 2s 114us/sample - loss: 0.3702 - accuracy: 0.9092 - val_loss: 0.4414 - val_accuracy: 0.8623\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 48us/sample - loss: 0.3309 - accuracy: 0.9225 - val_loss: 0.4469 - val_accuracy: 0.8597\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.3039 - accuracy: 0.9243 - val_loss: 0.4568 - val_accuracy: 0.8624\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 79us/sample - loss: 0.2757 - accuracy: 0.9331 - val_loss: 0.4543 - val_accuracy: 0.8614\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 81us/sample - loss: 0.2554 - accuracy: 0.9354 - val_loss: 0.5007 - val_accuracy: 0.8594\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 47us/sample - loss: 0.2365 - accuracy: 0.9406 - val_loss: 0.5282 - val_accuracy: 0.8583\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 48us/sample - loss: 0.2237 - accuracy: 0.9463 - val_loss: 0.5405 - val_accuracy: 0.8588\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 45us/sample - loss: 0.2040 - accuracy: 0.9474 - val_loss: 0.6111 - val_accuracy: 0.8568\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 89us/sample - loss: 0.1946 - accuracy: 0.9526 - val_loss: 0.6237 - val_accuracy: 0.8565\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 75us/sample - loss: 0.1842 - accuracy: 0.9523 - val_loss: 0.6808 - val_accuracy: 0.8570\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 45us/sample - loss: 0.1712 - accuracy: 0.9583 - val_loss: 0.7246 - val_accuracy: 0.8541\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 46us/sample - loss: 0.1671 - accuracy: 0.9587 - val_loss: 0.7802 - val_accuracy: 0.8537\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 67us/sample - loss: 0.1570 - accuracy: 0.9633 - val_loss: 0.8825 - val_accuracy: 0.8510\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 46us/sample - loss: 0.1520 - accuracy: 0.9646 - val_loss: 0.8430 - val_accuracy: 0.8504\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 78us/sample - loss: 0.1480 - accuracy: 0.9659 - val_loss: 1.0063 - val_accuracy: 0.8497\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 80us/sample - loss: 0.1397 - accuracy: 0.9675 - val_loss: 1.0058 - val_accuracy: 0.8520\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_dropout = model.fit(x_train, y_train, epochs=20, batch_size=256, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let’s plot the results:\n",
    "\n",
    "然后绘制结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fae7fe9d7d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, history_old.history['val_loss'], 'b+', label='Original model')\n",
    "plt.plot(x, history_dropout.history['val_loss'], 'bo', label='Dropout-regularized model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Again, a clear improvement over the reference network.\n",
    "To recap: here the most common ways to prevent overfitting in neural networks:\n",
    "\n",
    "> - Getting more training data.\n",
    "- Reducing the capacity of the network.\n",
    "- Adding weight regularization.\n",
    "- Adding dropout.\n",
    "\n",
    "再次可以看到和参照网络的过拟合比较有了明显改善。总结一下，在神经网络中预防过拟合的最通用方法包括：\n",
    "\n",
    "- 获得更多训练数据。\n",
    "- 减少网络的容量。\n",
    "- 增加权重正则化。\n",
    "- 增加dropout。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 机器学习通用工作流\n",
    "\n",
    "> What we present here is a universal blueprint you can use to attack and solve any\n",
    "machine learning problem, tying together the different concepts you learned about in this\n",
    "chapter: problem definition, evaluation, feature engineering, and fighting overfitting.\n",
    "\n",
    "下面我们将会展示可以用来解决任何机器学习问题的通用蓝图，我们将会把本章中学习到的所有概念结合起来：定义问题、验证、特征工程和对抗过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 定义问题和收集数据集\n",
    "\n",
    "> First, you must define the problem at hand:\n",
    "\n",
    "> - What will your input data will be? What will you be trying to predict? You can only learn\n",
    "to predict something if you have available training data, e.g. you can only learn to\n",
    "classify the sentiment of movie reviews if you have both movie reviews and sentiment\n",
    "annotations available. As such, data availability is usually the limiting factor at this stage\n",
    "(unless you have the means to pay people to collect data for you).\n",
    "- What type of problem are you facing—is it binary classification? Multi-class\n",
    "classification? Scalar regression? Vector regression? Multi-class, multi-label\n",
    "classification? Something else, like clustering, generation or reinforcement learning?\n",
    "Identifying the problem type will guide your choice of model architecture, loss function,\n",
    "and so on.\n",
    "\n",
    "首先你必须对手头的问题进行定义：\n",
    "\n",
    "- 你的输入数据会是什么样的？你试图去预测的是什么？当你有了可用的训练数据时，你才可能对某类问题进行预测，例如只有当你有了影评数据和相应的情绪分类标签数据之后，你才能学习对影评积极消极情绪进行分类。因此数据的可用性通常是这个阶段的限制因素（除非你有途径花钱让人们为你收集数据）。\n",
    "- 你正在面对的是哪种问题 - 是否二元分类？多类别分类？标量回归？向量回归？多类别多标签分类？其他如聚类、生成或者强化学习？确定问题的类别能够对你在模型结构、损失函数等等的选择上给出指导。\n",
    "\n",
    "> You cannot move to the next stage until you know what your inputs and outputs are,\n",
    "and what data you will be using. Be aware of the hypotheses that you are making at this\n",
    "stage:\n",
    "\n",
    "> - You are hypothesizing that your outputs can be predicted given your inputs.\n",
    "- You are hypothesizing that your available data is sufficiently informative to learn the\n",
    "relationship between inputs and outputs.\n",
    "\n",
    "在你知道输入和输出以及你将要使用什么数据之前，无法进行下一步。在这个阶段要特别注意你做出的猜想和假设：\n",
    "\n",
    "- 你猜测输出能够通过给定的输入进行预测。\n",
    "- 你假设你手头可用的数据足够支撑在输入和输出之间学习到它们的关联。\n",
    "\n",
    "> Until you have a working model, then these are merely hypotheses, waiting to be\n",
    "validated or invalidated. Not all problems can be solved; just because you have\n",
    "assembled examples of inputs X and targets Y doesn’t mean that X contains enough\n",
    "information to predict Y. For instance, if you are trying to predict the movements of a\n",
    "stock on the stock market given its recent price history, your are unlikely to succeed,\n",
    "since price history simply doesn’t contain much predictive information.\n",
    "\n",
    "在你获得一个工作良好的模型之前，这些都仅仅是假设，等待着被证实或者被证伪。不是所有的问题都能得到解决；仅仅由于你已经收集了输入X和目标Y的样本数据，并不意味着X包含了足够的信息来预测Y。例如，如果你试图依靠最近的价格历史数据预测股市上的股价波动，你很可能不会成功，因为价格数据并没有包含很多可预测的信息。\n",
    "\n",
    "> One class of unsolvable problems of which you should be specifically aware is\n",
    "non-stationary problems. Suppose that you are trying to build a recommendation engine\n",
    "for clothing, and that you are training it on one month of data, August, and that you want\n",
    "to start generating recommendations in the winter. One big issue is that the kind of\n",
    "clothes that people buy changes from season to season, i.e. clothes buying is a\n",
    "non-stationary phenomenon over the scale of a few months. What you are trying to\n",
    "model changes over time. In this case the right move would be to constantly retrain your\n",
    "model on data from the recent past, or gather data at a timescale where the problem is\n",
    "stationary. For a cyclical problem like clothes buying, a few years worth of data would\n",
    "suffice to capture seasonal variation—but then you should remember to make the time of\n",
    "the year an input of your model!\n",
    "\n",
    "有一种无法解决的问题类别你需要特别注意，那就是非稳态问题。假设你希望构建一个衣服的推荐引擎，然后你使用了一个月的数据来训练它，如八月，然后你希望能够在冬季生成推荐内容，这里的问题在于，购买衣服在几个月的区间上是一个非稳态现象。你希望建立的模型总是随着时间而变化。在这个场景中正确的做法是让你的模型持续的从最近收集到的数据中进行学习，或者采集足够时间跨度的数据使得问题称为稳态的。对于一个想购买衣服这样周期性的问题来说，几年的数据应该可以足够让你捕捉到季节的变化- 但是你应该记住将年份也作为你的模型输入。\n",
    "\n",
    "> Keep it in mind: machine learning can only be used to memorize patterns which are\n",
    "present in your training data. You can only recognize what you have seen before. Using\n",
    "machine learning trained on past data to predict the future is making the assumption that\n",
    "the future will behave like the past. That is often not the case.\n",
    "\n",
    "牢记于心：机器学习只能够用来记忆你训练数据中展示的模式。你只能认得你已经见过的模式。使用机器学习在过去的数据上训练来预测未来，都默认假定未来会想过去一样演进。这通常都不正确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 选择衡量成功的指标\n",
    "\n",
    "> To control something, you need to be able to observe it. To achieve success, you must\n",
    "define what you mean by success—accuracy? Precision-Recall? Customer retention rate?\n",
    "Your metric for success will guide the choice of your loss function, i.e. the choice of\n",
    "what your model will optimize. It should directly align with your higher-level goals, such\n",
    "as the success of your business.\n",
    "\n",
    "要控制一件事情，你需要能够观察它。要获得成功，你必须定义成功的含义：准确率？精确度召回率？客户留存率？你对成功的指标将会指导损失函数的选择，也就是模型优化的选择。它应该与你的高层次目标相匹配，例如你商业上的成功标准。\n",
    "\n",
    "> For balanced classification problems, where every class is equally likely, accuracy\n",
    "and ROC-AUC are common metrics. For class-imbalanced problems, one may use\n",
    "Precision-Recall. For ranking problems or multi-label classification, one may use Mean\n",
    "Average Precision. And it isn’t uncommon to have to define your own custom metric by\n",
    "which you will measure success. To get a sense of the diversity of machine learning\n",
    "success metrics and how they relate to different problem domains, it is helpful to browse\n",
    "data science competitions on Kaggle.com , as they showcase a wide range of different\n",
    "problems and evaluation metrics.\n",
    "\n",
    "对于平衡分类问题来说，也就是每个类别都平均分布的情况，准确率和ROC或AUC曲线是较常用的指标。对于非平衡分类问题来说，可以使用精确度召回率。对于排名问题或者多标签分类问题来说，可以使用平均精度。并且自定义指标来衡量成功也并不罕见。要了解机器学习成功指标的分类知识以及它们对应的不同问题领域，可以浏览数据科学竞赛网站[Kaggle.com](https://kaggle.com)，上面展示了覆盖了广泛的问题领域和验证指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 确定一个验证规则\n",
    "\n",
    "> Once you know what you are aiming for, you must establish how you will measure your\n",
    "current progress. We have previously reviewed three common evaluation protocols:\n",
    "\n",
    "> - Maintaining a hold-out validation set; this is the way to go when you have plenty of data.\n",
    "- Doing K-fold cross-validation; this is the way to go when you have too few samples for\n",
    "hold-out validation to be reliable.\n",
    "- Doing iterated K-fold validation; this is for performing highly accurate model evaluation\n",
    "when little is available.\n",
    "\n",
    "当你确定好目标之后，你必须建立一个机制来测量训练过程的性能。我们前面已经介绍了三种主要的验证规则：\n",
    "\n",
    "- 维护一个留出验证集；这是当你有着大量数据时候的自然选择。\n",
    "- 使用K折交叉验证；这是当你的数据不足以采用留出验证集的情况下使用的方法。\n",
    "- 使用迭代K折验证；这是当数据不足且你需要进行高度精确的模型验证的情况下使用的方法。\n",
    "\n",
    "> Just pick one of these; in most cases the first one will work well enough.\n",
    "\n",
    "从上面中选择一个；在大多数情况下第一个方式就可以满足你的需要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.4 准备数据\n",
    "\n",
    "> Once you know what you are training on, what you are optimizing for, and how to\n",
    "evaluate your approach, you are almost ready to start training models. But first, you\n",
    "should format your data in a way that can be fed into a machine learning model—here we\n",
    "will assume a deep neural network.\n",
    "\n",
    "> - As we saw previously, your data should be formatted as tensors.\n",
    "- The values taken by these tensors should almost typically be scaled to small values, e.g.\n",
    "in the [-1, 1] range or [0, 1] range.\n",
    "- If different features take values in different ranges (heterogenous data), then the data\n",
    "should be normalized.\n",
    "- You may want to do some feature engineering, especially for small data problems.\n",
    "\n",
    "这时候你已经知道你需要训练的内容了，你想要优化的是什么，如何能够验证你的方法，你已经快可以开始训练模型了。不过首先，你应该将数据转换成能够输入到机器学习模型当中的格式：这里我们假定的都是一个深度神经网络。\n",
    "\n",
    "- 正如前面看到的，你的数据应该被格式化为张量。\n",
    "- 这些张量中的数值几乎都应该缩小成较小区间的数值，如$[-1, 1]$或$[0, 1]$的区间。\n",
    "- 如果不同的特征采用了不同的取值区间（异质数据），那么这些数据需要进行标准化。\n",
    "- 你可能需要进行一些特征工程，特别是对于较小的数据问题而言。\n",
    "\n",
    "> Once your tensors of input data and target data are ready, you can start training\n",
    "models.\n",
    "\n",
    "一旦你准备好了输入数据和目标数据的张量，你就可以开始训练模型了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.5 构建一个比基线更好的网络\n",
    "\n",
    "> Your goal at this stage is to achieve \"statistical power\", i.e. develop a small model that is\n",
    "capable of beating a dumb baseline. In our MNIST digits classification example,\n",
    "anything that gets an accuracy higher than 0.1 can be said to have statistical power; in our\n",
    "IMDB example it would be anything with an accuracy higher than 0.5.\n",
    "\n",
    "这个阶段你的目标是获得“统计超越”，也就是开发一个小型的模型，能够击败简朴的基线即可。在我们MNIST数字分类例子中，任何获得超过0.1的准确的方法都可以认为是具有统计超越；在我们IMDB例子中，那就是任何超过0.5准确率的方法。\n",
    "\n",
    "> Note that it is not always possible to achieve statistical power. If you cannot beat a\n",
    "random baseline after trying multiple reasonable architectures, it may be that the answer\n",
    "to the question you are asking isn’t actually present in the input data. Remember that you\n",
    "are making two hypotheses:\n",
    "\n",
    "> - You are hypothesizing that your outputs can be predicted given your inputs.\n",
    "- You are hypothesizing that your available data is sufficiently informative to learn the\n",
    "relationship between inputs and outputs.\n",
    "\n",
    "需要注意的是并不是任何情况下都能达到统计超越。如果当你尝试了很多种合理的架构之后，仍然无法击败随机的基线，那么可能是你所需要的问题答案并不体现在输入数据当中。请记住你作出了以下两个前提假设：\n",
    "\n",
    "- 假设输出能够通过给定的输入获得。\n",
    "- 假设你手头可用的数据具有足够的信息来学习到输入和输出之间的关联。\n",
    "\n",
    "> It may well be that these hypotheses are false, in which case you would have to go\n",
    "back to the drawing board.\n",
    "\n",
    "上面两个假设都有可能最终被证伪的，无论哪个假设是否定的，你都必须回到白板上来重新规划你的问题了。\n",
    "\n",
    "> Assuming that things go well—there are three keys choices you need to make in order\n",
    "to build your first working model:\n",
    "\n",
    "> - Choice of the last-layer activation. This establishes useful constraints on the network’s\n",
    "output: for instance in our IMDB classification example we used sigmoid in the last\n",
    "layer, in the regression example we didn’t use any last-layer activation, etc.\n",
    "- Choice of loss function. It should match the type of problem you are trying to solve: for\n",
    "instance in our IMDB classification example we used binary_crossentropy , in the\n",
    "regression example we used mse , etc.\n",
    "- Choice of optimization configuration: what optimizer will you use? What will its learning\n",
    "rate be? In most cases it is safe to go with rmsprop and its default learning rate.\n",
    "\n",
    "如果事情进行的顺利 - 下面就有三个关键的选择，直接关系着能否构建一个良好的模型：\n",
    "\n",
    "- 选择最后一层的激活函数。这个选择会对你的模型的输出作出限定：比如在IMDB分类例子中我们使用`sigmoid`作为最后一层的激活函数，在回归的例子中我们在最后一层没有使用任何的激活函数等等。\n",
    "- 选择损失函数。这个选择应该匹配你试图解决问题的类型：例如IMDB分类例子中我们使用`binary_crossentropy`，在回归例子中我们使用`mse`等等。\n",
    "- 选择优化配置。你应该使用哪个优化器？它的学习率应该是多少？在很多情况下，使用rmsprop和他默认的学习率都是安全的选择。\n",
    "\n",
    "> Regarding the choice of a loss function: note that it isn’t always possible to directly\n",
    "optimize for the metric that measures success on a problem. Sometimes there is no easy\n",
    "way to turn a metric into a loss function; loss functions, after all, need to be computable\n",
    "given only a mini-batch of data (ideally, a loss function should be computable for as few\n",
    "as a single data point) and need to be differentiable (otherwise you cannot use\n",
    "backpropagation to train your network). For instance, the widely used classification\n",
    "metric ROC-AUC (Receiver Operating Characteristic Area Under the Curve) cannot be\n",
    "directly optimized. Hence in classification tasks it is common to optimize for a proxy\n",
    "metric of ROC-AUC, such as crossentropy. In general, one can hope that the lower the\n",
    "crossentropy gets, the higher the ROC-AUC will be.\n",
    "\n",
    "关于损失函数的选择：要说明的是并不是所有的情况下都能找到直接优化测量问题解决成功度的方式。有时候将这个指标转换成损失函数是很困难的；损失函数归根结底来说，需要能够在小批量数据上是可计算的（理想情况下损失函数应该在单个数据点上也是可计算的），而且还应该是可微的（否则就不能应用反向传播来训练你的网络）。例如ROC-AUC是广泛使用的分类指标（接收者操作特征曲线下面积），但是它不能直接进行优化。因此在分类任务中，我们通常使用它的替代品来进行优化，例如交叉熵。在大部分情况下，可以认为越低的交叉熵会得到更高的ROC-AUC。\n",
    "\n",
    "> Here is a table to help you pick a last-layer activation and a loss function for a few\n",
    "common problem types:\n",
    "\n",
    "下表列出了一些通用问题类型的最后层激活函数以及损失函数，帮助你进行选择：\n",
    "\n",
    "| 问题类型 | 最后层激活函数 | 损失函数 |\n",
    "| - | - | - |\n",
    "| 二元分类 | sigmoid | binary_crossentropy |\n",
    "| 多类别单标签分类 | softmax | categorical_crossentropy |\n",
    "| 多类别多标签分类 | sigmoid | binary_crossentropy |\n",
    "| 任意值回归 | None | mse |\n",
    "| $[0, 1]$区间值回归 | sigmoid | mse 或者 binary_crossentropy |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.6 扩展：开发一个过拟合的模型\n",
    "\n",
    "> Once you have obtained a model that has statistical power, the question becomes: is your\n",
    "model powerful enough? Does it have enough layers and parameters to properly model\n",
    "the problem at hand? For instance, a network with a single hidden layer with 2 units\n",
    "would have statistical power on MNIST, but would not be sufficient to solve the problem\n",
    "well. Remember that the universal tension in machine learning is between optimization\n",
    "and generalization; the ideal model is one that stands right at the border between\n",
    "under-fitting and over-fitting; between under-capacity and over-capacity. To figure out\n",
    "where this border lies, first you must cross it.\n",
    "\n",
    "到这个阶段你已经有一个统计超越的模型了，现在问题就变成：你的模型是否足够强大？它是否有了足够的层和参数能够正确的表达手头的问题？例如，一个具有单个层其中两个隐藏单元的网络，就可以在MNIST问题上达到统计超越了，但是这肯定不足以很好的解决这个问题。回想一下机器学习中的核心问题是优化和泛化；理想的模型是介于欠拟合和过拟合之间的边界上；位于容量不足和容量太大之间。要找到边界在哪，首先你得跨越它。\n",
    "\n",
    "> To figure out how big a model you will need, you must develop a model that overfits.\n",
    "This is fairly easy:\n",
    "\n",
    "> - Add layers.\n",
    "- Make your layers bigger.\n",
    "- Train for more epochs.\n",
    "\n",
    "要找到你需要多大的模型，你需要开发一个过拟合的模型。这其实很容易：\n",
    "\n",
    "- 增加更多的层。\n",
    "- 将层变得更大。\n",
    "- 进行更多次的迭代。\n",
    "\n",
    "> Always monitor the training loss and validation loss, as well as the training and\n",
    "validation values for any metrics you care about. When you see that the performance of\n",
    "the model on the validation data starts degrading, you have achieved overfitting.\n",
    "\n",
    "永远都需要监控训练损失和验证损失，再加上训练和验证中其它任何你关心的指标。当你看到模型的性能在验证集上开始下降时，就达到过拟合了。\n",
    "\n",
    "> The next stage is to start regularizing and tuning your model, in order to get as close\n",
    "as possible to the ideal model, that is neither underfitting nor overfitting.\n",
    "\n",
    "下一个阶段就需要对你的模型进行正则化和调参，最终目标是获得尽可能理想的模型，它应该既不是欠拟合也不是过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.7 正则化模型和调整超参数\n",
    "\n",
    "> This is the part that will take you the most time: you will repeatedly modify your model,\n",
    "train it, evaluate on your validation data (not your test data at this point), modify it\n",
    "again... until your model is as good as it can get.\n",
    "\n",
    "下面将是最花费时间的部分了：你会重复修改你的模型，再训练，在验证数据上进行验证（这里不要使用测试数据），再次修改模型......直到你的模型达到它能获得的最佳结果。\n",
    "\n",
    "> These are some of things you should be trying:\n",
    "\n",
    "> - Add dropout.\n",
    "- Try different architectures, add or remove layers.\n",
    "- Add L1 / L2 regularization.\n",
    "- Try different hyperparameters (such as the number of units per layer, the learning rate of\n",
    "the optimizer) to find the optimal configuration.\n",
    "- Optionally iterate on feature engineering: add new features, remove features that do not\n",
    "seem to be informative.\n",
    "\n",
    "下面是你可以尝试的工作：\n",
    "\n",
    "- 增加dropout。\n",
    "- 尝试不同的架构，增加或减少层。\n",
    "- 增加L1或L2正则化\n",
    "- 尝试不同的超参数（例如每层隐藏单元的数量，优化器的学习率）来获得最优的配置。\n",
    "- 选择性的进行一些特征工程：增加新的特征，去掉那些看起来没有什么信息量的特征。\n",
    "\n",
    "> Be mindful of the following: every time you are using feedback from your validation\n",
    "process in order to tune your model, you are leaking information about your validation\n",
    "process into your model. Repeated just a few times, this is innocuous, but done\n",
    "systematically over many iterations will eventually cause your model to overfit to the\n",
    "validation process (even though no model is directly trained on any of the validation\n",
    "data). This makes your evaluation process less reliable, so keep it in mind.\n",
    "\n",
    "需要牢记下面的内容：每次你使用验证过程的反馈结果来调整模型，都会泄露部分验证数据信息到模型当中。重复进行这个过程几次，并不会造成太大危害。但如果重复进行很多次这样的过程，就会最终导致你的模型对验证数据产生过拟合（即使模型并没有直接在验证数据上进行训练）。这会导致验证的结果不可靠，请记住这一点。\n",
    "\n",
    "> In summary, this is the universal workflow of machine learning:\n",
    "\n",
    "> 1. Define the problem at hand and the data you will be training on; collect this data or\n",
    "annotate it with labels if need be.\n",
    "2. Choose how you will measure success on your problem. Which metrics will you be\n",
    "monitoring on your validation data?\n",
    "3. Determine your evaluation protocol: hold-out validation? K-fold validation? Which\n",
    "portion of the data should you use for validation?\n",
    "4. Develop a first model that does better than a basic baseline: a model that has\n",
    "\"statistical power\".\n",
    "5. Develop a model that overfits.\n",
    "6. Regularize your model and tune its hyperparameters, based on performance on the\n",
    "validation data.\n",
    "\n",
    "总结来说，下面就是机器学习的通用工作流程：\n",
    "\n",
    "1. 定义手头的问题和你将要进行训练数据；采集数据或者对数据标签进行标记。\n",
    "2. 选择衡量问题成功的标准。在验证数据上面应该使用哪个指标来进行监控。\n",
    "3. 指定验证规则：留出验证？K折验证？数据中的哪个部分应该用作验证？\n",
    "4. 开发第一个模型，只需要比基线性能要强即可：也就是达到统计超越的模型。\n",
    "5. 开发一个产生过拟合的模型。\n",
    "6. 使用模型在验证数据上的性能来正则化模型和调整超参数。\n",
    "\n",
    "> A lot of machine learning research tends to focus only on the last step—but keep in\n",
    "mind the big picture.\n",
    "\n",
    "很多机器学习的研究都聚焦在最后一步，对于你来说需要记住的是整体的图景。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<< [第三章：进入神经网络](Chapter3_Getting_started_with_neural_networks.ipynb) || [目录](index.md) || [第五章：计算机视觉中的深度学习](Chapter5_Deep_learning_for_computer_vision.ipynb) >>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
